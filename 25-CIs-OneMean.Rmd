
# CIs for one mean {#OneMeanConfInterval}



```{r, child = if (knitr::is_html_output()) {'./introductions/26-CIs-OneMean-HTML.Rmd'} else {'./introductions/26-CIs-OneMean-LaTeX.Rmd'}}
```


## Describing the sampling distribution: $\sigma$ known {#SamplingDistSampleMeanSigmaKnown}
\index{Sampling distribution!one mean ($\sigma$ known)}


<div style="float:right; width: 222x; border: 1px; padding:10px">
<img src="Illustrations/pexels-skitterphoto-705171.jpg" width="200px"/>
</div>


In this chapter, we study the situation where a population mean $\mu$ (the parameter) is estimated by a sample mean $\bar{x}$ (the statistic).
The sample mean is computed from just one of the many possible samples, and each possible sample is likely to produce a different value of $\bar{x}$.
That is, the value of the sample mean varies from sample to sample, called *sampling variation*.


::: {.importantBox .important data-latex="{iconmonstr-warning-8-240.png}"}
Remember: Studying a sample leads to the following observations:
\vspace{-2ex}

* Every sample is likely to be different.
* We observe just one of the many possible samples.
* Every sample is likely to yield a different value for the sample statistic.
* We observe just one of the many possible values for the statistic.
\vspace{-2ex}

Since many values for the sample mean are possible, the possible values of the sample mean vary (called *sampling variation*) and have a *distribution* (called a *sampling distribution*).
:::


Consider rolling dice again.
Suppose a die is rolled $n = 25$ times, and the *mean* of the sample of $25$ numbers that are rolled is recorded.
Since every face of the die is equally likely to appear on any one roll, the population mean of all possible rolls is $\mu = 3.5$ (in the middle of the numbers on the faces of the die, so this is also the *median*).

What will be the sample mean of the numbers in the $25$ rolls?
We cannot be sure, as the sample mean will vary from sample to sample (*sampling variation*).
But suppose we roll a die $25$ times, to see how the sample mean varies in $25$ rolls
`r if (knitr::is_latex_output()) {
   '(see Fig.\\ \\@ref(fig:RollDiceMeanFig) for ten samples of $25$ rolls).'
} else {
   'as shown in the animation below for ten samples of $25$ rolls.'
}`
The mean of the $25$ rolls clearly varies, as expected.
In the simulation, the sample mean of $25$ rolls was as low as $3.08$ and as high as $3.76$.


```{r}
die.mn <- sum( (1:6) * (1/6) )  # 3.5 as expected
die.vr <-  sum( ((1:6) - 3.5)^2 * (1/6) ) #  2.916667
die.se <- sqrt( die.vr / 25)
```




```{r RollDiceMeanHTML, animation.hook="gifski", interval=0.5, dev=if (is_latex_output()){"pdf"}else{"png"}}
if (knitr::is_html_output()){
  set.seed(99999)
    num.rolls <- 25
    num.sims <- 10
    x.loc <- 1:(num.rolls)
    y.loc <- 1
    mean.even <- array(dim = num.sims)
    all.rolls <- array( dim = c(num.sims, num.rolls))
    
    for (i in 1:num.sims){
      
      par( mar = c(5.1, 5.1, 4.1, 2.1))
      
      plot( c(1, (num.rolls + 2)), c(1, num.sims),
            type = "n",
            las = 1,
            xlab = "",
            ylab = "",
            main = paste("Sample number", i),
            axes = FALSE)
      
      roll <- sample(1:6, 
                     num.rolls, 
		     replace = TRUE)
      all.rolls[i, ] <- roll
      mean.even[i] <- mean(roll)
      
      col1 <- col2rgb("darkolivegreen2")
      col2 <- col2rgb("indianred2")
 
      for (j in 1:i){
        text(y = j, 
	     x = 1:num.rolls,
             labels = all.rolls[j, ])
        
        # Add some slight background colour under the sample proportions
        polygon( c(num.rolls + 1, num.rolls + 1, num.rolls + 3, num.rolls + 3),
                 c(j - 0.5, j + 0.5, j + 0.5, j - 0.5),
                 border = NA,
                 col = ifelse( mean.even[j] > 3.5, rgb( col1[1], col1[2], col1[3], alpha = 75, max = 255), 
                                                   rgb( col2[1], col2[2], col2[3], alpha = 75, max = 255) ) )

      }
      # Add x-bar heading
      mtext(expression(bar( italic(x) ) ), 
            side = 3, 
	    line = 0, 
	    at = num.rolls + 2 )
      # Add the roll number to the left-hand side
      axis(side = 2, 
           at = 1:i,
           las = 1,
           labels = paste("Sample #", 1:i, sep = "") )
      
      # Add the sample mean to right-hand side 
      text(num.rolls + 2, 
           1:i, 
           labels = format(round(mean.even[1:i], 2), nsmall = 2) )
      #Add dividing line
      abline(v = num.rolls + 1, 
             col = "grey")
      # Add line dividing roll sets
      abline(h = seq(0.5, 10.5, by = 1), 
             col = "gray")
    }
  }
```



```{r RollDiceMeanFig, fig.align="center", fig.width=7, fig.height=4, out.width="85%", fig.cap="Rolling dice: The average of $25$ rolls, for ten samples" }
if (knitr::is_latex_output()){
  set.seed(99999)
  num.rolls <- 25
  num.sims <- 10
  x.loc <- 1:(num.rolls)
  y.loc <- 1
  mean.even <- array(dim = num.sims)
  all.rolls <- array( dim = c(num.sims, num.rolls))
  
  par(mar = c(0.5, 5.5, 5, 0.5) )
  plot( c(1, (num.rolls + 2)), c(1, num.sims),
        type = "n",
        las = 1,
        xlab = "",
        ylab = "",
        main = "The sample mean from a sample of 25 rolls\nfor each of 10 simulations",
        axes = FALSE)
  
  for (i in 1:num.sims){
    roll <- sample(1:6, 
                   num.rolls, 
                   replace = TRUE)
    all.rolls[i, ] <- roll
    mean.even[i] <- mean(roll)
  }
  for (j in 1:num.sims){
    text(y = j, 
         x = 1:num.rolls,
         labels = all.rolls[j, ])
  }
  # Add x-bar heading
  mtext(expression(bar( italic(x) ) ), 
        side = 3, 
        line = 0, 
        at = num.rolls + 2 )
  # Add the roll number to the left-hand side
  axis(side = 2, 
       at = 1:i,
       las = 1,
       labels = paste("Sample #", 1:i, sep = "") )
  
  # Add the sample mean to right-hand side 
  text(x = num.rolls + 2, 
       y = 1:i, 
       font = ifelse( mean.even > 3.5, 2, 1),
       labels = format(round(mean.even[1:i], 2), nsmall = 2) )
  
  #Add dividing line
  abline(v = num.rolls + 1, 
         col = "grey")
  
  # Add line dividing roll sets
  abline(h = seq(0.5, 10.5, by = 1), 
         col = "gray")
  #}
}
```


The mean for any single sample of $n = 25$ rolls will sometimes be higher than $\mu = 3.5$, and sometimes lower than $\mu = 3.5$, but most of the time the mean should be close to $3.5$.
If thousands of people made one sample of $25$ rolls each, and computed the mean for their sample, every person would have a sample mean for their sample, and we could produce a histogram of all these sample means
`r if (knitr::is_latex_output()) {
   '(Fig.\\ \\@ref(fig:RollDiceHistMeanFig)).'
} else {
   '(see the animation below).'
}`


```{r RollDiceHistMeanHTML, animation.hook="gifski", interval=0.4, dev=if (is_latex_output()){"pdf"}else{"png"}}
if (knitr::is_html_output()){
  set.seed(123456789)
  num.sims <- 1000
  num.rolls <- 25
  
  
  print_Histo <- rep(FALSE, num.sims)
  print_Histo[ c( 1:10,
                  seq(24, num.sims, 25) + 1,
                  num.sims) ] <- TRUE
  
  die.mn <- sum( (1:6) * (1/6))  # 3.5 as expected
  die.vr <-  sum( ((1:6) - 3.5)^2 * (1/6))
  
  meanList <- array( dim = num.sims)
  
  for (i in 1:num.sims){
    meanSingleRoll <- mean(sample(1:6, 
                                  num.rolls, 
                                  replace = TRUE))
    meanList[i] <- meanSingleRoll
    
    
    #Print every nth histogram only
    if (print_Histo[i]){
      out <- hist(meanList,
                  xlab = "Mean of 25 rolls",
                  ylab = "Number of times observed",
                  main = paste("Histogram of the mean of 25 rolls:\nSimulation number:", i),
                  sub = paste("(For this sample: mean roll is ", 
                              format(round(meanList[i], 2), nsmall = 2), 
                              ")", 
                              sep = "" ),
                  col = plot.colour,
                  las = 1,
                  xlim = c(1.5, 5.5), 
                  right = FALSE,
                  ylim = c(0, 300),
                  breaks = seq(2, 5, by = 0.2)
      )
      
      points(meanList[i], 0,
             pch = 19,
             col = plot.colour0)
      xx <- seq(1, 6, length=500)
      yy <- dnorm(xx, 
                  mean = die.mn, 
                  sd = sqrt(die.vr/num.rolls) )
      yy <- yy/max(yy) * max(out$count)
      
      lines(yy ~ xx, 
            col = "grey", 
            lwd = 2)  
    }
  }
}
```


```{r RollDiceHistMeanFig, fig.align="center", fig.width=5, fig.height=2.5, fig.cap="Rolling dice: The mean of 25 rolls, for thousands of repetitions" }
if (knitr::is_latex_output()){
  set.seed(123456789)
  num.sims <- 2000
  num.rolls <- 25
  
  die.mn <- sum( (1:6) * (1/6))  # 3.5 as expected
  die.vr <-  sum( ((1:6) - 3.5)^2 * (1/6))
  
  meanList <- array( dim=num.sims)
  
  for (i in 1:num.sims){
    meanSingleRoll <- mean(sample(1:6, 
                                  num.rolls, 
                                  replace = TRUE))
    meanList[i] <- meanSingleRoll
  }
  
  out <- hist(meanList,
              xlab = "Mean of 25 rolls",
              #ylab = "Number of times observed",
              ylab = "",
              main = "Histogram of the mean of 25 rolls:\nafter thousands of simulations",
              #                             num.sims, "simulations"),
              col = plot.colour,
              las = 1,
              axes = FALSE,
              #xlim = c(1.5, 5.5),
              right = FALSE,
              #ylim = c(0, 300),
              breaks = seq(2, 5, by = 0.2) 
  )
  axis(side = 1,
       at = 1:6)
  xx <- seq(1, 6, 
            length = 500)
  yy <- dnorm(xx, 
              mean = die.mn, 
              sd = sqrt(die.vr/num.rolls) )
  yy <- yy/max(yy) * max(out$count)
  
  lines(yy ~ xx, 
        col = "grey", 
        lwd = 2)  
  
  points(x = 3.5,
         y = 0,
         pch = 19)
  mtext(expression(mu),
        side = 1, 
        line = 0.5)
}      
```


`r if (knitr::is_latex_output()) {
   'From Fig.\\ \\@ref(fig:RollDiceHistMeanFig),'
} else {
   'From the animation above,'
}`
the sample means vary with an approximate normal distribution (as we saw with the sample proportions).
This normal distribution does *not* describe the data; it describes how the *values of sample means vary across all possible samples*.
Under certain conditions, the values of the sample means can vary with a normal distribution, and this normal distribution has a mean and a standard deviation.

This sampling distribution describes how *sample means* vary.
The mean of this sampling distribution---the *sampling mean*---has the value $\mu$.
The standard deviation of this sampling distribution is called the *standard error of the sample means*, denoted $\text{s.e.}(\bar{x})$.
When the *population* standard deviation $\sigma$ is *known*, the standard error happens to be  
\[
  \text{s.e.}(\bar{x}) = \frac{\sigma}{\sqrt{n}}.
\]
The possible values of the sample means have a *sampling distribution* described by:

* an approximate normal distribution,
* with a sampling mean whose value is $\mu$, and
* a standard deviation, called the standard error, of $\text{s.e.}(\bar{x}) = \sigma/\sqrt{n}$.

However, since the population standard deviation is rarely ever known, let's focus on the case where the value of $\sigma$ is unknown.


## Describing the sampling distribution: $\sigma$ unknown {#SamplingDistSampleMean}
\index{Sampling distribution!one mean ($\sigma$ unknown)}


A sample mean is used to estimate a population mean, but the sample mean varies from sample to sample: sampling variation exists.
Since the value of the *population* standard deviation $\sigma$ is almost never known, the sample standard deviation $s$ is used to give an estimate of the standard error of the mean: $\text{s.e.}(\bar{x}) = s/\sqrt{n}$.
With this information, the *sampling distribution of the sample mean* can be described.


::: {.definition #DEFSamplingDistributionXbar name="Sampling distribution of a sample mean with $\sigma$ unknown"}
When the *population* standard deviation is unknown, the *sampling distribution of the sample mean* is (when certain conditions are met; Sect.\ \@ref(ValiditySampleMean)) described by:
  
* an approximate normal distribution,
* centred around a sampling mean whose value is $\mu$,
* with a standard deviation (called the *standard error of the mean*) $\text{s.e.}(\bar{x})$, whose value is  
\begin{equation}
   \text{s.e.}(\bar{x}) = \frac{s}{\sqrt{n}},
   (\#eq:stderrorxbar)
\end{equation}
where $n$ is the size of the sample, and $s$ is the sample standard deviation of the observations. 
:::


::: {.importantBox .important data-latex="{iconmonstr-warning-8-240.png}"}
A mean or a median may be appropriate for describing the *data*.
However, the *sampling distribution* for the sample mean (under certain conditions) has a *normal distribution*, and so the mean is appropriate for describing the sampling distribution.
:::


<!-- ```{r NotationOneMeanCI} -->
<!-- OneMeanNotation <- array( dim = c(4, 2)) -->

<!-- OneMeanNotation[1, ] <- c("Individual values in the population", -->
<!--                           "Vary with mean $\\mu$ and standard deviation $\\sigma$") -->
<!-- OneMeanNotation[2, ] <- c("Individual values in a sample", -->
<!--                           "Vary with mean $\\bar{x}$ and standard deviation $s$") -->
<!-- OneMeanNotation[3, ] <- c("Sample means ($\\bar{x}$) across", -->
<!--                           "Vary with approx. normal distribution (under certain conditions):") -->
<!-- OneMeanNotation[4, ] <- c("all possible samples", -->
<!--                           "sampling mean $\\mu$; standard deviation $\\text{s.e.}(\\bar{x})$") -->


<!-- if( knitr::is_latex_output() ) { -->
<!--   kable( OneMeanNotation, -->
<!--          format = "latex", -->
<!--          booktabs = TRUE, -->
<!--          longtable = FALSE, -->
<!--          escape = FALSE, -->
<!--          caption = "The notation used for describing means, and the sampling distribution of the sample means", -->
<!--          align = c("r", "l"), -->
<!--          linesep = c("\\addlinespace", -->
<!--                      "\\addlinespace", -->
<!--                      ""), -->
<!--          col.names = c("Quantity", -->
<!--                        "Description") ) %>% -->
<!-- 	row_spec(0, bold = TRUE) %>% -->
<!--   kable_styling(font_size = 10) -->
<!-- } else { -->
<!--   OneMeanNotation[3, 1] <- paste(OneMeanNotation[3, 1],  -->
<!--                                  OneMeanNotation[4, 1]) -->
<!--   OneMeanNotation[3, 2] <- paste(OneMeanNotation[3, 2],  -->
<!--                                  OneMeanNotation[4, 2]) -->
<!--   OneMeanNotation[4, ] <- NA -->

<!--     kable( OneMeanNotation, -->
<!--          format = "html", -->
<!--          booktabs = TRUE, -->
<!--          longtable = FALSE, -->
<!--          escape = FALSE, -->
<!--          caption = "The notation used for describing means, and the sampling distribution of the sample means", -->
<!--          align = c("r", "l"), -->
<!--          linesep = c("\\addlinespace", -->
<!--                      "\\addlinespace", -->
<!--                      ""), -->
<!--          col.names = c("Quantity", -->
<!--                        "Description") ) %>% -->
<!-- 	row_spec(0, bold = TRUE)  -->
<!-- } -->
<!-- ``` -->


## Computing confidence intervals {#OneMeanCI}
\index{Confidence intervals!one mean}

We don't know the value of $\mu$ (the parameter), but we have an *estimate*: the value of $\bar{x}$, the sample mean (the statistic).
The actual value of $\mu$ might be a bit larger than $\bar{x}$, or a bit smaller than $\bar{x}$; that is, the value of $\mu$ is $\bar{x}$, give-or-take a bit.

Furthermore, the values of $\bar{x}$ vary from sample to sample (*sampling variation*), and vary with an approximate normal distribution.
So, the [$68$--$95$--$99.7$ rule](#def:EmpiricalRule) could be used to construct an approximate $95$% interval for the plausible values of $\mu$ that may have produced the observed values of the sample mean.\index{68@$68$--$95$--$99.7$ rule}
This is a *confidence interval*.

A confidence interval (CI) for the population mean is an interval surrounding a sample mean.
In general, a [confidence interval](#def:ConfidenceInterval) (CI) for $\mu$ is  
\[
   \bar{x} \pm \overbrace{(\text{multiplier}\times\text{s.e.}(\bar{x}))}^{\text{The `margin of error'}}.
\]
For an approximate $95$%\ CI, the multiplier is about $2$ (since about $95$% of values are within two standard deviations of the mean, from the [$68$--$95$--$99.7$ rule](#def:EmpiricalRule)).

CIs are commonly $95$%\ CIs, but *any* level of confidence can be used (but a different multiplier is needed).
In this book, a multiplier of $2$ is used when *approximate* $95$%\ CIs are created manually, and otherwise software is used.
Commonly, CIs are computed at $90$%, $95$% and $99$% confidence levels.


::: {.tipBox .tip data-latex="{iconmonstr-info-6-240.png}"}
The multiplier *is not* a $z$-score here (but is *like* a $z$-score).
The multiplier *is* be a $z$-score if the value of the *population* standard deviation was known (e.g., the situation in Sect.\ \@ref(SamplingDistSampleMeanSigmaKnown)).
When $\sigma$ is unknown, and the *sample* standard deviation is used instead, the multiplier is a $t$-score.

The values of $t$- and $z$-multipliers are *very* similar, and (except for small sample sizes) using an approximate multiplier of\ $2$ is reasonable for computing *approximate* $95$%\ CIs in either case.
:::


Pretend for the moment that the value of $\mu$ was unknown, and we tossed a die $25$ times, and found $\bar{x} = 3.2$ and $s = 2.5$.
Then,  
\[
   \text{s.e.}(\bar{x}) = \frac{s}{\sqrt{n}} = \frac{2.5}{\sqrt{25}} = 0.5.
\]
The sample means vary with an approximate normal distribution, centred around the unknown value of $\mu$, with a standard deviation of $\text{s.e.}(\bar{x}) = 0.5$ (Fig.\ \@ref(fig:DiceMeanNormal)).


```{r CIrelationshipsMean, out.width='75%', fig.align="center", fig.cap="A CI gives a range of values of $\\mu$ for which it is reasonable to produce the observed value of $\\bar{x}$. The shaded regions under the normal distributions represent the regions containing 95\\% of the values of $\\bar{x}$ for each value of $\\bar{x}$.", fig.width=6.5, fig.height=8}
#source("R/showCIForVariousMeans.R")                
```


```{r DiceMeanNormal, fig.cap="The sampling distribution is a normal distribution; it shows how the sample mean of 25 die rolls varies in samples of size $n = 25$", fig.align="center", fig.width=9.5, fig.height=2.75, out.width='95%'}

mn <- 3.5
n <- 25
stdd <- 2.5

se <- stdd / sqrt(n)

par( mar = c(4, 0.5, 0.5, 0.5) )
out <- plotNormal(mn,
                  se,
                  xlab = "Sample mean die roll, from 25 rolls of a die", 
                  cex.axis = 0.95,
                  ylim = c(0, 1.3),
                  showXlabels = c( 	
                    expression( mu-0.5),
                    expression( mu-1), 
                    expression( mu-1.5), 
                    expression( mu ),
                    expression( mu+0.5), 
                    expression( mu+1.0), 
                    expression( mu+1.5) ) )

arrows(x0 = mn,
       x1 = mn,
       y0 = 1.4 * max(out$y),
       y1 = max(out$y),
       lwd = 2,
       length = 0.15,
       angle = 15)
text(x = mn,
     y = 1.4 * max(out$y),
     pos = 3,
     labels = expression(Sampling~mean) )

arrows(x0 = mn ,
       x1 = mn + se,
       y0 = 0.35 * max(out$y),
       y1 = 0.35 * max(out$y),
       lwd = 2,
       code = 3,
       length = 0.15,
       angle = 15)
text(x = mn + (se / 2),
     y = 0.3 * max(out$y),
     pos = 3,
     labels = expression(Std~error))
text(x = mn + (se / 2),
     y = 0.32 * max(out$y),
     pos = 1,
     labels = expression(plain(s.e.)(bar(italic(x)))))
```


Our estimate of $\bar{x} = 3.2$ may be a bit smaller than the value of $\mu$, or a bit larger than the value of $\mu$; that is, the value of $\mu$ is in the range '$\bar{x}$ give-or-take a bit'.
A range of $\bar{x}$ values that are likely to straddle $\mu$ is given by a CI.
An *approximate* $95$%\ CI is from $3.2 - (2 \times 0.5)$ and $3.2 - (2 \times 0.5)$, or from $2.2$ to $4.2$.
Hence, values of $\mu$ between $2.2$ to $4.2$ could reasonably have produced a sample mean of $\bar{x} = 3.2$. 
<!-- (Fig.\ \@ref(fig:CIrelationshipsMu)). -->



<!-- ```{r, CIrelationshipsMu, out.width='90%', fig.align="center", fig.cap="Various values of $\\mu$ from which the observed value of $\\bar{x}$ could reasonably be observed. The intervals are the $95$\\% sampling intervals for the given value of $\\mu$. The confidence interval contains the values for which the sampling interval contains the observed value of $\\bar{x}$.", fig.width=8, fig.height=5} -->
<!-- source("R/rangeForMuCI.R")  -->
<!-- ``` -->


::: {.thinkBox .think data-latex="{iconmonstr-light-bulb-2-240.png}"}
Would a $99$%\ CI for $\mu$ be *wider* or *narrower* than the $95$%\ CI?
Why?\label{thinkBox:WiderNarrower}

`r if (knitr::is_latex_output()) '<!--'`
`r webexercises::hide()`
A *wider* interval is needed to be *more* confident that the interval contains the population mean.
`r webexercises::unhide()`
`r if (knitr::is_latex_output()) '-->'`
:::


<!-- ```{r BagsNormal, fig.cap="The sampling distribution is a normal distribution; it shows how the sample mean bag weight varies in samples of size $n = 586$", fig.align="center", fig.width=9.5, fig.height=2.75, out.width='95%'} -->

<!-- mn <- 2.8 -->
<!-- n <- 586 -->
<!-- stdd <- 0.94 -->

<!-- se <- stdd/sqrt(n) -->

<!-- par( mar = c(4, 0.5, 0.5, 0.5) ) -->
<!-- out <- plotNormal(mn, -->
<!--                   se, -->
<!--                   xlab = "Sample mean bag weight, in kg (sample size: 586)",  -->
<!--                   cex.axis = 0.95, -->
<!--                   showXlabels = c( 	 -->
<!--                     expression( mu-0.114), -->
<!--                     expression( mu-0.076),  -->
<!--                     expression( mu-0.038),  -->
<!--                     expression( mu ), -->
<!--                     expression( mu+0.038),  -->
<!--                     expression( mu+0.076),  -->
<!--                     expression( mu+0.114) ) ) -->
<!-- ``` -->



## Statistical validity conditions {#ValiditySampleMean}
\index{Statistical validity!one mean}

As with any confidence interval, the underlying mathematics requires [certain conditions to be met](#exm:StatisticalValidityAnalogy) so that the results are statistically valid (i.e., the sampling distribution is sufficiently like a normal distribution).
The CI for one mean will be *statistical valid* if *one* of these is true:

1. The sample size is at least $25$, *or*
2. The sample size is smaller than $25$ *and* the *population* data has an approximate normal distribution.

The sample size of $25$ is a rough figure, and some books give other (similar) values (such as $30$).

This condition ensures that the *sampling distribution of the sample means has an approximate normal distribution* (so that the [$68$--$95$--$99.7$ rule](#def:EmpiricalRule) can be used).\index{68@$68$--$95$--$99.7$ rule}
Provided the sample size is larger than about $25$, this will be approximately true *even if* the distribution of the individuals in the
population do not have a normal distribution.
That is, when $n > 25$ the sample means generally have an approximate normal distribution, even if the data themselves do not follow a normal distribution.


::: {.importantBox .important data-latex="{iconmonstr-warning-8-240.png}"}
When $n > 25$ approximately, we do *not* require that the *data* has a normal distribution.
The *sample means* need to have a normal distribution, which is approximately true if the statistical validity condition is true.
:::


Deciding whether the population has a normal distribution is obviously difficult; we do not have access to the whole population.
All we can reasonably do is to identify (from the sample) populations that are likely to be very non-normal (when the CI would be not valid).


<div style="float:right; width: 222x; border: 1px; padding:10px">
<img src="Illustrations/pexels-anna-shvets-4225923.jpg" width="200px"/>
</div>


::: {.example #AssumptionsCT name="Assumptions"}
A study [@data:silverman:CT; @data:zou:fluoroscopy] to examine exposure to radiation for CT scans in the abdomen assessed $n = 17$ patients.
As the sample size is 'small' (less than $25$), the *population data* must have a normal distribution for a CI for $\mu$ to be statistically valid.

A histogram of the total radiation dose received using the *sample* data (Fig.\ \@ref(fig:CTscanHistogram)) suggests this is very unlikely.
Even though the histogram is from *sample* data, it seems improbable that the data in the sample would have come from a *population* with a normal distribution.

A CI for the mean of these data will probably be *not* statistically valid.
Other methods (beyond the scope of this course) are possible for computing a CI for the mean.
:::


```{r CTscanHistogram, fig.cap="The radiation doses from CT scans for 17 people", fig.align="center", fig.width=5, fig.height=3.25}
data(Fluoro)

hist(Fluoro$Dose,
	col = plot.colour,
	las = 1,
	xlab = "Radiation dose (in rads)",
	ylab = "Number of people",
	main = "Radiation dose for 17 people\nundergoing a CT scan")
box()
```


::: {.example #DiceConditions name="Statistical validity"}
In the die example (Sect.\ \@ref(OneMeanCI)), where $n = 25$, the CI is statistically valid.
:::


<iframe src="https://learningapps.org/watch?v=ppetqnq4322" style="border:0px;width:100%;height:500px" allowfullscreen="true" webkitallowfullscreen="true" mozallowfullscreen="true"></iframe>


## Example: cadmium in peanuts {#Cadmium-In-Peanuts}


<div style="float:right; width: 222x; border: 1px; padding:10px">
<img src="Illustrations/tom-hermans-ZPfd3ZobOc0-unsplash.jpg" width="200px"/>
</div>


A study of peanuts from the United States [@data:Blair2017:Peanuts] found the sample mean cadmium concentration was $\bar{x} = 0.076$\ ppm with a standard deviation of $s = 0.0460$\ ppm, from a sample of $290$ peanuts gathered from a variety of regions at various times (a representative sample).
The parameter is $\mu$, the population mean cadmium concentration in peanuts.

Every sample of $n = 290$ peanuts is likely to produce a different sample mean, so *sampling variation* in $\bar{x}$ exists and can be measured using the standard error:  
\[
	\text{s.e.}(\bar{x}) = \frac{s}{\sqrt{n}} = \frac{0.0460}{\sqrt{290}} = 0.002701\text{ ppm}.
\]
The approximate $95$%\ CI\ is $0.0768 \pm (2 \times 0.002701)$, or $0.0768 \pm 0.00540$, which is from $0.0714$ to $0.0822$\ ppm.
(The *margin of error* is $0.00540$.)\index{Margin of error}
We write:

> The sample mean cadmium concentration of peanuts is $\bar{x} = 0.0768$\ ppm (s.e.: $0.00270$; $n = 290$), with an approximate $95$%\ CI from $0.0714$ to $0.0822$\ pmm.

If we repeatedly took samples of size $290$ from this population, about $95$% of the $95$%\ CIs would contain the population mean (but *our* CI may or may not contain the value of $\mu$).
The plausible values of $\mu$ that could have produced $\bar{x} = 0.0768$ are between $0.0714$ and $0.0822$\ ppm.
Alternatively, we are about $95$% confident that the CI of $0.0714$ to $0.0822$\ ppm straddles the population mean.

Since the sample size is larger than $25$, the CI is statistically valid.


```{r}
data(LungCap)

LC.F11 <- subset(LungCap, 
                 Age == 11 & Gender == "F")
```



## Chapter summary

To compute a confidence interval (CI) for a mean, compute the sample mean, $\bar{x}$, and identify the sample size $n$.
Then compute the standard error, which quantifies how much the value of $\bar{x}$ varies across all possible samples:  
\[
  \text{s.e.}(\bar{x})
  =
  \frac{ s }{\sqrt{n}},
\]
where $s$ is the sample standard deviation.
The *margin of error* is (Multiplier$\times$standard error), where the multiplier is $2$ for an approximate $95$%\ CI (using the $68$--$95$--$99.7$ rule).
Then find the CI:  
\[
   \bar{x} \pm \left( \text{Multiplier}\times\text{standard error} \right).
\]
Always check whether the [statistical validity conditions](#ValiditySampleMean) are satisfied.


## Quick review questions {#Chap22-QuickReview}

::: {.webex-check .webex-box}
1. True or false: The value of $\bar{x}$ varies from sample to sample. \tightlist  
`r if( knitr::is_html_output() ) {torf(answer = TRUE )}`
1. True or false: A CI for $\mu$ is never statistically valid if the histogram of the *data* has a non-normal distribution.  
`r if( knitr::is_html_output() ) {torf(answer = FALSE )}`
1. A sample of data produces $s = 8$ and $n = 20$. 
   Which *one* of the following is *definitely* true?  
`r if( knitr::is_html_output() ) {longmcq( c(
  answer = "The standard error of the mean is 1.7889.",
  "The standard error of the mean is 8.",
  "Since the sample size is less than 25, the standard error is not statistically valid.") )}`
`r if( !knitr::is_html_output() ) {
'   * The standard error of the mean is $1.7889$.
   * The standard error of the mean is $8$.
   * Since $n$ is less than $25$, the standard error is not statistically valid.'}`
:::



## Exercises {#OneMeanConfIntervalExercises}

Selected answers are available in App.\ \@ref(Answers).


::: {.exercise #OneMeanCIBears}
A study of American black bears [@bartareau2017estimating] found the mean weight of the $n = 185$ male bears was  $\bar{x} = 84.9$\ kg, with a standard deviation of $s = 51.1$\ kg.

1. Write down the *parameter* of interest.
1. Compute the standard error of the mean.
1. Compute the approximate $95$%\ CI.
1. Write a conclusion.
1. Is the CI likely to be statistically valid?
:::


::: {.exercise #BagsCI}
A study of the school bags that $586$ children (in Grades 6--8 in Tabriz, Iran) take to school found the mean weight was $\bar{x} = 2.8$\ kg with a standard deviation of $s = 0.94$\ kg [@data:Dianat2014:schoolbags].

1. Write down the *parameter* of interest.
1. Compute the standard error of the mean.
1. Compute the approximate $95$%\ CI.
1. Write a conclusion.
1. Is the CI likely to be statistically valid?
:::


::: {.exercise #CIOneMeanLungCapacityInChildren}
A study of the lung capacity of children in East Boston [@data:Tager:FEV; @BIB:data:FEV] measured the forced expiratory volume (FEV) of children in the area.
The sample contained $n = 45$ eleven-year-old girls.
For these children, the mean lung capacity was $\bar{x} = 2.85$ litres and the standard deviation was $s = 0.43$ litres.
Find an approximate $95$%\ CI for the population mean lung capacity of eleven-year-old females from East Boston.
:::


::: {.exercise #CIOneMeanEnvironmentalPollution}
A study of lead smelter emissions near children's public playgrounds [@data:Taylor2013:Lead] found the mean lead concentration at one playground (Memorial Park, Port Pirie, in South Australia) to be $6956.41$ micrograms per square metre, with a standard deviation of $7571.74$ micrograms of lead per square metre, from a sample of $n = 58$ wipes taken over a seven-day period.
(As a reference, the Western Australian Government recommends a maximum of $400$ micrograms of lead per square metre.)

Find an approximate $95$%\ CI for the mean lead concentration at this playground.
Would these results apply to other playgrounds?
:::


::: {.exercise #CIOneMeanToothbrushing}
A study [@data:Macgregor1985:ToothbrushinghYoungAdults] of the brushing time for $60$ young adults (aged 18--22 years old) found the mean brushing time was $33.0$ seconds, with a standard deviation of $12.0$ seconds.
Find an approximate $95$%\ CI for the mean brushing time for young adults.
:::


::: {.exercise #CIOneMeanBloodLoss}
A study of paramedics [@data:Williams2007:BloodLoss] asked participants ($n = 199$) to estimate the amount of blood loss on four different surfaces.
When the actual amount of blood spill on concrete was $1000$\ ml, the mean guess was $846.4$\ ml (with a standard deviation of $651.1$\ ml).

1. What is the approximate $95$%\ CI for the mean guess of blood loss?
1. Are the participants good at estimating the amount of blood loss on concrete?
1. Is this CI likely to be valid?
:::


::: {.exercise #OneMeanCINHANESInterpret}
Using data from the NHANES study [@data:NHANES3], the approximate $95$%\ CI for the mean direct HDL cholesterol is $1.356$ to $1.374$\ mmol/L.
Which (if either) of these interpretations are acceptable?
Explain *why* are the other interpretations are incorrect.

1. In the *sample*, about $95$% of individuals have a direct HDL concentration between $1.356$ to $1.374$\ mmol/L.
1. In the *population*, about $95$% of individuals have a direct HDL concentration between $1.356$ to $1.374$\ mmol/L.
1. About $95$% of the *samples* are between $1.356$ to $1.374$\ mmol/L.
1. About $95$% of the *populations* are between $1.356$ to $1.374$\ mmol/L.
1. The *population* mean varies so that it is between $1.356$ to $1.374$\ mmol/L about $95$% of the time.
1. We are about $95$% sure that *sample* mean is between $1.356$ to $1.374$\ mmol/L.
1. It is plausible that the *sample* mean is between $1.356$ to $1.374$\ mmol/L.
:::


::: {.exercise #OneMeanStdError}
An article [@data:Grabosky2016:Trees] describes the diameter of *Quercus bicolor* trees planted in a lawn as having a mean of $25.8$\ cm, with a standard error of $0.64$\ cm, from a sample of $19$ trees.
Which (if any) of the following is correct?
  
1. About $95$% of the trees in the *sample* will have a diameter between $25.8 - (2\times 0.64)$ and $25.8 + (2\times 0.64)$ (based on using the $68$--$95$--$99.7$ rule).
1. About $95$% of these types of trees in the *population* will have a diameter between $25.8 - (2\times 0.64)$ and $25.8 + (2\times 0.64)$ (based on using the $68$--$95$--$99.7$ rule)?
:::


::: {.exercise #ChewingTime}
In a study of $n = 30$ five-year-old children [@watanabe1995estimation], the mean time for the children to eat a cookie was $61.3$\ s, with a standard deviation of $29.4$\ s.

1. What is an approximate $95$%\ CI for the population mean time for a five-year-old child to eat a cookie? 
2. Is the CI likely to be statistically valid?
:::




<!-- QUICK REVIEW ANSWERS -->
`r if (knitr::is_html_output()) '<!--'`
::: {.EOCanswerBox .EOCanswer data-latex="{iconmonstr-check-mark-14-240.png}"}
**Answers to in-chapter questions:**

- Sect. \ref{thinkBox:WiderNarrower}: A *wider* interval is needed to be *more* confident that the interval contains the population mean.

- \textbf{\textit{Quick Revision} questions:}
**1.** True
**2.** False.
**3.** Standard error is $1.7889$.
:::
`r if (knitr::is_html_output()) '-->'`

