# Tests for one proportion {#TestOneProportion}


<!-- Introductions; easier to separate by format -->
```{r, child = if (knitr::is_html_output()) {'./introductions/28Testing-OneProportion-HTML.Rmd'} else {'./introductions/28Testing-OneProportion-LaTeX.Rmd'}}
```



## Introduction: Rolling dice {#ProportionTestIntro}


<div style="float:right; width: 222x; border: 1px; padding:10px"><img src="Illustrations/LoadedDice.png" width="200px"/></div>


`r if (knitr::is_html_output()) '<!--'`
\begin{wrapfigure}{R}{.25\textwidth}
  \begin{center}
    \includegraphics[width=.20\textwidth]{Illustrations/LoadedDice.png}
  \end{center}
\end{wrapfigure}
`r if (knitr::is_html_output()) '-->'`


Once when in a toy store (for my children, of course...) I saw 'loaded dice' for sale.
The packaging claimed 'One loaded \& one normal'.
I bought two packs!
When I got home, I didn't know *which* die was 'loaded', and which was 'normal'.
How could I find out? 
I guess had to roll the dice...

If I rolled the *fair* die, I know what to expect: each face would appear about one-sixth of the time (using [classical probability](#ProbClassical)).
So I could roll both dice, and see how often a `r include_graphics("Dice/die1.png", dpi=1500)` (for example) actually appeared.
Using the [decision-making process](#DecisionMaking) discussed earlier, I could  decide which die was fair.


## Statistical hypotheses and notation: One proportion

The [decision-making process](#DecisionMaking) begins with making an assumption about the population; here, assume that the die is fair, and hence that the *population* proportion of rolling a `r include_graphics("Dice/die1.png", dpi=1500)` is $p = 1/6$, or approximately $p = 0.16667$.
Now, I can compare what I see in practice (the sample proportion, $\hat{p}$) to this expectation to determine if the die is fair.

However, even with a fair die, rolling a `r include_graphics("Dice/die1.png", dpi=1500)` *exactly* one-sixth of the time is unlikely. 
The value of $\hat{p}$ may be slightly higher than $1/6$, or slightly lower.
This variation in the sample proportion is [*sampling variation*](#SamplingVariation).

When the sample proportion of rolls that are `r include_graphics("Dice/die1.png", dpi=1500)` is not *exactly* $0.1666...$, two possibilities exist:

* The *sample* proportion is not exactly $p = 0.1666...$ due to sampling variation; or
* The *sample* proportion is not exactly $p = 0.1666...$ because the die is not fair.

These two possible explanations are called *statistical hypotheses*.
Formally, the two statistical hypotheses above are written:

* $H_0$: $p = 0.1666...$, the *null hypothesis*; and
* $H_1$: $p \ne 0.1666...$, the *alternative hypothesis*.

The alternative hypothesis is open to the value of $p$ being smaller *or* larger than $0.1666...$.
That is, two possibilities are considered: for this reason, this alternative hypothesis is called a *two-tailed* alternative hypothesis.
(An alternative hypothesis like $p > 0.1666...$ or $p < 0.1666...$ is a *one-tailed* hypothesis.)


## Sampling distribution: One proportion {#OnePropTestSamplingDist}

When the proportion of rolls that show a `r include_graphics("Dice/die1.png", dpi=1500)` really is $p = 0.1666...$, what values of the *sample* proportion are reasonable to expect, given sampling variation?
The answer depends on the sample size.
In *one* roll of a die, rolling a `r include_graphics("Dice/die1.png", dpi=1500)`, and hence finding a sample proportion of $\hat{p} = 1$, is not unreasonable.
However, in 20,000 rolls, a sample proportion of $\hat{p} = 1$ would be *incredibly* unlikely for a fair die.

Earlier (Sect. \@ref(SamplingDistributionKnownp)), we saw how to describe the [sampling distribution of a sample proportion](#def:SamplingDistProp).
For an assumed value of $p$, the sample proportion $\hat{p}$ is expected to vary from sample to sample, and this variation is described by

* an approximate normal distribution;
* centred around a mean of $p$;
* with a standard deviation (called the *standard error* of $\hat{p}$) of

\begin{equation}
   \text{s.e.}(\hat{p}) 
   = \sqrt{\frac{p \times (1 - p)}{n}},
   (\#eq:StdErrorPknownTest)
\end{equation}
when [certain conditions are met](#ValidityProportionsTest), where $n$ is the size of the sample.
This is the *sampling distribution of the sample proportion*.

I decide to use 100 rolls.
So, if $p$ really was $0.1666...$, and if [certain conditions are met](#ValidityProportionsTest), the possible values of the sample proportion that could be expected in practice can described using:

* An approximate normal distribution;
* With mean 0.1666...;
* With a standard deviation of 
  $\displaystyle
  \text{s.e.}(\hat{p}) 
  = \sqrt{\frac{p\times (1 - p)}{n}} 
  = \sqrt{\frac{0.1666... \times(1 - 0.1666...)}{100}} = 0.037267$.
  This is the standard deviation of all possible sample proportions.


::: {.importantBox .important data-latex="{iconmonstr-warning-8-240.png}"}
When computing the standard error for a proportion, take care!

* The formula for a confidence interval uses the **sample proportion** $\hat{p}$ (see Eq. \@ref(eq:StdErrorPknownTest)), since we only have sample information to work with when forming a confidence interval.
* The formula for a hypothesis test uses the **population proportion** $p$ from the null hypothesis (see Eq. \@ref(eq:StdErrorCI)), since hypothesis testing *assumes that the null hypothesis is true*, and hence the value of $p$ is known.
* In both cases, make sure you are using a *porportion* in the formula, not a *percentage* (i.e., using 0.16666 rather than 16.666%). 

Also: Don't forget to take the square root!
:::


A picture of this sampling distribution (Fig. \@ref(fig:RollsSixesSD)) shows how the *sample* proportion varies when $n = 100$, simply due to sampling variation, when $p = 0.1666...$.
A value of $\hat{p}$ larger than 0.25 looks unlikely; a value less than 0.10 also looks quite unlikely, but not impossible.
A value above 0.3, or lower than 0.05, looks almost impossible.


```{r, RollsSixesSD, fig.height=3.25, out.width='60%', fig.align="center", fig.cap="The sampling distribution, showing the distribution of the sample proportion of 1s when the population proportion is 0.1666..., in 50 die rolls"}
p <- 1/6
n <- 100

sep <- sqrt( p * (1 - p) / n )

out <- plotNormal(mu = p,
                  sd = sep,
                  round.dec = 3,
                  xlab = "Values of the sample proportion",
                  main = "Sampling distribution of the sample proportion\nin 100 rolls ")
```


In my 100 rolls of one die, I observed 41 that showed a `r include_graphics("Dice/die1.png", dpi=1500)`, a sample proportion of $\hat{p} = 41/100 = 0.41$.
From Fig. \@ref(fig:RollsSixesSD), this is practically impossible *if the die was fair*.
What I observed was almost impossible... but I really did observe it.
A reasonable conclusion is that the assumption I was making---that the die is fair---is not tenable.


## The test statistic: $z$-score {#OnePropTestStatistic}

One way to measure how far the sample proportion $\hat{p} = 0.41$ is from the population proportion $p = 0.166\dots$ in 100 rolls is to use a $z$-score, since the sampling distribution (Fig. \@ref(fig:RollsSixesSD)) has a normal distribution.
The $z$-score is

\begin{align*}
   z 
   = \frac{\text{sample statistic} - \text{population mean}}{\text{standard deviation of statistic}}
   &= \frac{\hat{p} - p }{\text{s.e.}(\hat{p})} \\
   &= \frac{0.41 - 0.1666...}{037267} = 6.53.
\end{align*}
(Remember that the standard deviation of the distribution in Fig. \@ref(fig:RollsSixesSD) is the the standard error: the amount of variation in the sample proportions.)
The observed sample proportion is more than six standard deviations from the mean, which is *highly unusual* according to the [68--95--99.7 rule](#def:EmpiricalRule).



## $P$-values: One proportion {#OnePropTestP}

The value of the $z$-score shows that the value of $\hat{p}$ is highly very unusual... but how unusual?
Quantifying *how* unusual is assessed more precisely using a $P$-value, which is used widely in scientific research. The $P$-value is a way of measuring how unusual an observation is, when $H_0$ is assumed to be true.

$P$-values can be approximated using the 68--95--99.7 rule with a diagram (Sect. \@ref(ApproxPProportion)).
In most cases, $P$-values are found using software (Sect. ???).


### Approximating $P$-values using the 68--95--99.7 rule {#ApproxPProportion}

$P$-values are the area **more extreme** than the calculated $z$-score in the normal distribution. 
For *two-tailed* $P$-values, the $P$-value is the combined area in the two tails; for *one-tailed* $P$-values, the $P$-value is the area in one tails.
For example:

* *If* the calculated $z$-score was $z = 1$, the two-tailed $P$-value would be the shaded area in Fig. \@ref(fig:OnePropTestP) (left panel):
  About 32%, based on the 68--95--99.7 rule; the $P$-value would be the same if $z = -1$.
  The *one-tailed* $P$-value would the the area in one-tail: 
  About 16%, based on the 68--95--99.7 rule.
* *If* the calculated $z$-score was $z = 2$, the two-tailed $P$-value would be the shaded area shown in Fig. \@ref(fig:OnePropTestP) (middle panel): 
  About 5%, based on the 68--95--99.7 rule; the $P$-value would be the same if $z = -2$.
  The *one-tailed* $P$-value would the the area in one-tail:
  About 2.5%, based on the 68--95--99.7 rule.

Clearly, from what the $P$-value means, a $P$-value is always between 0 and 1.


```{r, OnePropTestP, fig.cap="The two-tailed P-value is the combined area in the two tails of the distribution; left panel: when $z = 1$; right panel: when $z = 2$", fig.height = 2.5, out.width='85%', fig.width=7, fig.align="center"}
par(mfrow = c(1, 2),
    mar = c(4, 1, 4, 1) + 0.1)

out <- plotNormal(mu = 0,
           sd = 1,
           main = expression(The~italic(P)*"-value"~when~italic(z)==1),
           xlab = expression(italic(z)*"-score")
           )

shadeNormal(out$x, out$y,
            lo = -5, 
            hi = -1)
shadeNormal(out$x, out$y,
            lo = 1, 
            hi = 5)


out <- plotNormal(mu = 0,
           sd = 1,
           main = expression(The~italic(P)*"-value"~when~italic(z)==2),
           xlab = expression(italic(z)*"-score")
           )
shadeNormal(out$x, out$y,
            lo = -5, 
            hi = -2)
shadeNormal(out$x, out$y,
            lo = 2, 
            hi = 5)
```



A more precise $P$-value is found using the $z$-tables (App. \@ref(ZTablesOnlineBackwards)) to compute the area in the tails (see Sect. \@ref(Z-Score-Forestry)).
A $z$-score as large as 6.53 means that tail area is *very* small, and zero to four decimal places.



## Making decisions with $P$-values {#OnePropTestDecisions}

$P$-values tells us the probability of observing the sample statistic (or something even more extreme), assuming the null hypothesis is true.
In this context, the $P$-value tells us the probability of observing the value of $\hat{p}$ (or something more extreme), just through sampling variation (chance) if $p = 0.1666\dots$.

The $P$-value is a probability, albeit a probability of something quite specific, so it is a value between 0 and 1. 
Then `r if( knitr::is_html_output() ) {
   "(see Fig. \\@ref(fig:PvaluesAnimation)):"
}`
`r if( knitr::is_latex_output() ) {
   "(see Fig. \\@ref(fig:PvaluesBigSmall)):"
}`

* 'Big' $P$-values mean that the sample statistic (i.e., $\bar{p}$) could reasonably have occurred through sampling variation, if the assumption made about the parameter (stated in $H_0$) was true: 
   The data *do not* contradict the assumption in $H_0$.
* 'Small' $P$-values mean that the sample statistic (i.e., $\hat{p}$) is unlikely to have occurred through sampling variation, if the assumption made about the parameter (stated in $H_0$) was true: 
   The data *do* contradict the assumption.

What is meant by 'small' and 'big'? 
It is arbitrary: no definitive rules exist.
A $P$-value smaller than 1% (that is, smaller than 0.01) is usually considered 'small', and a $P$-value larger than 10% (that is, larger than 0.10) is usually considered 'big'.
Between the values of 1% and 10% is often a 'grey area', though a $P$-value less than 0.05 if often considered 'small'.


```{r PvaluesAnimation, animation.hook="gifski",  interval=0.20, echo=FALSE, fig.cap="The strength of evidence: P-values. As the $z$-score becomes larger, the $P$-value becomes smaller, and the evidence is greater to support the alternative hypothesis.", fig.height = 2.75, fig.align="center", dev=if (is_latex_output()){"pdf"}else{"png"}}
if (knitr::is_html_output()) {
  par( mar = c(0.1, 0.1, 0.1, 0.1) ) # Number of margin lines on each side

  zList <- c( seq(0.5,
                  1,
                  by = 0.1),
              seq(1, 3.5, 
                  by = 0.05) )
  pMeaning <- function(pValue){
    if (pValue > 0.10) Meaning <- "Insufficient"
    if ( (pValue >= 0.05)  & (pValue < 0.10)) Meaning <- "Slight"
    if ( (pValue >= 0.01)  & (pValue < 0.05)) Meaning <- "Moderate"
    if ( (pValue >= 0.001) & (pValue < 0.01)) Meaning <- "Strong"
    if (pValue < 0.001) Meaning <- "Very strong"
    Meaning
    }
  
  pColours <- viridis( length(zList), 
                       begin = 0.5 ,
                       end = 1,
                       option = "H")
  
  for (i in (1:length(zList))){
    zScore <- zList[i]
    pValue <- pnorm( -zScore )
    pValue2 <- ifelse( pValue < 0.001, 
                       "< 0.001",
                       round(pValue, 4) )
    
    
    plot.normZ(mu = 0,
               sd = 0,
               shade.lo.z = zScore,
               shade.hi.z = 10,
               shade.col = pColours[i],
               xlab = expression(italic(z)~"-score"),
               main = paste("Evidence to support alternative hypothesis:\n", 
                            pMeaning(pValue) ) 
               )
    plot.normZ(mu = 0,
               sd = 0,
               new = FALSE,
               shade.lo.z = -10,
               shade.hi.z = -zScore,
               shade.col = pColours[i],
               xlab = expression(italic(z)~"-score"),
               main = paste("Meaning:", 
                            pMeaning(pValue) ) 
               )
    abline(v = zScore,
           col = "grey")
    abline(v = -zScore,
           col = "grey")
    
    polygon(x = c(-1.4, -1.4, 1.4, 1.4),
            y = c(0.02, 0.10, 0.10, 0.02),
            border = NA,
            col = "white")
    text(0,
         y = 0.06,
         label = paste("Two-tailed P-value:", pValue2 ) )
  }  
  
}

```


```{r PvaluesBigSmall, echo=FALSE, fig.cap="The strength of evidence: P-values. As the $z$-score becomes larger, the $P$-value becomes smaller, and the evidence is greater to support the alternative hypothesis.", fig.height = 3, fig.width=7, out.width='80%', fig.align="center", dev=if (is_latex_output()){"pdf"}else{"png"}}
if (knitr::is_latex_output()) {
  
  par(mfrow = c(1, 2) )
#  par( mar = c(0.1, 0.1, 0.1, 0.1) ) # Number of margin lines on each side

  zList <- c( 1.5, # Two-tailed P-value: 10% -1.645
              2.4 ) # Two-tailed P-value: 1% -2.576

  pMeaning <- function(pValue){
    if (pValue > 0.10) Meaning <- "Insufficient"
    if ( (pValue >= 0.05)  & (pValue < 0.10)) Meaning <- "Slight"
    if ( (pValue >= 0.01)  & (pValue < 0.05)) Meaning <- "Moderate"
    if ( (pValue >= 0.001) & (pValue < 0.01)) Meaning <- "Strong"
    if (pValue < 0.001) Meaning <- "Very strong"
    Meaning
    }
  
  pColours <- viridis( length(zList), 
                       begin = 0.5 ,
                       end = 1,
                       option = "H")
  
  for (i in (1:length(zList))){
    zScore <- zList[i]
    pValue <- pnorm( -zScore )
    pValue2 <- ifelse( pValue < 0.001, 
                       "< 0.001",
                       round(pValue, 4) )
    
    
    plot.normZ(mu = 0,
               sd = 0,
               shade.lo.z = zScore,
               shade.hi.z = 10,
               shade.col = pColours[i],
               xlab = expression(italic(z)~"-score"),
               main = paste("Evidence to support alternative\nhypothesis:", 
                            pMeaning(pValue) ) 
               )
    plot.normZ(mu = 0,
               sd = 0,
               new = FALSE,
               shade.lo.z = -10,
               shade.hi.z = -zScore,
               shade.col = pColours[i],
               xlab = expression(italic(z)~"-score"),
               main = paste("Meaning:", 
                            pMeaning(pValue) ) 
               )
    abline(v = zScore,
           col = "grey")
    abline(v = -zScore,
           col = "grey")
    
    polygon(x = c(-3, -3, 3, 3),
            y = c(0.12, 0.20, 0.20, 0.12),
            border = NA,
            col = rgb(255, 255, 255, max = 255, alpha = 200) ) # Translucent white
    text(0,
         y = 0.16,
         label = paste("Two-tailed P-value:", pValue2 ) )
  }  
  
}

```

## Communicating results: One proportion {#OnePropTestCommunicate}

In general, to communicate the results of any hypothesis test, report:

* An answer to the RQ, in terms of how strong the evidence is to support the *alternative* hypothesis; 
* The evidence used to reach that conclusion (such as the $z$-score and $P$-value, including if it is a one- or two-tailed $P$-value); and
* Sample summary information, including a CI, summarising the data used to make the decision.

So write:

> The sample provides very strong evidence ($z = 6.53$; two-tailed $P < 0.001$) that the proportion of sixes is not $1/6$ ($n = 100$ rolls; 41 sixes) in the population.

The components are:

* An answer to the RQ: 'The sample provides very strong evidence... that the population proportion is not $1/6$'; notice the wording states how much evidence exists in the sample to support the *alternative* hypothesis.
* The evidence used to reach the conclusion: '$z = 6.53$; two-tailed $P < 0.001$)'.
* Some sample summary information (including a CI).

::: {.importantBox .important data-latex="{iconmonstr-warning-8-240.png}"}
Since the *null* hypothesis is assumed to be true, the onus is on the evidence to refute the null hypothesis.
\smallskip
Hence, conclusions are worded in terms of how much evidence exists in the sample to support the alternative hypothesis. 
:::


## Hypothesis testing for one proportion: A summary {#OnePropTestSummary}

Let's recap the decision-making process, in this context about rolling a `r include_graphics("Dice/die1.png", dpi=1500)`:

1. **Assumption**: 
   Write the *null hypothesis* about the *parameter* (based on the RQ): $H_0$: $p = 01.666...$, and write the alternative hypothesis $H_1$: $p \ne 0.1666...$.
   (This is a two-tailed alternative hypothesis.)
2. **Expectation**: 
   The sampling distribution describes what values to expect from the sample statistic *if* the null hypothesis is true: under certain circumstances, the sample proportions will vary with an approximate normal distribution around a mean of $\mu = 0.1666...$ with a standard deviation of $\text{s.e.}(\hat{p}) = 0.0372678$.
3. **Observation**: 
   Compute the $z$-score: $z = 6.53$ to measure the distance between the assumed population value, and the observed sample value.
4. **Consistency?**: 
   Determine if the data are consistent with the assumption, by computing the $P$-value. 
  Here, the $P$-value is (much) less than $0.001$.
  The $P$-value can be computed by software, or approximated using the 68--95--99.7 rule.

The **conclusion** is that very strong evidence exists that $p$ is *not* $0.16667$, based on the evidence.


::: {.example #POnePropTestMeasles name="One sample proportion test"}
A study of the measles-rubella vaccination in Korea [@kim2004sero] compared the proportion of children with measles antibodies to the World Health Organization (WHO) target proportion (for children aged 5 to 9 years old: 10%).

In the study, 55 children out of 972 had the antibody present; that is, $\hat{p} = 55/972 = 0.056584...$.
Of course, every sample of 972 children would produce a different sample proportion (depending on which children were selected to be in the sample), so the difference between this sample proportion and the target proportion (of 10%, or $p = 0.10$) could be due to sampling variation.

To test if the proportion of Korean children with the measles antibody in the *population* was 10% or better (lower), the hypotheses are:

* $H_0$: $p = 0.10$ (assume the target is met, and the difference between $p$ and $\hat{p}$ is due to sampling variation); and
* $H_1$: $p < 0.10$ (one-tailed, since the RQ is whether the target is 10% or *lower*).

The *standard error* for the sample proportion is

\[
   \text{s.e.}(\hat{p}) 
   = \sqrt{\frac{p (1 - p)}{n}}
   = 0.0096225...
\]
The *test statistic* is:

\[
   z 
   = \frac{\hat{p} - p}{\text{s.e.}(p)}
   = \frac{0.056584 - 0.10}{0.0096225} = -4.51.
\]
This is a *very* large (and *negative*) $z$-score, so expect a *very* small $P$-value from using the 68--95--99.7 rule: there is very strong evidence to support the alternative hypothesis.
We write:

> Very strong evidence exists in the sample that the population proportion is less than the target of $p = 0.10$ (Korean sample proportion: $\hat{p} = 0.0566$; $n = 972$; approximate 95% CI from $0.042$ to $0.071$).
:::


## Statistical validity conditions: One proportion {#ValidityProportionsTest}

All inference procedures have underlying [conditions to be met](#exm:StatisticalValidityAnalogy) so that the results are statistically valid; that is, the $P$-values can be found accurately because the sampling distribution is an approximate normal distribution.
For a hypothesis test for one proportion, these conditions are similar to those for the [CI for one proportion](#ValidityProportions).

The *statistical validity conditions* for a test for a single proportion is that the *expected* number of individuals in the group of interest (i.e, $n\times p$) and in the group *not* of interest (i.e., $n\times (1 - p)$ both exceed five; that is:

* $n\times p > 5$, **and** $n\times (1 - p) > 5$.

The value of 5 here is a rough figure here, and some books give other values (such as 10 or 15).
This condition ensures that the *distribution of the sample proportions has an approximate normal distribution* so that the [68--95--99.7 rule](#def:EmpiricalRule) can be used.

In addition to the statistical validity condition, the test will be

* [**internally valid**](#def:InternalValidity) if the study was well designed; and
* [**externally valid**](#def:ExternalValidity) if the sample is a [simple random sample](#SRS) and is internally valid.


::: {.example #StatisticalValidityDice name="Statistical validity"}
The hypothesis test regarding the dice is statistically valid, since  $n\times p = 100 \times (1/6) = 16.666\dots$ and $n\times (1 - p) = 83.333\dots$, so *both* exceed five.
:::


::: {.example #StatisticalValidityMeasles name="Statistical validity"}
The hypothesis test regarding measles in Korea (Example \@ref(exm:POnePropTestMeasles)) is statistically valid, since  $n\times p = 972 \times 0.10 = 97.2$ and $n\times (1 - p) = 874.8$, so *both* exceed five.
:::



## Example: Dominance of birds

A study [@barve2017elevational] compared two types of birds (male green-backed tits; male cinereous tits) to see which was more behaviourally dominant over winter.
If the species were equally-dominant, then about 50% of the interactions would be won by each species (i.e., $p = 0.50$).
However, in the 45 interactions observed between the two species, green-backed tits won 37 of these interactions (i.e., $\hat{p} = 0.82222$).

Of course, every sample of 45 interactions would produce a different sample proportion, so the difference between this sample proportion and $p = 0.5$ could be due to sampling variation.
To test if the proportion of interactions were equally shared, the hypotheses are:

\[
   \text{$H_0$: } p = 0.5\quad\text{and}\quad\text{$H_1$: } p \ne 0.5 \text{ (two-tailed)}.
\]
The test will be statistically valid, since $n\times p = 45\times 0.5 = 22.5$ and $n\times (1 - p) = 22.5$ both exceed five.
The *standard error* for the sample proportion is

\[
   \text{s.e.}(\hat{p}) 
   = \sqrt{\frac{p (1 - p)}{n}} 
   = \sqrt{\frac{0.50 \times (1 - 0.50)}{45}} 
   = 0.0745356...
\]
Then, the *test statistic* is:

\[
   z 
   = \frac{\hat{p} - p}{\text{s.e.}(p)}
   = \frac{0.82222 - 0.50}{0.0745356}
   = 4.322.
\]
This is a *very* large $z$-score, so expect a very small $P$-value from using the 68--95--99.7 rule.

The 95% CI for the proportion requires the standard error computed using the *sample* proportion:
\[
   \text{s.e.}(\hat{p}) 
   = \sqrt{\frac{\hat{p} (1 - \hat{p})}{n}} 
   = \sqrt{\frac{0.82222 \times (1 - 0.82222)}{45}} 
   = 0.056999...
\]
So the approximate 95% CI is $0.82222 \pm(2 \times 0.056999...)$, or from 0.708 to 0.936.
We write:

> There is *very* strong evidence in the sample ($P < 0.001$; $z = 4.325$) that the interactions were not won equally between each species ($\hat{p} = 0.8222$ won by green-backed tits; $n = 45$; approximate 95% CI: 0.708 to 0.936) in the population.


## Example: Obesity

@kolanska2010high compared the rate of obesity in $n = 143$ Polish patients with adrenal tumours to that of the general population of Poland ($p = 0.125$).
The hypotheses are:

\[
   \text{$H_0$: } p = 0.125\quad\text{and}\quad\text{$H_1$: } p > 0.125\text{ (one-tailed)}.
\]
Assuming the null hypothesis is true, the standard error is (remembering to use $p$):

\[
   \text{s.e.}(\hat{p}) 
   = \sqrt{\frac{p (1 - p)}{n}} 
   = \sqrt{\frac{.125 \times (1 - 0.125)}{143}} 
   = 0.027656...
\]

In their sample, 57 were obese, so $\hat{p} = 57/143 = 0.3986...$.
Then, the *test statistic* is:

\[
   z 
   = \frac{\hat{p} - p}{\text{s.e.}(p)}
   = \frac{0.3986 - 0.125}{0.027656}
   = 9.89.
\]
This is an *extremely* large $z$-score, so expect a very small $P$-value using the 68--95--99.7 rule.

The 95% CI for the proportion requires the standard error computed from the *sample* proportion:
\[
   \text{s.e.}(\hat{p}) 
   = \sqrt{\frac{\hat{p} (1 - \hat{p})}{n}} 
   = \sqrt{\frac{0.3986 \times (1 - 0.3986)}{143}} 
   = 0.040943...
\]
The approximate 95% CI is $0.3986 \pm(2 \times 0.040943...)$.
We write:

> *Very* strong evidence exists in the sample (one-tailed $P < 0.001$; $z = 9.89$) that the rate of obesity in patients with adrenal tumours ($\hat{p} = 0.3986$; $n = 143$; approximate 95% CI: 0.317 to 0.480) is higher than the general Polish population.


NEED EXAMPLE WHERE P is 'large'.





## Summary {#Chapxx-Summary}

weded


## Quick review questions  {#Chapxx-QuickReview}

A study of diseases in native Americans [@kizer2006digestive] found 381 obese or overweight patients in 449 patients.
In the USA general population, the rate of Americans obese or overweight is 65%.
The researchers wanted to determine of the rate of obesity/overweight native Americans was *greater* than that of the general population.

1. True or false: The *population* proportion of overweight/obese native Americans is 0.65.
`r if( knitr::is_html_output(exclude = "epub") ) {torf(answer=TRUE)}`

1. True or false: The sample size is $n = 381$.
`r if( knitr::is_html_output(exclude = "epub") ) {torf(answer=FALSE)}`

1. The *sample* proportion $\hat{p}$ is (to *four* decimal places):
`r if( knitr::is_html_output(exclude = "epub") ) {
   fitb(num=TRUE, tol=0.0001, answer=0.84855)
} else {
   "________________"
}`

1. True or false: The *null* hypothesis is $H_0$: $p = 0.65$.
`r if( knitr::is_html_output(exclude = "epub") ) {torf(answer=TRUE)}`

1. True or false: The *alternative* hypothesis is *one*-tailed.
`r if( knitr::is_html_output(exclude = "epub") ) {torf(answer=TRUE)}`

1. True or false: To compute the standard error for the sample proportion, $\text{s.e.}(\hat{p})$, we use $\hat{p}$ in the formula.
`r if( knitr::is_html_output(exclude = "epub") ) {torf(answer=FALSE)}`

1. True or false: In a one-sample test of proportion, the $z$-score is always large.
`r if( knitr::is_html_output(exclude = "epub") ) {torf(answer=FALSE)}`

1. For this test, the computed $z$-score is (to *two* decimal places):
`r if( knitr::is_html_output(exclude = "epub") ) {
   fitb(num=TRUE, tol=0.005, answer=-8.82079)
} else {
   "________________"
}`

1. True or false? We accept the *null* hypothesis.
`r if( knitr::is_html_output(exclude = "epub") ) {torf(answer=FALSE)}`



`r if (!knitr::is_html_output()) '<!--'`
::: {.progressBox .progress}
**Progress:**  `r webexercises::total_correct()`
:::
`r if (!knitr::is_html_output()) '-->'`



## Exercises {#OneProportionTestExercises}

Selected answers are available in Sect. \@ref(TestOneProportionAnswer).


::: {.exercise #OneProportionTestExercisesPlacebos}
The study of herbal medicines is complicated as *blinding* subjects is difficult: placebos are often easily-identifiable by eye, by taste, or by smell.

One study [@loyeung2018experimental] examined if subjects could identify  potential placebos, performing *better* than just guessing.
The 81 subjects were each presented with a choice of five different supplements, and asked to select which one was the legitimate herbal supplement based on the *taste*.
Of these, 50 correctly selected the true herbal supplement.

1. If the subjects were selecting the true herbal supplement randomly, what proportion of subjects would be expected to select the correct supplement as the true herbal medicine?
2. Write the hypotheses for addressing the aims of the study.
3. Is this a one- or two-tailed test? 
   Explain.
4. Sketch the *sampling distribution* of the sample proportion, assuming the null hypothesis is correct.
5. Is there evidence to support the idea that people can identify the true supplement by taste?
:::


::: {.exercise #OneProportionTestExercisesEPL}
In the 2019/2020 English Premier League (EPL), at full-time the home team had won 91 out of 208 games, while the away team won 67.
(50 games were draws.)
(Data from: https://sports-statistics.com/sports-data/soccer-datasets/)

Ignoring draws, is there evidence of a home-side advantage; that is, that the home-side winning percentage is greater than 50%?
:::




::: {.exercise #OneProportionTestExercisesPedalMachines}
In a study to increase activity in library users [@maeda2013introducing], pedal machine were introduced on the first floor of Joyner Library at East Carolina University, where 60.2% of all students were females.
Students were observed using the machine on 589 occasions, of which 295 times were by females

Is there evidence that the proportion of females users of the machines was lower than the overall female proportion at the university?
What would you conclude?
:::


::: {.exercise #OneProportionTestExercisesCasinos}
In a 1995 study, 357 visitors to Las Vegas casinos 88 were smokers.
At the time, 25.5% of the general U.S. population were smokers (based on data from the U.S. National Center for Health Statistics).

Are casino-goers just as likely to be a smokers as the general U.S. population?
:::


:::{.exercise #OneProportionBreadfruitPasta}
Researchers developed a gluten-free pasta made from breadfruit [@nochera2019development].
In the study sample, 57 of the 71 participants stated that they liked the pasta.

Do the researchers have sufficient evidence to claim that the 'majority of people like breadfruit pasta'?
:::


::: {.exercise #OneProportionTestExercisesIguanas}
A study of black spiny-tailed iguanas in Florida (an invasive species) compared the snout-vent length (SVL) for iguanas of various sizes [@avery2014invasive].
275 iguanas with a SVL between 100 and 149mm were found in the study, of which 146 were female.

Assuming female and male iguanas were equally present in the population, is there evidence that female and male iguanas were equally-likely to found with SVL in this range?
:::


::: {.exercise #OneProportionTestExercisesCTS}
Carpal Tunnel Syndrome (CTS) is a illness in the wrists.
A study [@boltuch2020palmaris] was interested in whether 

> ...a relationship exists between the palmaris tendon [and] carpal tunnel syndrome (CTS)
>
> --- @boltuch2020palmaris, p. 493.

The palmaris longus tendon is visually *present* (PLA: wrong!!! pla is "absent" so check) in about 15% of the population.

The researchers found PLA in 33 of 516 CTS wrists (that 6.4%) in their sample.

A one-proportion $z$-test was used as it compares an observed proportion to a theoretical one when two categories are being investigated. 
Thus, is was used to compare the 6.4% rate of PLA in the operative cohort compared to the expected 15% rate of PLA from the population matched cohort. 
:::


::: {.exercise #OneProportionTestExercisesStars}
"B&CE Insurance, which analysed 458 personal accident claims, said Geminis accounted for 12 per cent of all the claims it received." [https://www.scotsman.com/arts-and-culture/geminis-branded-most-accident-prone-star-sign-2507417]

(May 22 to June 31: 31 days out of 365 in 2003 when data reported)

So, $n = 458$ and $\hat{p} = 0.12$.
With $p = 1/12 = 0.08333$ we get sep =  0.01518446 and z = 2.41.
:::


::: {.exercise #OneProportionTestExercisesBorers}
In a study of [@siegfried2014estimating] resistance of some commercial corn variety to the European corn borer, borers were collected from corn in Iowa and Nebraska.

Researchers aimed to estimate the frequency of resistance to the toxin in the corn, by mating borers collected from the field with various resistant laboratory individuals.
By doing so, they could determine what proportion of resistant individuals to expect in the second generation offspring.

In one study of $n = 172$ second-generation individuals, 24 were found to be resistant. 
The expectation was that 1-in-16 would be resistant if the field borers were resistant.

(NOT SURE ABOUT THAT!)

Perform a hypothesis test to determine if the data suggest that the borers were resistant (that is, if the population proportion is $1/16$) as expected.
:::


::: {.exercise #OneProportionTestExercisesLEDlights}
In a study of streetlight preferences of drivers [@davidovic2019drivers], drivers were asked to conduct a series of manoeuvres under 3000K LED light and then under 4000K LED lights.
They were then asked to decide which streetlight they preferred.

Out of the 52 subjects, 29 preferred the 3000K LED lights.
Is there evidence that the choice between the two streetlights is random, or is there evidence of a preference for one over the other? 
:::



::: {.exercise #OneProportionTestExercisesExam}
Something about a MC exam question with five option (one correct); better than guessing?
:::


::: {.exercise #OneProportionTestExercisesPenguins}
A study of Magellanic penguins [@vanstreels2013female] found dead or stranded on the southern Brazilian coast found 73 adult penguins.
Of these, 47 were female,

Assuming female and male penguins were equally present in the population, we would expect about half the dead or stranded penguins to be female and male.
Is this what the data suggest?
:::



<!-- QUICK REVIEW ANSWERS -->
`r if (knitr::is_html_output()) '<!--'`
::: {.QRanswerBox .QRanswer data-latex="{iconmonstr-check-mark-14-240.png}"}
\textbf{Answers to \textit{Quick Revision} questions:}
**1.** True.
**2.** False.
**3.** 0.84855.
**4.** True.
**5.** True.
**6.** False.
**7.** False.
**8.** $z = -8.82079$.
**9.** False.
:::
`r if (knitr::is_html_output()) '-->'`

