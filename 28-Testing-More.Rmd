
# More about hypothesis testing {#MoreAboutTests}

```{block2, type="rmdobjectives"}
So far,
you have learnt to
ask a RQ, 
identify different ways of obtaining data,
design the study,
collect the data
describe the data,
summarise data graphically and numerically,
understand the tools of inference,
and 
to form *confidence intervals*.

**In this chapter**,  
you will learn about *hypothesis tests*.
You will learn to:
  
* communicate the results of hypothesis tests.
* interpret $P$-values.

```





```{r echo=FALSE, fig.cap="", fig.align="center", fig.width=3, out.width="35%"}
SixSteps(5, "Tests")
```







## Introduction {#Chap28-Intro}

In Chap. \@ref(TestOneMean),
hypothesis tests for one mean were studied.
In later chapters,
hypothesis tests are discussed in other contexts, too.

The general approach to hypothesis testing 
is the same for any hypothesis test,
and so some general ideas are discussed in this chapter.
All hypothesis tests answer questions 
about unknown *population* quantities
(such as the population mean $\mu$),
based on *sample* statistics 
(such as the sample mean $\bar{x}$).

The sections that follow
discuss:

* The **assumptions** and forming hypotheses (Sect. \@ref(AboutHypotheses)).
* The sampling distribution, and the **expectations** (Sect. \@ref(SamplingDistributionsExpectation)).
* The **observations** and the test statistic (Sect. \@ref(TestStatistic)).
* Weighing the evidence for **consistency**: $P$-values (Sect. \@ref(AboutPvalues)).
* Wording **conclusions** (Sect. \@ref(WordingConclusion)).


When raw data are provided,
begin by producing graphical and numerical summaries of the data.
The statistical validity conditions,
which vary for different hypothesis tests,
should always be checked to see if the test is statistically valid.







## About hypotheses and assumptions {#AboutHypotheses}

Two hypotheses  are made about the population parameter:

* [The null hypothesis $H_0$](#HypothesisNull); and
* [The alternative hypothesis $H_1$](#HypothesisAlternative).





### Null hypotheses {#HypothesisNull}

Hypotheses *always concern a population parameter*.
Hypothesising,
for example,
that the *sample* mean body temperature 
is equal to  $37.0^\circ\text{C}$ is pointless,
because it clearly isn't:
the sample mean is  $36.8051^\circ\text{C}$.
Besides,
the RQ is about the unknown *population*:
the **P** in **P**OCI stands for **P**opulation.

The **null hypothesis** $H_0$
offers one possible reason why the value of the sample statistic 
(such as the sample mean)
is not the same as the value of the proposed population parameter 
(such as the population mean): 
*sampling variation*.
Every sample is different,
and so the *sample statistic* will vary from sample to sample;
it may not be equal to the *population parameter*, 
just 
because of the sample used by chance.
Null hypotheses always have an 'equals' in them
(for example,
the population mean *equals* 100, 
is *less than or equal to* 100, 
or is *more than or equal to* 100),
because 
(as part of the [decision making process](#DecisionMaking)),
something specific must be assumed for the population parameter.

The parameter can take many different forms,
depending on the context.
The null hypothesis about the parameter
is the *default* value of that parameter;
for example,

* there is *no difference* between the parameter value 
  in two (or more) groups;
* there is *no change* in the parameter value; or
* there is *no relationship* as measured by a parameter value.



```{block2 type="rmdimportant"}
The null hypothesis is assumed to be true.
The onus is on the data to provide 
evidence to refute this default position.
```


```{definition NullHypothesis, name="Null hypothesis"}
The **null hypothesis** proposes that *sampling variation* explains
the difference between the proposed value of the parameter,
and the observed value of the statistic.
```


### Alternative hypotheses {#HypothesisAlternative}

The other hypothesis is called the 
**alternative hypothesis** $H_1$.
The alternative hypothesis offers another possible reason 
why the value of the sample statistic
(such as the sample mean) 
is not the same as the value of the 
proposed population parameter 
(such as the population mean).
The alternative hypothesis proposes that 
the value of the population parameter really is
not the value claimed in the null hypothesis.


```{definition AltHypothesis, name="Alternative hypothesis"}
The **alternative hypothesis** proposes that 
the difference between the proposed value of the parameter
and the observed value of the statistic
cannot be explained by *sampling variation*:
the proposed value of the parameter is probably not true.```
```


Alternative hypotheses can be *one-tailed* or *two-tailed*.
A *two*-tailed alternative hypothesis means,
for example,
that the population mean could be
either smaller *or* larger than what is claimed.
A *one*-tailed alternative hypothesis admits
only one of those two possibilities.
Most (but not all) hypothesis tests are two-tailed.

The decision about whether the alternative hypothesis 
is one- or two-tailed
is made by reading the RQ
(*not* by looking at the data).
Indeed,
*the RQ and hypotheses should 
(in principle)
be formed before the data are obtained*,
or at least before looking at the data 
if the data are already collected.

The ideas are the same
whether the alternative hypothesis is one- or two-tailed:
based on the data and the sample statistic,
a decision is to be made about whether 
the alternative hypotheses is supported by the data.


```{example AltHypothesisBodyTemp, name="Alternative hypotheses"}
For the body-temperature study,
the alternative hypothesis is 
*two-tailed*:\index{two-tailed alternative hypothesis}
The RQ asks if the population mean 
is $37.0^\circ\text{C}$ or *not*. 
That is,
two possibilities are considered: 
that $\mu$  could be either larger *or*
smaller than $37.0^\circ\text{C}$.

A *one-tailed alternative hypothesis*\index{one-tailed alternative hypothesis}
would be appropriate if the RQ was:
'Is the *population* mean internal body temperature *greater* than $37.0^\circ\text{C}$?',
or
Is the *population* mean internal body temperature *smaller* than $37.0^\circ\text{C}$?.
```


```{block2, type="rmdimportant"}
Important points about forming hypotheses:

* Hypotheses always concern a *population* parameter.
* Null hypotheses always contain an 'equals'.
* Alternative hypothesis are one-tailed or two-tailed, 
  depending on the RQ.
* Hypotheses emerge from the RQ (not the data):
  The RQ and the hypotheses could be written down 
  *before* collecting the data.

```






## About sampling distributions and expectations {#SamplingDistributionsExpectation}

The *sampling distribution*\index{sampling distribution}
describes, approximately, how the sample statistic (such as $\bar{x}$)
is likely to vary from sample to sample over many repeated samples,
when $H_0$ is true:
it describes the *sampling variation*.\index{sampling variation}
Under certain circumstances,
sampling distributions often have an approximate normal distribution,
which is the basis for computing $P$-values
(or approximating $P$-values using 
the [68--95--99.7 rule](#def:EmpiricalRule)).

When the sampling distribution is described by a normal distribution,
the *mean* of the normal distribution is the 
parameter value given in the *assumption* ($H_0$),
and the *standard deviation* of the normal distribution 
is called the *standard error*.

In some cases,
the sample statistic may not have a normal distribution, 
but a quantity easily derived from the sample statistic 
does have a normal distribution
(for example, 
the odds ratio^[In this case, 
the *logarithm* of the odds ratio has an approximate normal distribution.]).




## About observations and the test statistic {#TestStatistic}

The sampling distribution describes what values 
the sample statistic can reasonably be expected to have,
over many repeated samples.
Since the sampling distribution of the statistic
has an approximate normal distribution under certain conditions,
the observed value of the sample statistic 
can be expressed as a something like a $z$-score
(called a $t$-score when the *population* standard deviation is unknown).
In general, 
$t$-scores always have the same form:

\[
   \text{statistic} = 
   \frac{\text{sample statistic} - \text{assumed population parameter}}
        {\text{measure of variation of the sample statistic}}.
\]
The $t$-score here is the *test statistic*,\index{test statistic}
since it is based on sample data ('a statistic')
and used in a hypothesis test.



```{block2, type="rmdnote"}
A $t$-score is similar to a $z$-score;
both the $z$- and $t$-scores have the same form:
\[
   \frac{\text{sample value} - \text{population value}}
        {\text{measure of variation of the sample value}}.
\]

Then:

* If the 'sample value' refers to an *individual* observation $x$,
  the measure of variation is the standard deviation,
  because the standard deviation measures the 
  variation in the individual observations. 
* If the 'sample value' is a *sample statistic*, 
  the measure of variation is a *standard error*,
  because the standard deviation measures the variation 
  in the sample statistic. 

In both cases,
if the measure of variation uses a known *population* value, 
a $z$-score is found;
if the measure of variation uses a *sample* value, 
a $t$-score is found.
```


 


## About finding $P$-values {#AboutFindingPvalues}

As demonstrated in Sect. \@ref(ApproxP),
often $P$-values can be *approximated* by using the 
the [68--95--99.7 rule](#def:EmpiricalRule) and using a diagram of a normal distribution.
The $P$-value is 
the area *more extreme* than the calculated $t$-score;
the [68--95--99.7 rule](#def:EmpiricalRule) 
can be used to approximate this tail area.

For **two-tailed** tests,
the $P$-value is the *combined* area in the left and right tails.
For **one-tailed** tests,
the $P$-value is the area in just the left or right tail.


```{block2, type="rmdimportant"}
When software reports *two-tailed $P$-values*,
a one-tailed $P$ is found by 
*halving the two-tailed $P$-value*.
```


More accurate estimates of the $P$-value can be found using
$z$-tables, though we do not demonstrate this in this book.
Even more precise estimates of $P$-values can be found using 
specially-prepared $t$-tables.
Again, we do not do so in this book.

For more precise $P$-values,
we will take the $P$-values from software output.

```{block2, type="rmdspss"}
When using software to obtain $P$-values,
be sure to check if the software reports one- or two-tailed $P$-values.

For example, some software (such as SPSS) always reports two-tailed $P$-values.
```







## About interpreting $P$-values {#AboutPvalues}

A $P$-value is the likelihood of observing the sample statistic
(or something even more extreme) over repeated sampling,
under the assumption that the null hypothesis 
about the population parameter is true.
$P$-values can be computed because the 
sampling distribution often has an approximate normal distribution.

```{block2, type="rmdimportant"}
Conclusion are **always** about the population values.

No-one needs $P$-values to see if the sample values are the same:
We can just look at them, and see.

$P$-values are needed to determine what we learn about the unknown **population** values,
based on what we see in the **sample** values.
```


Commonly,
a $P$-value smaller than 5% is considered 'small',
but this is *arbitrary*.
More reasonably,
$P$-values should be interpreted as giving varying degrees of evidence 
in support of the alternative hypothesis 
(Table \@ref(tab:PvaluesInterpretation2)),
but these are only guidelines.
Conclusions should be written in the context of the problem.
Sometimes,
authors will write that the results are 
'statistically significant'\index{statistically significant} 
when $P<0.05$.


```{definition Pvalue, name="$P$-value"}
A $P$-value is the likelihood of observing the sample statistic 
(or something more extreme) 
over repeated sampling, 
under the assumption that the null hypothesis 
about the population parameter is true.
```



```{r PvaluesInterpretation2, echo=FALSE}
    Meaning.latex <- c("\\emph{Insufficient} evidence to support $H_1$",
                "\\emph{Slight} evidence to support $H_1$",
                "\\emph{Moderate} evidence to support $H_1$",
                "\\emph{Strong} evidence to support $H_1$",
                "\\emph{Very strong} evidence to support $H_1$")
Meaning.html <- c("*Insufficient* evidence to support $H_1$",
                "*Slight* evidence to support $H_1$",
                "*Moderate* evidence to support $H_1$",
                "*Strong* evidence to support $H_1$",
                "*Very strong* evidence to support $H_1$")
Pvals <- c(0.5, 0.075, 0.025, 0.005, 0)
#kable(
#      format="html",
#      booktabs=TRUE,
#      longtable=FALSE,
#      )
if( knitr::is_html_output() ) {
PVTable <- data.frame( 
  Value =c("Larger than 0.10", 
           "Between 0.05 and 0.10", 
           "Between 0.01 and 0.05",
           "Between 0.001 and 0.01",
           "Smaller than 0.001"),
	Meaning = Meaning.html
)

PVTable %>%
 mutate_if(is.numeric, function(x) {
    cell_spec(x,
              color = spec_color(x, end = 0.9),
              font_size = spec_font_size(x))
  }) %>%
  mutate(Meaning = cell_spec(
    Meaning, color = "white", bold = T,
    background = spec_color(1:5, begin=0.2, end = 0.9, option = "D", direction = -1) # FOR COLORS, see  https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html
  )) %>%
  kable(escape = FALSE, 
        align = c("r", "l"), 
        caption="A guideline for interpreting $P$-values. $P$-values should be interpreted in context.", 
        col.names=c("If the $P$-value is...", "Write the conclusion as...")) %>%
  kable_styling(c("striped", "condensed"), full_width = F)
}

if( knitr::is_latex_output() ) {
PVTable <- data.frame( 
  Value =c("Larger than 0.10", 
           "Between 0.05 and 0.10", 
           "Between 0.01 and 0.05",
           "Between 0.001 and 0.01",
           "Smaller than 0.001"),
	Meaning = Meaning.latex
)

kable(PVTable,
      format="latex",
      booktabs=TRUE,
      escape=FALSE,
      longtable=FALSE,
      align = c("r", "l"),
      col.names=c("If the $P$-value is...", "Write the conclusion as..."),
      caption="A guideline for interpreting \\(P\\)-values. \\(P\\)-values should be interpreted in context.") %>%
  kable_styling(font_size=10)
}

```


```{block2, type="rmdimportant"}
$P$-values are never exactly zero.
When SPSS reports that '$P=0.000$',
it means that the $P$-value is less than 0.001,
which we write as '$P<0.001$'.

jamovi usually reports very small $P$-values as '$P<0.001$'.
```


$P$-values are commonly used in research,
but they need to be used
and interpreted correctly
[@greenland2016statistical].
Specifically:

* A $P$-value **is not** 
  the probability that the null hypothesis is true.
* A $P$-value **does not** *prove* anything.
* A big $P$-value **does not** 
  mean that the null hypothesis $H_0$ is true, or that $H_1$ is false.
* A small $P$-value **does not** 
  mean that the null hypothesis $H_0$ is false, or that $H_0$ is true.
* A small $P$-value **does not** indicate that 
  the results are practically important
  (Sect. \@ref(PracticalSignificance)).
* A small $P$-value does not mean a large difference 
  between the statistic and parameter; 
  it means that the difference
  could not reasonably be attributed to *sampling variation* (chance). 





## About writing conclusions {#WordingConclusion}

When reporting a conclusion, 
three things should be included:

1. The *answer to the RQ*;
2. The *evidence* used to reach that conclusion 
   (such as the $t$-score and $P$-value, 
   clarifying if the $P$-value is *one-tailed* or *two-tailed*); and
3. Some *sample summary statistics* 
   (such as sample means and sample sizes), 
   including a CI 
   (which indicates the precision with which the statistic has been 
   estimated).

Conclusions can never be made with *certainty*
from one sample.
Partly this is because a *sample* has been studied,
while the RQ asks about the whole *population*:
The entire population wasn't studied.

For this reason,
care must be taken when answering the RQ.
A hypothesis test *never* *proves*. anything:
It might conclude that evidence exists
(perhaps weak evidence; perhaps strong evidence)
to support the alternative hypothesis.
Of course,
there may be no evidence to support the alternative hypothesis either.

Since the value of the parameter in the null hypothesis is assumed true,
the onus is on the data to provide evidence
to refute this default position.
For this reason,
*conclusions are worded in terms of the 
level of support for the alternative hypothesis*.



```{block2, type="rmdimportant"}
Conclusions are always made in terms of 
how much evidence supports the *alternative* hypothesis.
Hypothesis tests assume the null hypothesis is true,
so the onus is on the data to provide evidence 
in support of the alternative hypothesis.
```


```{lemma Conclusions, name="Conclusions"}
What is wrong with the following conclusion?

> The evidence proves that the mean internal body temperature has changed.

```




## About practical importance and statistical significance {#PracticalSignificance}

Hypothesis tests assess 
*statistical significance*,\index{statistical significance}
which answers the question:
'Is there evidence of a difference between 
the value of the statistic and the value of the assumed parameter?'.
Even very small differences 
between the sample statistic and the population parameter
can be *statistically* different if the sample size is large enough. 

In contrast,
*practical importance*\index{practical importance}
asks the question:

> Is the difference between 
> the value of the statistic and the value of the assumed parameter
> of any *practical* importance?

'Practical importance' and 'statistical significance' 
are two separate (but both important) issues.
Whether a results is of practical importance depends
upon the context:
what the data are being used for, by whom, and for what purpose.


```{example PracticalImportance, name="Practical importance"}
In the body-temperature study,
very strong evidence exists that the mean body temperature had changed
('statistical significance').

But the change was so small,
that for most purposes it has no practical importance.
(There may be other (e.g., medical) situations 
where it *does* have practical importance however.)
```


```{block2, type="rmdtip"}
*Practical importance*\index{practical importance}
depends on the context in which the results will be used.
```


```{example PracticalImportanceHerbal, name="Practical importance"}
A study of some herbal medicines [@maunder2020effectiveness]
for weight loss found:

> *Phaseolus vulgaris* resulted in a statistically significant
> weight loss compared to placebo, 
> although this was not considered clinically significant. 

In other words,
although the difference in weight loss between placebo and *Phaseolus vulgaris*
was unikely to be explained by chance ($P<0.001$, which is 'statistical significant'),
the difference was so small in size (a mean weight loss of just 1.61&nbsp;kg) 
that it was unlikely to be of any use in practice
('practical importance').

In this context,
a weight loss of at least 2.5&nbsp;kg was considered to be of practical importance.
```




## Validity and hypothesis testing {#ValidityHTs}

When performing hypothesis tests,
certain *statistical validity conditions* must be true.
These conditions ensure that the sampling distribution is sufficiently
close to a normal distribution for the
[68--95--99.7 rule](#def:EmpiricalRule) rule to apply
and hence for $P$-values to be computed^[Not all sample statistics have normal distributions, but all the sample statistics in this book are either normally distributed or are closely related to normal distributions.].

If these conditions are *not* met,
the sampling distribution may not be normally distributed,
so the $P$-values (and hence conclusions)
maybe inappropriate.

In addition to the statistical validity condition,
the *internal validity* and 
*external validity* of the study should be discussed also
(Fig.&nbsp;\@ref(fig:ValiditiesHT)).
These are usually (but not always) the same as for CIs 
(Sect. \@ref(ValidityCIs)).

Regarding *external validity*,
all the computations in this book 
assume a *simple random sample*.
If the sample is from a [*random* sampling method](#RandomSamples), 
but not from a [*simple random sample*](#SRS),
then methods exist for conducting hypothesis tests that are externally valid,
but are more complicated than those described in this book.

If the sample is a [non-random sample](#NonRandomSamples),
then the hypothesis test may be reasonable for the quite specific population 
that *is* represented by the sample;
however,
the sample probably does not represent 
the more general population that is probably intended.

*Externally validity* requires that 
a study is also internally valid.
*Internal validity* can only be discussed
if details are known about the study design.


```{r ValiditiesHT, echo=FALSE, fig.cap="Three types of validities for studies.", fig.align="center", fig.height=4, out.width="90%"}
par( mar=c(0.15, 0.15, 0.15, 0.15))
openplotmat()

pos <- array(NA, dim=c(9, 2))
pos[1, ] <- c(0.30, 0.90) # External
pos[2, ] <- c(0.60, 0.90) # Internal
pos[3, ] <- c(0.90, 0.90)   # Statistical
pos[4, ] <- c(0.30, 0.50)   # EX: conditions
pos[5, ] <- c(0.60, 0.50)   # IN: conditions
pos[6, ] <- c(0.90, 0.50)   # ST: conditions
pos[7, ] <- c(0.30, 0.08)   # EX: upshot
pos[8, ] <- c(0.60, 0.08)   # IN: upshot
pos[9, ] <- c(0.90, 0.08)   # ST: upshot

textplain( mid = c(0.13, pos[1, 2]),
           lab = "Type:",
           adj = c(0.5, 1))
textplain( mid = c(0.13, pos[4, 2]),
           lab = "Condition:",
           adj = c(0.5, 1))
textplain( mid = c(0.13, pos[7, 2]),
           lab = "Implication:",
           adj = c(0.5, 1))


straightarrow(from = pos[1, ], to = pos[4, ], 
              lty = 1, 
              lcol = "black",
              lwd = 2)
straightarrow(from = pos[2, ], to = pos[5, ], 
              lwd = 2,
              lcol = "black",
              lty = 1)
straightarrow(from = pos[3, ], to = pos[6, ], 
              lwd = 2,
              lcol = "black",
              lty = 1)


straightarrow(from = pos[4, ], to = pos[7, ], 
              lty = 3, 
              lcol = "black",
              lwd = 2)
straightarrow(from = pos[5, ], to = pos[8, ], 
              lwd = 3,
              lcol = "black",
              lty = 3)
straightarrow(from = pos[6, ], to = pos[9, ], 
              lwd = 2,
              lcol = "black",
              lty = 3)


#straightarrow(from = pos[1, ], to = pos[2, ], 
#              lwd = 2,
#              code = 3,
#              lcol = "black",
#              lty = 3)


textrect( pos[1, ], 
           lab = "External\n validity", 
           radx = 0.09, 
           rady = 0.125, 
           shadow.size = 0,
           box.col = "aliceblue",
           lcol = "aliceblue")
textrect( pos[2, ], 
           lab = "Internal\n validity", 
           radx = 0.09, 
           rady = 0.125, 
           shadow.size = 0,
           box.col = "aliceblue",
           lcol = "aliceblue")
textrect( pos[3, ], 
           lab = "Statistical\n validity", 
           radx = 0.09, 
           rady = 0.125, 
           shadow.size = 0,
           box.col = "aliceblue",
           lcol = "aliceblue")




textrect( pos[4, ], 
           lab = "Random\n samples", 
           radx = 0.09, 
           rady = 0.125, 
           shadow.size = 0,
           box.col = "darkseagreen1",
           lcol = "darkseagreen1")
textrect( pos[5, ], 
           lab = "Study is\n well-designed", 
           radx = 0.09, 
           rady = 0.125, 
           shadow.size = 0,
           box.col = "darkseagreen1",
           lcol = "darkseagreen1")
textrect( pos[6, ], 
           lab = "Specific\n conditions", 
           radx = 0.09, 
           rady = 0.125, 
           shadow.size = 0,
           box.col = "darkseagreen1",
           lcol = "darkseagreen1")



textrect( pos[7, ], 
           lab = "Sample\n represents\n population", 
           radx = 0.09, 
           rady = 0.150, 
           adj = c(0.5, 0.25),
           shadow.size = 0,
           box.col = "mistyrose",
           lcol = "mistyrose")
textrect( pos[8, ], 
           lab = c("Within-sample\n conclusions\n are sound"), 
           radx = 0.09, 
           rady = 0.150, 
           adj = c(0.5, 0.25),
           shadow.size = 0,
           box.col = "mistyrose",
           lcol = "mistyrose")
textrect( pos[9, ], 
           lab = "Statistical\n methods\n appropriate", 
           radx = 0.10, 
           rady = 0.150, 
           adj = c(0.5, 0.25),
           shadow.size = 0,
           box.col = "mistyrose",
           lcol = "mistyrose")

```


In addition,
hypothesis tests also require that the sample size is less than 10% of the population size;
however this is almost always the case.





## Summary {#Chap28-Summary}

Hypothesis testing formalises the steps of the decision-making process.
Starting with an **assumption** about a population parameter of interest,
a description of what values the sample statistic might take 
(based on this assumption)
is produced:
this describes what values the statistic is **expected** to take, 
just through sampling variation.
This sampling distribution is often a normal distribution,
or related to a normal distribution.

The sample statistic (the *estimate*)
is then **observed**, 
and a *test statistic*, which often is a $t$-score, 
is computed to describe this sample statistic.
Using a $P$-value,
a decision is made about whether
the sample evidence supports or contradicts the initial assumption, 
and hence a **conclusion** is made.
Since $t$-scores are like $z$-scores,
$P$-values can often be approximated using the 68--95--99.7 rule.


<iframe src="https://usc.h5p.com/content/1291006940268248549/embed" width="1088" height="637" frameborder="0" allowfullscreen="allowfullscreen" allow="geolocation *; microphone *; camera *; midi *; encrypted-media *"></iframe><script src="https://usc.h5p.com/js/h5p-resizer.js" charset="UTF-8"></script>



## Quick review questions {#Chap28-QuickReview}

1. True or false? 
   When a $P$-value is very small, 
   a very large difference exists between the statistic and parameter.  
`r if( knitr::is_html_output() ) { torf( answer=FALSE )}`
1. True or false?
The alternative hypothesis is one-tailed
if the sample statistic is larger than the
hypothesised population mean.  
`r if( knitr::is_html_output() ) { torf( answer=FALSE )}`
1. What is wrong (if anything) with this null hypothesis: $H_0=37$?  
`r if( knitr::is_html_output() ) { mcq( c(
  "There is nothing wrong",
  answer = "There is no parameter",
  "This is the alternative (not the null) hypothesis") )}`
1. True or false: When the sampling distribution
   is a normal distribution,
   the standard deviation of this normal distribution
   is called the *standard error*.  
`r if( knitr::is_html_output() ) { torf( answer=TRUE )}`
1. True or false? 
   Both $z$-scores and $t$-scores *can* be test statistics.  
`r if( knitr::is_html_output() ) { torf( answer=TRUE )}`
1. True or false?  $P$-values can never be exactly zero.  
`r if( knitr::is_html_output() ) { torf( answer=TRUE )}`
1. True or false? A $P$-value is the probability
   that the null hypothesis is true.  
`r if( knitr::is_html_output() ) { torf( answer=FALSE )}`







## Exercises {#MoreAboutTestsExercises}

Selected answers are available in
Sect. \@ref(MoreAboutTestsAnswer).


```{exercise MoreAboutExercisesApproximatingPValues}

Use the [68--95--99.7 rule](#def:EmpiricalRule)
to approximate the *two*-tailed $P$-value if:

1. the $t$-score is $3.4$.
2. the $t$-score is $-2.9$.
3. the $t$-score is $1.2$.
4. the $t$-score is $-0.95$.
5. the $t$-score is $-0.2$.
6. the $t$-score is $6.7$.

```

```{exercise MoreAboutExercisesApproximatingPValuesOneTailed}

Consider the $t$-scores in 
Exercise \@ref(exr:MoreAboutExercisesApproximatingPValues).
Use the [68--95--99.7 rule](#def:EmpiricalRule)
to approximate the *one*-tailed $P$-values in each case.
```





```{exercise MoreAboutTestsInterpretingResults}
Suppose a hypothesis test results in a $P$-value of 0.0501.
What would we conclude?
What about if the $P$-value was 0.0499?
```


```{exercise MoreAboutTestsInterpretingHypotheses}
Consider again the study to determine the mean body temperature,
where $\bar{x} = 36.8051^{\circ}\text{C}$.
What, if anything, is wrong with these hypotheses?
Explain.

1. $H_0$: $\bar{x} = 36$ and $H_1$: $\bar{x} \ne 36$.
1. $H_0$: $\bar{x} = 36.8051$ and $H_1$: $\bar{x} > 36.8051$.
1. $H_0$: $\mu = 36.8051$ and $H_1$: $\mu \ne 36.8051$.
1. $H_0$: $\mu = 36$ and $H_1$: $\mu = 36.8051$.
1. $H_0$: $\mu > 36$ and $H_1$: $\bar{x} > 36$.
1. $H_0$: $\mu = 36$ and $H_1$: $\mu > 36$.

```







```{exercise MoreAboutTestsConclusions}
The recommended daily energy intake for women
is 7725kJ
(for a particular cohort, in a particular country;
@data:Altman1991:PracticalStats).
The daily energy intake for 11 women was measured
to see if this is being adhered to.
The RQ was

>	Is the population mean daily energy intake 7725kJ?

The test produced $P=0.018$.
What, if anything, is wrong with these conclusions after completing the hypothesis test?

1. There is moderate evidence ($P = 0.018$) that the energy intake is not meeting the recommended daily energy intake.
1. There is moderate evidence ($P = 0.018$) that the sample mean energy intake is not meeting the recommended daily energy intake.
1. There is moderate evidence ($P = 0.018$) that the population energy intake is not meeting the recommended daily energy intake.  

```






```{r echo=FALSE,cache=TRUE}
bt <- structure(list(Brand = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L), .Label = c("Energizer", "Ultracell"), class = "factor"), 
    Voltage = c(1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 
    1.2, 1.2, 1.2, 1.2, 1.2, 1.2, 1.2, 1.2, 1.2, 1.1, 1.1, 1.1, 
    1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
    0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.8, 0.8, 0.8, 
    0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 
    1.3, 1.3, 1.3, 1.2, 1.2, 1.2, 1.2, 1.2, 1.2, 1.2, 1.2, 1.2, 
    1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1, 1, 1, 1, 
    1, 1, 1, 1, 1, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 
    0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8), Time = c(1.4, 
    1.39, 1.35, 1.38, 1.35, 1.36, 1.31, 1.26, 1.37, 2.86, 2.77, 
    2.71, 2.81, 2.65, 2.73, 2.48, 2.44, 2.76, 5.71, 5.64, 5.63, 
    5.78, 5.63, 5.7, 4.65, 4.67, 5.57, 7.58, 7.46, 7.46, 7.59, 
    7.46, 7.52, 6.83, 6.89, 7.45, 8.45, 8.34, 8.35, 8.49, 8.33, 
    8.41, 7.88, 7.94, 8.32, 8.86, 8.65, 8.74, 8.91, 8.72, 8.85, 
    8.52, 8.62, 8.68, 1.56, 1.54, 1.53, 1.54, 1.54, 1.47, 1.49, 
    1.54, 1.5, 3.57, 3.55, 3.55, 3.54, 3.54, 3.51, 3.54, 3.54, 
    3.56, 5.76, 5.73, 5.74, 5.71, 5.72, 5.72, 5.71, 5.68, 5.74, 
    7.5, 7.48, 7.47, 7.48, 7.48, 7.41, 7.47, 6.96, 7.48, 8.35, 
    8.35, 8.32, 8.32, 8.31, 8.28, 7.99, 7.93, 8.34, 8.76, 8.81, 
    8.81, 8.7, 8.73, 8.76, 8.68, 8.64, 8.79), Battery = c(1L, 
    2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 
    8L, 9L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 1L, 2L, 3L, 4L, 
    5L, 6L, 7L, 8L, 9L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 1L, 
    2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 
    8L, 9L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 1L, 2L, 3L, 4L, 
    5L, 6L, 7L, 8L, 9L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 1L, 
    2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 
    8L, 9L)), .Names = c("Brand", "Voltage", "Time", "Battery"
), class = "data.frame", row.names = c(NA, -108L))

out <- t.test(Time ~ Brand, data=subset(bt, Voltage==1.1) )
```


```{exercise MoreAboutTestsBatteries}
A study compared ALDI batteries to another brand of battery.
In one test comparing the length of time it takes for 1.5 volt AA batteries to reach 1.1 volts,
the ALDI brand battery took 5.73 hours, and the other brand (Energizer) took 5.44 hours
[@mypapers:Dunn:BatteryData].

1. The $P$-value for comparing these two means is about $P=0.70$. What does this mean?
1. Is this difference likely to be of any practical importance? Explain.
1. What would be a useful, but correct, conclusion for ALDI to report from the study? Explain.
1. What else would be useful to know in comparing the two brands of batteries?

```



