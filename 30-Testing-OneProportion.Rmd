# Tests for one proportion {#TestOneProportion}


<!-- Introductions; easier to separate by format -->
```{r, child = if (knitr::is_html_output()) {'./introductions/30-Testing-OneProportion-HTML.Rmd'} else {'./introductions/30-Testing-OneProportion-LaTeX.Rmd'}}
```


<!-- Define colours as appropriate -->
```{r, child = if (knitr::is_html_output()) {'./children/coloursHTML.Rmd'} else {'./children/coloursLaTeX.Rmd'}}
```




## Introduction: rolling dice {#ProportionTestIntro}


<div style="float:right; width: 222x; border: 1px; padding:10px"><img src="OtherImages/SmiffyDice-OneLoaded.png" width="200px"/></div>


`r if (knitr::is_html_output()) '<!--'`
\begin{wrapfigure}[9]{R}{.29\textwidth}
  \centering%
  \vspace{-17pt}% This removes some white space
  \includegraphics[width=.27\textwidth]{OtherImages/SmiffyDice-OneLoaded.png}%
\end{wrapfigure}
`r if (knitr::is_html_output()) '-->'`


In a toy store one day (for my children, of course), I saw 'loaded dice for sale.
The packaging claimed <span style="font-variant:small-caps;">one loaded \& one normal</span>.
I bought two sets!
However, there was no indication as to *which* die was the loaded die.
So how could I determine which of the dice was 'loaded'?
I guess had to roll the dice.

Suppose I selected one die to roll.
If that die happened to be the fair die, I'd expect that each face would appear *approximately* (not exactly) one-sixth of the time (using classical probability; Sect.\ \@ref(ProbClassical)).
So, I could roll one die, and see how often (for example) a 
`r if (knitr::is_latex_output()) {
   '\\Largedice{1}'
} else {
   '<span class="larger-die">&#9856;</span>'
}`
actually appeared.
(I could have chosen any number on the die, of course.)
Then, using the decision-making process (Sect.\ \@ref(DecisionMaking)), I could decide if that die seemed to be the fair die.

For one specific die, I could ask the decision-making RQ:

> For this die, is the population proportion of rolls that show a
`r if (knitr::is_latex_output()) {
   '\\Largedice{1}'
} else {
   '<span class="larger-die">&#9856;</span>'
}`
equal to $1/6$?

Answering a decision-making RQ, such as this, requires a *hypothesis test*.


## Statistical hypotheses and notation

First, define $p$ as the *population* proportion of rolls that show a
`r if (knitr::is_latex_output()) {
   '\\Largedice{1};'
} else {
   '<span class="larger-die">&#9856;</span>;'
}`
that is, $p = 1/6$ *if* the die is a fair die (and we are not sure, of course).
Then, define $\hat{p}$ as the proportion of rolls in the *sample* that show a
`r if (knitr::is_latex_output()) {
   '\\Largedice{1}.'
} else {
   '<span class="larger-die">&#9856;</span>.'
}`

Even if the die was fair, and the value of $p$ really was $1/6$, the value of $\hat{p}$ would not necessarily be *exactly* $1/6$, due to *sampling variation*.
Sometimes the value of $\hat{p}$ would be a bit smaller than $1/6$, and sometimes a bit larger, even if the value $p$ really was $1/6$.

However, if I assume the value of $p$ is $1/6$, the possible values of the *sample* proportion from all possible rolls of the fair die could be described; that is, the *sampling distribution* could be described.
The sampling distribution would show what values of $\hat{p}$ could reasonably be expected from a die with $p = 1/6$.

Suppose we find that the value of $\hat{p}$ is not *exactly* $1/6$.
One of two explanations could explain why:

* The *population* proportion *really is* $p = 1/6$.  
  However, the value of $\hat{p}$ is not exactly $1/6$ due to sampling variation.
* The *population* proportion *really is not* $p = 1/6$.  
  That is, the value of $\hat{p}$ is not exactly $1/6$ because the die is not fair.

These two possible explanations are called *statistical hypotheses*.
The hypotheses above can be written as:

* $p = 1/6$, called the *null hypothesis* (denoted $H_0$); and
* $p \ne 1/6$, the called *alternative hypothesis* (denoted $H_1$, or sometimes $H_a$).

The hypotheses propose values for the unknown *population* proportion (the *parameter* $p$).


::: {.importantBox .important data-latex="{iconmonstr-warning-8-240.png}"}
The decision-making process begins by assuming the *null hypothesis* is true.
Thus, *the onus is on the data to refute the null hypothesis, the initial assumption*.

That is, the null hypothesis is retained unless compelling evidence emerges to change our mind.
:::


Here, the RQ here is open to the value of $p$ being smaller *or* larger than $1/6$; that is, two possibilities are considered.
Hence, we write $p\ne 1/6$, which is called a *two-tailed* alternative hypothesis.
An alternative hypothesis like $p > 1/6$ or $p < 1/6$ is a *one-tailed* hypothesis.


::: {.importantBox .important data-latex="{iconmonstr-warning-8-240.png}"}
The form of the alternative hypothesis (one- or two-tailed) depends on what the research question asks, *not the data*.
:::


## Sampling distribution of $\hat{p}$ {#OnePropTestSamplingDist}
\index{Sampling distribution!one proportion (test)}


As part of the decision-making process, *hypothesis testing always begins by assuming the null hypothesis is true*.
Here, that means initially assuming $p = 1/6$.
In Chap.\ \@ref(CIOneProportion), the sampling distribution of a sample proportion was given when the value of $p$ is known (Sect.\ \@ref(SamplingDistributionKnownp)).
If I decide to use $n = 100$ die rolls, for example, the sampling distribution can be described as:

* an approximate normal distribution,
* with mean whose value is $1/6$,
* with a standard deviation of 
  $\displaystyle \text{s.e.}(\hat{p}) 
  = \sqrt{\frac{ (1/6) \times \left(1 - (1/6)\right)}{100}} = 0.037267$ from Eq.\ \@ref(eq:StdErrorPknown).

Provided certain conditions are met (Sect.\ \@ref(ValidityProportionsTest)), this describes how the values of $\hat{p}$ would vary if $p$ really was $1/6$; see Fig.\ \@ref(fig:RollsSixesSD).


::: {.importantBox .important data-latex="{iconmonstr-warning-8-240.png}"}
The notation $\text{s.e.}(\hat{p})$ denotes the *standard error of the sample proportion*.
Its value is the standard deviation of the proportions computed from all possible samples of a given size\ $n$.
:::


The *mean* of this distribution (the *sampling mean*) is the *mean* of the values of $\hat{p}$ from all possible samples; the value of that mean is $p$.
Similarly, the standard deviation of this distribution is the *standard error*, denoted $\text{s.e.}(\hat{p})$, and is the standard deviation of all possible values of the statistic $\hat{p}$ (Fig.\ \@ref(fig:RollsSixesSD)).


::: {.importantBox .important data-latex="{iconmonstr-warning-8-240.png}"}
When computing the *standard error for a proportion*, take care!

* The standard error for a *confidence interval* uses the *sample proportion* $\hat{p}$ (see Eq.\ \@ref(eq:stderrorphat)), since we only have sample information when forming a CI.
* The standard error for a *hypothesis test* uses the *population proportion* $p$ from the null hypothesis (see Eq.\ \@ref(eq:StdErrorPknown)), since hypothesis testing *assumes the null hypothesis is true*, and hence the value of $p$ is known.

In both cases, use a *proportion* in the formula, not a *percentage* (i.e., $0.16666...$ rather than $16.666...$%). 
Don't forget to take the square root!
:::


Figure\ \@ref(fig:RollsSixesSD) shows how the *sample* proportion varies when $n = 100$ across all possible samples, simply due to sampling variation, when $p = 1/6 = 0.1666...$.
Values of $\hat{p}$ between about $0.13$ and $0.20$ would seem to occur reasonably frequently when $p = 1/6$.
Values of $\hat{p}$ larger than $0.25$ look unlikely when $n = 100$; values less than $0.10$ also appear unlikely, but not impossible.
A value above $0.30$ almost never occurs in any possible sample.


```{r, RollsSixesSD, fig.height=3.5, fig.width=8.75, out.width='100%', fig.align="center", fig.cap="The sampling distribution, showing the distribution of the sample proportion of ones when the population proportion is $1/6$, in $50$ die rolls"}
p <- 1/6
n <- 100
mu <- p

sep <- sqrt( p * (1 - p) / n )

x <- 0.41
z <- (x - mu)/sep

par( mar = c(5.1, 0.05, 4.1, 0.05) )

out <- plotNormal(mu = mu, 
                  sd = sep,
                  round.dec = 3,
                  xlim.hi = 0.45,
                  ylim = c(0, 16), 
                  showX = seq(-3, 7, by = 1) * sep + p, # The tick marks
                  xlab = "Values of the sample proportion",
                  main = "Sampling distribution of the sample proportion of ones\nin 100 rolls ")

arrows( y0 = 0.75 * max(out$y),
        x0 = x,
        y1 = 0,
        x1  = x,
        angle = 15,
        length = 0.1)
#text(y =  0.75 * max(out$y),
#     x = x,
#     cex = 0.8,
#     pos = 3,
#     labels = expression(italic(z)==6.53) )
text(y =  0.75 * max(out$y),
     x = x,
     cex = 0.9,
     pos = 3,
     labels = expression( atop(41~ones~"in"~100~rolls,
                               group("(",hat(italic(p))==0.41, ")" ) ) ) )



# Sampling mean
arrows(x0 = mu,
       x1 = mu,
       y0 = 13.3,
       y1 = max(out$y),
       angle = 15,
       length = 0.1)
text(y =  13,
     x = mu,
     cex = 0.9,
     pos = 3,
     labels = expression( Sampling~mean~italic(p) ) )

# Std error
arrows(x0 = mu,
       x1 = mu + sep,
       y0 = 1.25,
       y1 = 1.25,
       code = 3, # Arrows both ends
       angle = 15,
       length = 0.1)
text(y =  1.225,
     x = mu + (sep / 2),
     cex = 0.9,
     pos = 3,
     labels = expression( atop(Std.~error,
                               0.0373 ) ) )


mtext( expression( group( "(", italic(z)==-2, ")" ) ) ,
      side = 1,
      cex = 0.85,
      at = mu - 2*sep, 
      line = 2)
mtext( expression( group( "(", italic(z)==0, ")" ) ) ,
      side = 1,
      cex = 0.85,
      at = mu + 0*sep, 
      line = 2)
mtext( expression( group( "(", italic(z)==2, ")" ) ) ,
      side = 1,
      cex = 0.85,
      at = mu + 2*sep, 
      line = 2)
mtext( expression( group( "(", italic(z)==4, ")" ) ) ,
      side = 1,
      cex = 0.85,
      at = mu + 4*sep, 
      line = 2)
mtext( expression( group( "(", italic(z)==6, ")" ) ) ,
      side = 1,
      cex = 0.85,
      at = mu + 6*sep, 
      line = 2)
```


In my $100$ rolls of one die, $41$ showed a 
`r if (knitr::is_latex_output()) {
   '\\Largedice{1}'
} else {
   '<span class="larger-die">&#9856;</span>'
}`, 
so that $\hat{p} = 41/100 = 0.41$.
From Fig.\ \@ref(fig:RollsSixesSD), this is practically impossible *if the die was fair*: it basically never occurs when we look at all possible samples.
What I observed was almost impossible; but I really did observe it.
A reasonable conclusion is that the assumption I was making---that the die is fair---is not tenable, nor supported by the evidence (i.e., the data).


## Computing the value of the test statistic: $z$-tests {#OnePropTestStatistic}
\index{Hypothesis testing!one proportion}\index{Test statistic!z@$z$-score}

One way to measure how far the sample proportion $\hat{p} = 0.41$ is from the population proportion $p = 1/6$ in $100$ rolls is to use a $z$-score, since the sampling distribution (Fig.\ \@ref(fig:RollsSixesSD)) has an approximate normal distribution.
Since the mean of the distribution is $p$ and standard deviation of the distribution is $\text{s.e.}(\hat{p})$, the $z$-score is
\begin{align*}
   z 
   &= \frac{\text{sample statistic} - \text{mean of the distribution}}{\text{standard deviation of the distribution}}\\
   &= \frac{\hat{p} - p }{\text{s.e.}(\hat{p})}
    = \frac{0.41 - 0.1666...}{0.037267} = 6.53.
\end{align*}
In this context, the $z$-score is called a *test statistic*.
The observed sample proportion is more than six standard deviations from the mean of the distribution; this is *highly unusual* according to the $68$--$95$--$99.7$ rule (and Fig.\ \@ref(fig:RollsSixesSD)).


## Determining $P$-values {#OnePropTestP}

The value of the $z$-score shows that the observed value of $\hat{p}$ is very unusual, but *how* unusual?
Quantifying how unusual is assessed using a $P$-value, which is used widely in scientific research. 

$P$-values refer to the area *more extreme* than the calculated $z$-score in the normal distribution; that is, in the *tails* of the distribution.
This is a way of measuring how unusual the calculated $z$-score is.
For *two-tailed* alternative hypotheses, the $P$-value is the combined area in the lower and upper tails.
For *one-tailed* alternative hypotheses, the $P$-value is the area in one tail only.
Clearly, since the $P$-value is a probability, its value is always between $0$ and $1$.


<!-- ```{r, OneTwoP, fig.cap="One-tailed $P$ values (left; center) and two-tailed P-values (right). The two-tailed $P$-value is the combined area in the two tails of the distribution", fig.width=10, fig.height=3, out.width='90%', fig.align="center"} -->
<!-- par(mfrow = c(1, 3),  -->
<!--     mar = c(4, 1, 4, 1) + 0.1) -->


<!-- #####  -->

<!-- out <- plotNormal(mu = 0, -->
<!--            sd = 1, -->
<!--            main = expression( atop( A~one-tailed~italic(P)*"-value"~"if"~italic(z)==1~or~italic(z)==-1*":", -->
<!--                                     approx.~italic(P)*"-"*value*":"~0.32) ), -->
<!--            xlab = expression(italic(z)*"-score") -->
<!--            ) -->

<!-- shadeNormal(out$x, out$y, -->
<!--             lo = -5,  -->
<!--             hi = -1, -->
<!--             col = plot.colour) -->
<!-- polygon(x = c(-0.9, -0.9, 0.9, 0.9), # White-ish background for above text -->
<!--         y = c(0.05, 0.14, 0.14, 0.05), -->
<!--         border = NA, -->
<!--         col = "white") -->

<!-- anchorN <- c(-2, 0.08) -->

<!-- arrows(x0 = -1.25,  -->
<!--        x1 = anchorN[1], -->
<!--        y0 = 0.04, -->
<!--        y1 = anchorN[2], -->
<!--        angle = 15, -->
<!--        length = 0.15) # BOTH ENDS -->
<!-- text(anchorN, -->
<!--      pos = 3, -->
<!--      label = "One tail only") -->



<!-- ##### -->


<!-- out <- plotNormal(mu = 0, -->
<!--            sd = 1, -->
<!--            main = expression( atop( A~one-tailed~italic(P)*"-value"~"if"~italic(z)==1~or~italic(z)==-1*":", -->
<!--                                     approx.~italic(P)*"-"*value*":"~0.32) ), -->
<!--            xlab = expression(italic(z)*"-score") -->
<!--            ) -->

<!-- shadeNormal(out$x, out$y, -->
<!--             lo = 1,  -->
<!--             hi = 5, -->
<!--             col = plot.colour) -->
<!-- polygon(x = c(-0.9, -0.9, 0.9, 0.9), # White-ish background for above text -->
<!--         y = c(0.05, 0.14, 0.14, 0.05), -->
<!--         border = NA, -->
<!--         col = "white") -->

<!-- anchorP <- c(2, 0.08) -->

<!-- arrows(x0 = 1.25,  -->
<!--        x1 = anchorP[1], -->
<!--        y0 = 0.04, -->
<!--        y1 = anchorP[2], -->
<!--        angle = 15, -->
<!--        length = 0.15) # BOTH ENDS -->
<!-- text(anchorP, -->
<!--      pos = 3, -->
<!--      label = "One tail only") -->



<!-- ##### -->


<!-- out <- plotNormal(mu = 0, -->
<!--            sd = 1, -->
<!--            main = expression( atop( The~italic(P)*"-value"~"if"~italic(z)==2~or~italic(z)==-1*":", -->
<!--                                     approx.~italic(P)*"-"*value*":"~0.05) ), -->
<!--            xlab = expression(italic(z)*"-score") -->
<!--            ) -->
<!-- shadeNormal(out$x, out$y, -->
<!--             lo = -5,  -->
<!--             hi = -2, -->
<!--             col = plot.colour) -->
<!-- shadeNormal(out$x, out$y, -->
<!--             lo = 2,  -->
<!--             hi = 5, -->
<!--             col = plot.colour) -->

<!-- polygon(x = c(-1.4, -1.4, 1.4, 1.4), # White-ish background for above text -->
<!--         y = c(0.05, 0.14, 0.14, 0.05), -->
<!--         border = NA, -->
<!--         col = "white") -->

<!-- arrows(x0 = 1.25,  -->
<!--        x1 = anchorP[1], -->
<!--        y0 = 0.04, -->
<!--        y1 = anchorP[2], -->
<!--        angle = 15, -->
<!--        length = 0.15) -->
<!-- arrows(x0 = -1.25,  -->
<!--        x1 = anchorP[1], -->
<!--        y0 = 0.04, -->
<!--        y1 = anchorP[2], -->
<!--        angle = 15, -->
<!--        length = 0.15) -->

<!-- text(anchorP, -->
<!--      pos = 3, -->
<!--      label = "Two-tailed") -->
<!-- ``` -->




$P$-values can be approximated using the $68$--$95$--$99.7$ rule and a diagram (Sect.\ \@ref(ApproxProbs); Sect.\ \@ref(OnePropTestP6895997)), or more precisely using the $z$-tables
`r if (knitr::is_latex_output()) {
   'in Appendices\\ \\@ref(ZTablesNEG) and \\@ref(ZTablesPOS)'
} else {
   'in App.\\ \\@ref(ZTablesOnline)'
}`
(Sect.\ \@ref(ZScoreForestry); Sect.\ \@ref(OnePropTestPTables)).
$P$-values are also reported by software for most statistical tests.


### Approximating $P$-values using the $68$--$95$--$99.7$ rule {#OnePropTestP6895997}
\index{68@$68$--$95$--$99.7$ rule}

The $68$--$95$--$99.7$ rule can be used to determine *approximate* $P$-values.
To demonstrate, suppose the computed $z$-score was $z = 1$.
Then, the two-tailed $P$-value is the shaded area in Fig.\ \@ref(fig:OnePropTestP) (left panel): about $32$%, based on the $68$--$95$--$99.7$ rule.
The two-tailed $P$-value would be the same if $z = -1$.
The *one-tailed* $P$-value would be the area in one-tail: about $16$%, based on the $68$--$95$--$99.7$ rule.

As another example, suppose the calculated $z$-score was $z = 2$.
Then, the two-tailed $P$-value  is the shaded area shown in Fig.\ \@ref(fig:OnePropTestP) (right panel): about $5$%, based on the $68$--$95$--$99.7$ rule.
The two-tailed $P$-value would be the same if $z = -2$.
The *one-tailed* $P$-value would be the area in one-tail: about $2.5$%, based on the $68$--$95$--$99.7$ rule.


```{r, OnePropTestP, fig.cap="The two-tailed $P$-value is the combined area in the two tails of the distribution. Left panel: if $z = 1$ (or $z = -1$), the two-tailed $P$-value is approximately $0.16$. Right panel: if $z = 2$ (or $z = -2$), the two-tailed $P$-value is approximately $0.05$. (The one-tailed $P$-values are half the two-tailed $P$-values; i.e., in one tail only.)", fig.width=10, fig.height=2.75, out.width='95%', fig.align="center"}
par(mfrow = c(1, 2), 
    mar = c(4, 1, 4, 1) + 0.1)

out <- plotNormal(mu = 0,
           sd = 1,
           main = expression( atop( The~italic(P)*"-value"~"if"~italic(z)==1~or~italic(z)==-1*":",
                                    approx.~two*"-"*tailed~italic(P)*"-"*value*":"~0.32) ),
           xlab = expression(italic(z)*"-score")
           )

shadeNormal(out$x, out$y,
            lo = -5, 
            hi = -1,
            col = plot.colour)
shadeNormal(out$x, out$y,
            lo = 1, 
            hi = 5,
            col = plot.colour)
polygon(x = c(-0.9, -0.9, 0.9, 0.9), # White-ish background for above text
        y = c(0.05, 0.14, 0.14, 0.05),
        border = NA,
        col = "white")
arrows(x0 = -1, 
       x1 = 1,
       y0 = 0.04,
       y1 = 0.04,
       angle = 15,
       length = 0.15,
       code = 3) # BOTH ENDS
text(0,
     y = 0.07,
     label = "Area: 68%")




out <- plotNormal(mu = 0,
           sd = 1,
           main = expression( atop( The~italic(P)*"-value"~"if"~italic(z)==2~or~italic(z)==-2*":",
                                    approx.~two*"-"*tailed~italic(P)*"-"*value*":"~0.05) ),
           xlab = expression(italic(z)*"-score")
           )
shadeNormal(out$x, out$y,
            lo = -5, 
            hi = -2,
            col = plot.colour)
shadeNormal(out$x, out$y,
            lo = 2, 
            hi = 5,
            col = plot.colour)

polygon(x = c(-1.4, -1.4, 1.4, 1.4), # White-ish background for above text
        y = c(0.05, 0.14, 0.14, 0.05),
        border = NA,
        col = "white")
arrows(x0 = -2, 
       x1 = 2,
       y0 = 0.04,
       y1 = 0.04,
       angle = 15,
       length = 0.15,
       code = 3) # BOTH ENDS
text(0,
     y = 0.07,
     label = "Area: 95%")
```

Of course, calculated $z$-scores are unlikely to be exactly $z = 1$ or $z = -2$.
However, suppose the $z$-score is a little *larger* than $z = 1$; say $z = 1.2$.
Then, the tail area will be a little *smaller* than the tail area when $z = 1$ (Fig.\ \@ref(fig:OnePropTestP2), left panel).
The two-tailed $P$-value is a little *smaller* than $0.32$.

Similarly, suppose the $t$-score is a bit *smaller* than $z = 2$; say $z = 1.9$.
Then, the tail area will be a little *larger* than the tail area when $z = 2$ (Fig.\ \@ref(fig:OnePropTestP2), right panel).
The two-tailed $P$-value is a little *larger* than $0.05$.


```{r OnePropTestP2, fig.cap="The two-tailed $P$-value is the combined area in the two tails of the distribution. Left panel: when $z = 1.2$ (or $z = -1.2$). Right panel: when $z = 1.8$ (or $z = -1.8$).", fig.align="center", fig.width=10, fig.height=2.75, out.width='95%'}
par(mfrow = c(1, 2), 
    mar = c(4, 1, 4, 1) + 0.1)


out <- plotNormal(mu = 0,
           sd = 1,
           main = expression( atop(The~two*"-"*tailed~italic(P)*"-value"~when~italic(z)==1.2*":",
                                   a~bit~smaller~than~0.32)),
           xlab = expression(italic(z)*"-score")
           )
shadeNormal(out$x, out$y,
            lo = -5, 
            hi = -1.2,
            col = plot.colour)
shadeNormal(out$x, out$y,
            lo = 1.2, 
            hi = 5,
            col = plot.colour)

lines( x = c(-1, -1), 
       y = c(0, 1.37 * dnorm(-1)), 
       lwd = 2)
lines( x = c(1, 1), 
       y = c(0, 1.37 * dnorm(1)), 
       lwd = 2)
text(x = -1, 
     y = 1.37 * dnorm(-1), 
     pos = 2, 
     label = expression(italic(z) == -1~" "))
text(x = 1, 
     y = 1.37 * dnorm(1), 
     pos = 4,
     label = expression(" "~italic(z) == 1))

arrows(x0 = -1, 
       x1 = 1,
       y0 = 0.04,
       y1 = 0.04,
       angle = 15,
       length = 0.15,
       code = 3) # BOTH ENDS
text(0,
     y = 0.07,
     label = "Area: 68%")


out <- plotNormal(mu = 0,
           sd = 1,
           main = expression( atop(The~two*"-"*tailed~italic(P)*"-value"~when~italic(z)==1.9*":",
                                   a~bit~larger~than~0.05)),
           xlab = expression(italic(z)*"-score")
           )
shadeNormal(out$x, out$y,
            lo = -5, 
            hi = -1.9,
            col = plot.colour)
shadeNormal(out$x, out$y,
            lo = 1.9, 
            hi = 5,
            col = plot.colour)

lines( x = c(-2, -2), 
       y = c(0, 1.37 * dnorm(-1)), 
       lwd = 2)
lines( x = c(2, 2), 
       y = c(0, 1.37 * dnorm(1)), 
       lwd = 2)
text(x = -2,  
     y = 1.30 * dnorm(1), 
     pos = 2, 
     label = expression(italic(z) == -2))
text(x = 2, 
     y =  1.30 * dnorm(1),  
     pos = 4,
     label = expression(italic(z) == 2))

arrows(x0 = -2, 
       x1 = 2,
       y0 = 2.5 * dnorm(2),
       y1 = 2.5 * dnorm(2),
       angle = 15,
       length = 0.15,
       code = 3) # BOTH ENDS
text(0,
     y = 2.5 * dnorm(2),
     pos = 3,
     label = "Area: 95%")

```



### More precise $P$-values using tables {#OnePropTestPTables}

Using the tables of areas under normal distributions (`r if ( knitr::is_html_output()) { 'Appendix\\ \\@ref(ZTablesOnline).'} else {'Appendices\\ \\@ref(ZTablesNEG) and \\@ref(ZTablesPOS)'}`), more precise $P$-values can be found using the ideas from Sect.\ \@ref(ExactAreasUsingTables).
For instance (see Fig.\ \@ref(fig:OnePropTestP2)):

* For $z = 1.2$: the area to the *left* of $z = -1.2$ is $0.1151$, and the area to the *right* of $z = 1.2$ is $0.1151$, so the *two-tailed* $P$-value is $0.1151 + 0.1151 = 0.2302$.
  This is a little smaller than $0.32$, as estimated above.
* For $z = 1.9$: the area to the *left* of $z = -1.9$ is $0.0287$, and the area to the *right* of $z = 1.9$ is $0.0287$, so the *two-tailed* $P$-value is $0.0287 + 0.0287 = 0.0574$.
  This is a little larger than $0.05$, as estimated above.

In this die-rolling example, where $z = 6.53$, the tail area is *very* small (using `r if ( knitr::is_html_output()) { 'Appendix\\ \\@ref(ZTablesOnline)'} else {'Appendices\\ \\@ref(ZTablesNEG) and\\ \\@ref(ZTablesPOS)'}`),
and zero to four decimal places (Fig.\ \@ref(fig:RollsSixesSD)).
$P$-values are never exactly zero, so we write $P < 0.001$ (that is, the $P$-value is *less than* $0.001$).


## Making decisions with $P$-values {#OnePropTestDecisions}

$P$-values tells us the probability of observing the sample statistic (or one even more extreme), assuming the null hypothesis is true.
In the die-rolling example, the $P$-value is the probability of observing the value of $\hat{p} = 0.41$ (or more extreme), just through sampling variation (chance) if $p = 1/6$.
Since the $P$-value is a probability, it is a value between $0$ and $1$. 
Then `r if( knitr::is_html_output() ) {
   "(see the animation below)."
}`
`r if( knitr::is_latex_output() ) {
   "(see Fig.\\ \\@ref(fig:PvaluesBigSmall)):"
}`

* 'Big' $P$-values mean the sample statistic (i.e., $\hat{p}$) could reasonably have occurred through sampling variation in one of the many possible samples, if the assumption made about the parameter (stated in $H_0$) was true: 
   the data *do not* contradict the assumption in $H_0$.
   There *is no* compelling evidence to stop supporting the null hypothesis.
* 'Small' $P$-values mean the sample statistic (i.e., $\hat{p}$) is *unlikely* to have occurred through sampling variation in one of the many possible samples, if the assumption made about the parameter (stated in $H_0$) was true: 
   the data *do* contradict the assumption in $H_0$.
   There *is* compelling evidence to stop supporting the null hypothesis.

What is meant by 'small' and 'big' in this context?
In other words, what represents compelling evidence to stop supporting the null hypothesis?
A $P$-value smaller than $5$% (or $0.05$) is usually considered 'small', and compelling evidence to stop supporting the null hypothesis.
In contrast, a $P$-value larger than $5$% (or $0.05$) is usually considered 'big', and *not* compelling evidence to stop supporting the null hypothesis.
The value of $0.05$ is *arbitrary*, and in some applications the distinction is made when $P = 0.01$ or $P = 0.10$ instead. 
The decision-making process is shown in Fig.\ \@ref(fig:DecisionFlowDodgyDice).



```{r DecisionFlowDodgyDice, fig.cap = "A way to make decisions for the loaded-dice example", fig.align="center", out.width='100%', fig.width = 9.25, fig.height = 4}
source("R/showDecisionMaking.R")

showDecisionMaking(populationText = expression( atop(bold(Assume)~the,
                                                     die~is~fair)),
                   expectationText = expression(atop(bold(Expect)~to~find~about,
                                                     16.7*"%"~of~rolls~are~ones)),
                   oneSampleText = expression( atop(Roll~the~die,
                                                    100~times) ),
                   oneStatisticText =  expression( atop(bold(Observe)~41*"%"~of~rolls,
                                                        are~ones)),
                   Decision = "Reject"     
)

```

In this die-rolling example, where the $P$-value is *very* small, the data contradict the null hypothesis (that $p = 1/6$), and there is compelling evidence to stop supporting the assumption that $p = 1/6$.
This suggests that the die is very likely *not* fair.


::: {.importantBox .important data-latex="{iconmonstr-warning-8-240.png}"}
*Be careful interpreting the results!*
We cannot be *sure* that the die is unfair.
*A small $P$-value is not proof that the die is loaded.*
The die may be fair but, due to sampling variation, the sample we observed may simply have produced an unusually high proportion of 
`r if (knitr::is_latex_output()) {
   '\\Largedice{1}'
} else {
   '<span class="larger-die">&#9856;</span>'
}`
rolls by chance.

The result is interpreted as 'there is evidence that the die is unfair'.
Remember: *the onus is on the data to refute the null hypothesis, the initial assumption*.
:::


```{r PvaluesAnimation, animation.hook="gifski", dev=if (is_latex_output()){"pdf"}else{"png"}}
zList <- c( seq(0.5,
                1,
                by = 0.1),
            seq(1, 3.5, 
                by = 0.05) ) 
pMeaning <- function(pValue){
  if (pValue > 0.10) Meaning <- "Insufficient"
  if ( (pValue >= 0.05)  & (pValue < 0.10)) Meaning <- "Slight"
  if ( (pValue >= 0.01)  & (pValue < 0.05)) Meaning <- "Moderate"
  if ( (pValue >= 0.001) & (pValue < 0.01)) Meaning <- "Strong"
  if (pValue < 0.001) Meaning <- "Very strong"
  Meaning
}

pColours <- viridis( length(zList), 
                     begin = 0.5 ,
                     end = 1,
                     option = "H")

if (knitr::is_html_output()) {
  for (i in (1:length(zList))) {

    zScore <- zList[i]
    pValue <- pnorm( -zScore )
    pValue2 <- ifelse( pValue < 0.001, 
                       "< 0.001",
                       round(pValue, 4) )
    
   par( mar = c(0.1, 0.1, 2.5, 0.1) ) # Number of margin lines on each side
    out <- plotNormal(mu = 0,
                      sd = 1,
                      xlab = expression(italic(z)~"-score"),
                      main = paste("Evidence to support alternative hypothesis:\n",
                                   pMeaning(pValue)),
                      round.dec = 0)
    shadeNormal(out$x,
                out$y,
                col = pColours[i],
                lo = zScore,
                hi = 6)
    shadeNormal(out$x,
                out$y,
                col = pColours[i],
                hi = -zScore,
                lo = -6)

    abline(v = zScore,
           col = "grey")
    abline(v = -zScore,
           col = "grey")

    polygon(x = c(-1.4, -1.4, 1.4, 1.4), # White-ish background for above text
            y = c(0.02, 0.10, 0.10, 0.02),
            border = NA,
            col = "white")
    text(0,
         y = 0.06,
         label = paste("Two-tailed P-value:", pValue2 ) )
  }  
  
}
```


```{r PvaluesBigSmall, fig.cap="The strength of evidence: $P$-values. As the $z$-score becomes larger, the $P$-value becomes smaller, and it is more likely that the evidence contradicts the null hypothesis.", fig.height = 2.75, fig.width=10, out.width='100%', fig.align="center", dev=if (is_latex_output()){"pdf"}else{"png"}}
if (knitr::is_latex_output()) {
  
  par(mfrow = c(1, 2),
      mar = c(4.1, 0.25, 4.1, 0.25) )
#  par( mar = c(0.1, 0.1, 0.1, 0.1) ) # Number of margin lines on each side

  zList <- c( 1.25, # Two-tailed P-value: 10% -1.645
              2.2 ) # Two-tailed P-value: 1% -2.576
 
  
  zList <- c( qnorm(0.10),
              qnorm(0.015) )
  pMeaning <- function(pValue){
    if (  pValue >  0.10)                     Meaning <- "Insufficient"
    if ( (pValue >= 0.05)  & (pValue < 0.10)) Meaning <- "Slight"
    if ( (pValue >= 0.01)  & (pValue < 0.05)) Meaning <- "Moderate"
    if ( (pValue >= 0.001) & (pValue < 0.01)) Meaning <- "Strong"
    if (  pValue <  0.001)                    Meaning <- "Very strong"
    Meaning
    }
  
  pColours <- c(BlockColour,
                ResponseColour)
  
  for (i in (1:length(zList))){
    zScore <- zList[i]
    pValue <- pnorm( zScore )
    
  
    pValue2 <- ifelse( pValue < 0.001, 
                       "< 0.001",
                       round(pValue, 4) )
    
    out <- plotNormal(mu = 0,
                      sd = 1,
                      xlab = expression(italic(z)-score),
                      round.dec = 0,
                      main = paste("Evidence to support alternative\nhypothesis:", 
                            pMeaning(pValue))
                      )
    shadeNormal(out$x,
                out$y,
                col = pColours[i],
                lo = -zScore,
                hi = 10)
    shadeNormal(out$x,
                out$y,
                col = pColours[i],
                hi = zScore,
                lo = -10)
    
    abline(v = zScore,
           col = "grey")
    abline(v = -zScore,
           col = "grey")
    
    polygon(x = c(-2.1, -2.1, 2.1, 2.1), # White-ish background for the above text
           y = c(0.09, 0.21, 0.21, 0.09),
           border = NA,
           col = rgb(255, 255, 255, max = 255, alpha = 240) ) # Translucent white
    text(0,
         y = 0.14,
         label = paste("Two-tailed P-value:", pValue2 ) )
  }  
  
}

```



::: {.example #PvaluesInterpret name="Interpreting $P$-values"}
In the die example, suppose we found that the two-tailed $P$-value was $0.26$.
This is relatively 'large' (i.e., much larger than $0.05$).
This means that the observed value of $\hat{p}$ could easily be explained by chance, and is *not* compelling evidence to stop supporting the null hypothesis (that the die is fair).
We would say that there is no evidence that $p$ is not $1/6$.
:::


## Writing conclusions {#OnePropTestCommunicate}

In general, communicating the results of any hypothesis test requires:

* an answer to the RQ,  worded in terms of how much evidence exists to support the *alternative* hypothesis.
* a summary of the evidence used to reach that conclusion (such as the $z$-score and $P$-value, including if the $P$-value is one- or two-tailed).
* sample summary information, including a CI (see Chap.\ \@ref(CIOneProportion)), summarising the data used to make the decision.

So for the die-rolling example, write:

> The sample provides very strong evidence ($z = 6.53$; two-tailed $P < 0.001$) that the proportion of sixes is not $1/6$ ($\hat{p} = 0.41$; approx.\ $95$% CI: $0.312$ to $0.508$; $n = 100$ rolls) in the population.

This statement includes the three necessary components:

* an answer to the RQ: 'The sample provides very strong evidence... that the population proportion is not $1/6$'.
  The wording states how much evidence exists in the sample to support the *alternative* hypothesis.
* the evidence used to reach the conclusion: '$z = 6.53$; two-tailed $P < 0.001$)'.
* sample summary information (including a CI).


::: {.importantBox .important data-latex="{iconmonstr-warning-8-240.png}"}
Since the *null* hypothesis is initially assumed to be true, *the onus is on the evidence to refute the null hypothesis*. 
That is, we retain the null hypothesis unless there is compelling evidence to stop doing so.
Hence, conclusions are worded in terms of how strongly the evidence (i.e., sample data) supports the alternative hypothesis.  

The alternative hypothesis *may* or *may not* be true, but we report how strongly the evidence (data) supports the alternative hypothesis.
Conclusions are *not* worded in terms of how much evidence support the null hypothesis.
:::


In $100$ rolls of the *other* die, I found a 
`r if (knitr::is_latex_output()) {
   '\\Largedice{1}'
} else {
   '<span class="larger-die">&#9856;</span>'
}`
on $15$ rolls, so that $\hat{p} = 0.15$.
Following the procedures above (check!) and using the same hypotheses, $z = -0.45$ and (using tables) the two-tailed $P$-value is $2\times 0.3264 = 0.6529$.
This means that the sample result was not unusual if $p = 1/6$, and there is no compelling evidence to stop supporting the null hypothesis.
There is *no evidence* to suggest the second die is loaded.

This all suggests that the first die was the loaded die.
Now I need to decide how to remember which die is the loaded one\dots


::: {.importantBox .important data-latex="{iconmonstr-warning-8-240.png}"}
*A large $P$-value does not necessarily mean that the die is fair!*
It only means that the proportions of rolls that produce a 
`r if (knitr::is_latex_output()) {
   '\\Largedice{1}'
} else {
   '<span class="larger-die">&#9856;</span>'
}`
is not unusual... but perhaps the die is loaded in some other way (i.e., to produce more-than-expected rolls of a 
`r if (knitr::is_latex_output()) {
   '\\Largedice{5}).'
} else {
   '<span class="larger-die">&#9860;</span>).'
}`


*A large $P$-value does not necessarily mean that the die is fair!*
The die may indeed be loaded to produce a larger-than-expected numbers of 
`r if (knitr::is_latex_output()) {
   '\\Largedice{1}'
} else {
   '<span class="larger-die">&#9856;</span>'
}`
rolls, but (due to sampling variation) the sample we observed simply did not provide evidence to make that conclusion.

The result is interpreted in terms of how much evidence exists to support the alternative hypothesis.
The onus is on the data (i.e., evidence) to refute the assumption made in the null hypothesis.
:::


## Process overview {#OnePropTestOverview}

Let's recap the decision-making process, in this context about rolling a 
`r if (knitr::is_latex_output()) {
   '\\Largedice{1}'
} else {
   '<span class="larger-die">&#9856;</span>'
}`:

1. *Assumption*: 
   Write the *null hypothesis* and *alternative hypothesis* about the *parameter* (based on the RQ), where $p$ is the population proportion of rolls that are a
`r if (knitr::is_latex_output()) {
   '\\Largedice{1}'
} else {
   '<span class="larger-die">&#9856;</span>'
}`: 
   * $H_0$: $p = 1/6$, and 
   * $H_1$: $p \ne 1/6$ (this is a two-tailed alternative hypothesis).
2. *Expectation*: 
   The sampling distribution describes what values to reasonably expect from the sample statistic across all possible samples, *if* the null hypothesis is true.
   In this situation, the sampling distribution has a normal distribution.
3. *Observation*: 
   Compute the $z$-score ($z = 6.53$), a measure of the discrepancy between the assumed population value, and the observed sample value.
4. *Decision*: 
   Determine if the data are consistent with the assumption, by computing the $P$-value. 

Here, the $P$-value is (much) less than $0.001$, so very strong evidence exists that $p$ is *not* $1/6$.



## Statistical validity conditions {#ValidityProportionsTest}
\index{Statistical validity!one proportion}

The confidence intervals formed in this chapter assume the sampling distribution is approximately a normal distribution (and so, for example, the $68$--$95$--$99.7$ rule can be applied).
This is only true if certain conditions are met.
For a hypothesis test for one proportion, these conditions are similar to those for the CI for one proportion (Sect.\ \@ref(ValidityProportions)).

The *statistical validity conditions* for a test for a single proportion is that the *expected* number of individuals in the group of interest (i.e, $n\times p$) and in the group *not* of interest (i.e., $n\times (1 - p)$) both exceed five; that is:

* $n\times p > 5$, *and* $n\times (1 - p) > 5$.

The value of\ $5$ here is a rough figure; some books give other values (such as $10$).
This condition ensures that the *sampling distribution of the sample proportions has an approximate normal distribution* (so that, for example, the $68$--$95$--$99.7$ rule can be used).

The units of analysis are also assumed to be *independent* (e.g., from a simple random sample).

If the statistical validity conditions are not met, other similar options include using a binomial test\index{Non-parametric statistics} [@conover2003practical].



::: {.example #StatisticalValidityDice name="Statistical validity"}
The hypothesis test regarding the dice is statistically valid.
Firstly, $n\times p = 100 \times (1/6) = 16.666\dots$ (i.e., I would expect about 16.7 rolls to show a `r if (knitr::is_latex_output()) {
   '\\Largedice{1})'
} else {
   '<span class="larger-die">&#9856;</span>)'
}`,
and $n\times (1 - p) = 83.333\dots$ (i.e., I would expect about 83.3 rolls to *not* show a 
`r if (knitr::is_latex_output()) {
   '\\Largedice{1})'
} else {
   '<span class="larger-die">&#9856;</span>)'
}`.
*Both* comfortably exceed five.
:::


## Example: dominance of birds {#OneProportiontestBirds}

@barve2017elevational compared two types of birds (male green-backed tits; male cinereous tits) to see which was more behaviourally dominant over winter.
If the species were equally-dominant, then about $50$% of the interactions would be won by each species.
If we define $p$ as the proportion of interactions won by green-backed tits, then we would expect $p = 0.50$.
However, in the $45$ interactions observed between the two species, green-backed tits won $37$ of these interactions (i.e., $\hat{p} = 37/45 = 0.82222$).

Of course, every sample of $45$ interactions would produce a different sample proportion, so the difference between this sample proportion and $p = 0.5$ could be due to sampling variation.
To test if the population proportion of interaction wins could be equally shared, the hypotheses are:
$$
   \text{$H_0$: } p = 0.5\quad\text{and}\quad\text{$H_1$: } p \ne 0.5 \text{ (two-tailed)}.
$$
The test is statistically valid, since $n\times p = 45\times 0.5 = 22.5$ and $n\times (1 - p) = 22.5$; both exceed five (i.e., expected half of the $50$ interactions to be won by each species).
The *standard error* is
$$
   \text{s.e.}(\hat{p}) 
   = \sqrt{\frac{p \times (1 - p)}{n}} 
   = \sqrt{\frac{0.50 \times (1 - 0.50)}{45}} 
   = 0.0745356....
$$
Note the value of $p$, not $\hat{p}$, is used in the calculation.
Then, the value of the *test statistic* is:
$$
   z 
   = \frac{\hat{p} - p}{\text{s.e.}(\hat{p})}
   = \frac{0.82222 - 0.50}{0.0745356}
   = 4.322.
$$
This is a *very* large $z$-score, so the $P$-value will be very small, using the $68$--$95$--$99.7$ rule, or using tables.
This is compelling evidence to stop supporting the null hypothesis.

Computing the $95$% CI for the proportion requires using the standard error computed with\ $\hat{p}$ (not $p$):
$$
   \text{s.e.}(\hat{p}) 
   = \sqrt{\frac{\hat{p} \times (1 - \hat{p})}{n}} 
   = \sqrt{\frac{0.82222 \times (1 - 0.82222)}{45}} 
   = 0.056999...
$$
An approximate $95$% CI is from $0.708$ to $0.936$.
We write:

> *Very* strong evidence exists in the sample ($P < 0.001$; $z = 4.325$) that the interactions were not won equally by each species ($\hat{p} = 0.8222$ won by green-backed tits; $n = 45$; approximate $95$% CI: $0.708$ to $0.936$) in the population.





## Chapter summary {#Chap28Summary}

To test a hypothesis about a population proportion $p$:

* Write the null hypothesis ($H_0$) and the alternative hypothesis ($H_1$).
* Initially *assume* the value of $p$ in the null hypothesis to be true.
* Then, describe the *sampling distribution*, which describes what to *expect* from the sample statistic across all possible samples, based on this assumption: under certain statistical validity conditions, the sample mean varies with:
   *  an approximate normal distribution,
   *  with sampling mean whose value if the value of $p$,
   *  with a standard deviation of $\displaystyle \text{s.e.}(\hat{p}) = \sqrt{\frac{p \times (1 - p)}{n}}$, where $p$ is the hypothesised value given in the null hypothesis, and $n$ is the sample size.
* Compute the value of the *test statistic*:
$$
   z = \frac{ \hat{p} - p}{\text{s.e.}(\hat{p})}.
$$
* Compute an approximate *$P$-value* using the $68$--$95$--$99.7$ rule, or using tables.
* Make a decision, and write a conclusion.



## Quick review questions  {#Chap31-QuickReview}

::: {.webex-check .webex-box}
A study of diseases in native Americans [@kizer2006digestive] found $381$ obese or overweight patients in $449$ patients.
In the general population of the USA, the percentage obese or overweight is $65$%.
The researchers wanted to determine if the percentage of obesity/overweight native Americans was *greater* than that of the general population.

1. True or false: We initially assume the *population* proportion of overweight/obese native Americans is $0.65$. \tightlist
`r if( knitr::is_html_output() ) {torf(answer=TRUE)}`
1. True or false: The sample size is $n = 381$.
`r if( knitr::is_html_output() ) {torf(answer=FALSE)}`
1. What is the value of the *sample* proportion $\hat{p}$?
`r if( knitr::is_html_output() ) {
   paste("(Use *four* decimal places.) ", fitb(num=TRUE, tol=0.0001, answer=0.84855) )
}`
1. True or false: The *null* hypothesis is $H_0$: $p = 0.65$.
`r if( knitr::is_html_output() ) {torf(answer=TRUE)}`
1. True or false: The *alternative* hypothesis is *one*-tailed.
`r if( knitr::is_html_output() ) {torf(answer=TRUE)}`
1. True or false: In a one-sample test of proportion, the $z$-score is always large.
`r if( knitr::is_html_output() ) {torf(answer=FALSE)}`
1. What is the value of $z$-score for this example?
`r if( knitr::is_html_output() ) {
   paste("(Use *two* decimal places.)", fitb(num=TRUE, tol=0.005, answer=8.82079) )
}`
1. True of false: We have compelling evidence to stop supporting the null hypothesis in this example.
`r if( knitr::is_html_output() ) {torf(answer=TRUE)}`
1. True or false? We always accept the *null* hypothesis.
`r if( knitr::is_html_output() ) {torf(answer=FALSE)}`
:::


## Exercises {#OneProportionTestExercises}

Answers to odd-numbered exercises are available in App.\ \@ref(Answers).

`r if( knitr::is_latex_output() ) "\\captionsetup{font=small}"`

:::{.exercise #OneProportionTestExplainA}
Explain *why* the standard error is computed using $p$ for hypothesis testing, but using $\hat{p}$ in confidence intervals.
:::


:::{.exercise #OneProportionTestExplainB}
Explain why we compute $\text{s.e.}(\hat{p})$ and not $\text{s.e.}(p)$.
:::


:::{.exercise #OnePorportionTestExercisesDodgyA}
What is wrong with the following statement, after testing $H_0$: $p = 0.25$:

> There is very strong evidence that the sample proportion is greater than $0.25$.
:::


:::{.exercise #OnePorportionTestExercisesDodgyB}
Consider this statement from @davis2024higher, that appears under their Table\ 2:

> One proportion $z$-test with $H_0 = 0.076$, the proportion of UDT in our sample

What is wrong with this statement?
:::


::: {.exercise #OneProportionTestExercisesPlacebos}
The study of herbal medicines is complicated, as *blinding* subjects is difficult: placebos are often easily identifiable by eye, by taste, or by smell.

@loyeung2018experimental studied if subjects could identify potential placebos at a *better* rate than just guessing.
The $81$ subjects were each presented with a choice of five different supplements, four of which were placebos.
Subjects were asked to select which one was the legitimate herbal supplement based on the *taste*.
Of these, $50$ correctly selected the true herbal supplement.

1. If the subjects were selecting the true herbal supplement randomly, what proportion of subjects would be expected to select the correct supplement as the true herbal medicine?
2. Write the hypotheses for addressing the aims of the study.
3. Is this a one- or two-tailed test? 
   Explain.
4. Sketch the *sampling distribution* of the sample proportion, assuming the null hypothesis is correct.
5. Is there evidence that people can identify the true supplement by taste?
6. Are the statistical validity conditions satisfied?
:::


::: {.exercise #POnePropTestMeasles}
@kim2004sero studied the measles-rubella vaccination-rates in Korea.
They compared the proportion of children with measles antibodies to the World Health Organization (WHO) target proportion (for children aged $5$ to $9$ years old: $10$%).

The aim of the study was to test if the proportion of Korean children with the measles antibody in the *population* was $10$% or lower (i.e., better).
In the study, $55$ children out of $972$ had the antibody present

1. Compute the sample proportion $\hat{p}$ of children with measles antibodies.
2. Write the hypotheses for the test.
   Is the test one- or two-tailed?
3. Compute the standard error for the test.
4. Compute the $z$-score and determine the $P$-value.
5. Write a conclusion.
6. Are the statistical validity conditions satisfied?
:::


::: {.exercise #OneProportionTestTurtleSex}
@streeting2022optimising studied western saw-shelled turtles.
When eggs were incubated at $27^\circ$C, they observed that $29$ males and $44$ females hatched.
Are the proportions of male and female turtles that hatch at this temperature equal?
:::


::: {.exercise #OneProportionTestExercisesEPL}
[*Dataset*: `PremierL`]
In the 2019/2020 English Premier League (EPL), the home team won $91$ games, and the away team won $67$ games.
(Another $50$ games were draws.)

Use the $158$ games with a result to determine if there is evidence that the home team wins more often than $50$% (i.e., that there is a home-side advantage).
:::


::: {.exercise #OneProportionTestExercisesPedalMachines}
@maeda2013introducing introduced pedal machines on the first floor of Joyner Library for use by students at East Carolina University, to increase activity in library users.
At ECU, $60.2$% of all students were females (i.e., in the population).
Students were observed using the machine on $589$ occasions, of which $295$ times were by females

Is there evidence that the proportion of females users of the machines was *lower* than the overall female proportion at the university?
What would you conclude?
:::


::: {.exercise #OneProportionTestExercisesCasinos}
@koenen1995analysis found that $88$ of the $357$ visitors to Las Vegas casinos im 1995 were smokers.
At the time, $25.5$% of the general US population were smokers (based on data from the U.S. *National Center for Health Statistics*).
Are casino-goers just as likely to be a smokers as the general U.S. population?
:::


:::{.exercise #OneProportionBreadfruitPasta}
@nochera2019development developed a gluten-free pasta made from breadfruit.
In the study sample, $57$ of the $71$ participants stated that they liked the pasta.
Do the researchers have sufficient evidence to claim that the 'majority of people like breadfruit pasta'?
:::


::: {.exercise #OneProportionTestExercisesCTS}
Carpal Tunnel Syndrome (CTS) is a painful condition in the wrists.
@boltuch2020palmaris were interested in whether 'a relationship exists between the palmaris tendon [and] carpal tunnel syndrome (CTS)' (p. 493).
The palmaris longus (PL) tendon is visually absent in about $15$% of the population.
The researchers found PL was visually absent in $33$ of $516$ CTS wrists in their sample.

Is there evidence to suggest that rate of PL absence is *different* in CTS cases, compared to the general population? 
:::


::: {.exercise #OneProportionTestExercisesBorers}
@siegfried2014estimating studied resistance of some commercial corn varieties to the European corn borer. 
Borers were collected from corn in Iowa and Nebraska.

Researchers aimed to estimate the frequency of resistance to the toxin in the corn.
By mating borers collected from the field with various resistant laboratory individuals, they could determine what proportion of resistant individuals to expect in the second generation offspring.
In one study of $n = 172$ second-generation individuals, $24$ were found to be resistant. 
The expectation was that $1$-in-$16$ would be resistant if the field borers were resistant.

Perform a hypothesis test to determine if the data suggest that the borers were resistant (that is, if the population proportion is $1/16$) as expected.
:::


::: {.exercise #OneProportionTestExercisesLEDlights}
@davidovic2019drivers studied streetlight preferences of drivers.
Drivers were asked to conduct a series of manoeuvres under $3000$K LED light and then under $4000$K LED lights.
They were then asked to decide which streetlight they preferred.

Out of the $52$ subjects, $29$ preferred the $3000$K LED lights.
Is there evidence that the choice between the two streetlights is random, or is there evidence of a preference for one over the other? 
:::



::: {.exercise #OneProportionTestExercisesCoinSpin}
The euro was introduced as a currency on 01 January 1999.
According to a report by the 
`r if (knitr::is_latex_output()) {
   '*New Scientist*'
} else {
   '[*New Scientist*](https://www.newscientist.com/article/dn1748-euro-coin-accused-of-unfair-flipping/)'
}`,
students in Poland spun a Belgian one-euro coin $250$ times, and found $140$ heads (as reported by @data:Gelman2002:DiceCoins).
This resulted in an 'accusation of bias' in the *New Scientist* article.
However, every set of $250$ spins can produces a different proportion of heads, so perhaps the results is just due to randomness.

Does this sample of $250$ spins suggest that the one-euro Belgian coin is biased?
:::


::: {.exercise #OneProportionTestExercisesBirths}
As noted in Sect.\ \@ref(ProbRelFreq), the 
`r if (knitr::is_latex_output()) {
   '*Australian Bureau of Statistics* (ABS)'
} else {
   '[*Australian Bureau of Statistics* (ABS)](http:www.abs.gov.au/ausstats/abs@.nsf/0/B8865D71D84F5210CA2579330016754C?opendocument)'
}`
stated that:

> The sex ratio for all births registered in Australia generally fluctuates around $105.5$ male births per $100$ female births.

1. The value of $105.5$ is effectively a population odds ratio.
   Show that this is equivalent to stating the population proportion of male births is $0.51338$.
2. In 2021, there were $148\ 636$ male births and $140\ 944$ female births.
   Compute the *sample* proportion of male births in 2021 (to five decimal places).
3. Conduct a hypothesis test to determine if the 2021 data appear different to the long-term proportion.

(Another $23$ births were listed as 'other' or 'not stated', but are not needed for the ratio of male-to-female births.)

:::


`r if( knitr::is_latex_output() ) "\\captionsetup{font=normalsize}"`


<!-- QUICK REVIEW ANSWERS -->
`r if (knitr::is_html_output()) '<!--'`
::: {.EOCanswerBox .EOCanswer data-latex="{iconmonstr-check-mark-14-240.png}"}
\textbf{Answers to \textit{Quick Revision} questions:}
**1.** True.
**2.** False.
**3.** 0.84855.
**4.** True.
**5.** True.
**6.** False.
**7.** $z = -8.82079$.
**8.** True.
**9.** False.
:::
`r if (knitr::is_html_output()) '-->'`



<!-- ::: {.exercise #OneProportionTestExercisesIguanas} -->
<!-- @avery2014invasive studied black spiny-tailed iguanas in Florida (an invasive species). -->
<!-- They measured the iguanas' snout--vent length (SVL). -->
<!-- Of the $275$ iguanas with a SVL between $100$ and $149$\ mm, $146$ were female. -->

<!-- Assuming female and male iguanas were equally present in the population, is there evidence that female and male iguanas were equally-likely to be found with SVL in this range? -->
<!-- ::: -->


<!-- ::: {.exercise #OneProportionTestExercisesPenguins} -->
<!-- @vanstreels2013female studied Magellanic penguins found dead or stranded on the southern Brazilian coast. Of the $73$ adult penguins found, $47$ were female, -->

<!-- Assuming female and male penguins were equally present in the population, we would expect about half the dead or stranded penguins to be female and male. -->
<!-- Is this what the data suggest? -->
<!-- ::: -->

<!-- ## Example: obesity REMOVE {#OneProportiontestObesity} -->

<!-- @kolanska2010high compared the rate of obesity in $n = 143$ Polish patients with adrenal tumours to that of the general population of Poland ($p = 0.125$), to test if those with adrenal tumours were *more likely* to be obese that the general population. -->
<!-- The hypotheses are:   -->
<!-- \[ -->
<!--    \text{$H_0$: } p = 0.125\quad\text{and}\quad\text{$H_1$: } p > 0.125\text{ (one-tailed)}. -->
<!-- \] -->
<!-- Assuming the null hypothesis is true, the standard error is (remembering to use $p$):   -->
<!-- \[ -->
<!--    \text{s.e.}(\hat{p})  -->
<!--    = \sqrt{\frac{p (1 - p)}{n}}  -->
<!--    = \sqrt{\frac{.125 \times (1 - 0.125)}{143}}  -->
<!--    = 0.027656... -->
<!-- \] -->
<!-- In their sample, $57$ were obese, so $\hat{p} = 57/143 = 0.3986...$. -->
<!-- Then, the value of the *test statistic* is:   -->
<!-- \[ -->
<!--    z  -->
<!--    = \frac{\hat{p} - p}{\text{s.e.}(p)} -->
<!--    = \frac{0.3986 - 0.125}{0.027656} -->
<!--    = 9.89. -->
<!-- \] -->
<!-- This is an *extremely* large $z$-score, so expect a very small $P$-value using the 68--95--99.7 rule. -->

<!-- The $95$% CI for the proportion requires the standard error computed from the *sample* proportion:   -->
<!-- \[ -->
<!--    \text{s.e.}(\hat{p})  -->
<!--    = \sqrt{\frac{\hat{p} (1 - \hat{p})}{n}}  -->
<!--    = \sqrt{\frac{0.3986 \times (1 - 0.3986)}{143}}  -->
<!--    = 0.040943... -->
<!-- \] -->
<!-- The approximate $95$% CI is $0.3986 \pm(2 \times 0.040943...)$. -->
<!-- We write: -->

<!-- > *Very* strong evidence exists in the sample (one-tailed $P < 0.001$; $z = 9.89$) that the rate of obesity in patients with adrenal tumours ($\hat{p} = 0.3986$; $n = 143$; approximate 95% CI: 0.317 to 0.480) is higher than the general Polish population. -->




