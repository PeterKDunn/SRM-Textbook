# Tests for one mean {#TestOneMean}


<!-- Introductions; easier to separate by format -->
```{r, child = if (knitr::is_html_output()) {'./introductions/32-Testing-OneMean-HTML.Rmd'} else {'./introductions/32-Testing-OneMean-LaTeX.Rmd'}}
```




## Introduction: body temperatures {#BodyTemperature}


```{r}
data(BodyTemp)
```


<div style="float:right; width: 222x; border: 1px; padding:10px">
<img src="Illustrations/pexels-anna-shvets-3987141.jpg" width="200px"/>
</div>


The average internal body temperature is commonly believed to be $37.0^\circ\text{C}$ ($98.6^\circ$F).
This is based on data over 150 years old [@data:Wunderlich:BodyTemp].
@data:mackowiak:bodytemp re-examined this claim, to determine if these values are still appropriate (given changes in how temperature is measured, as much as actual body-temperature changes). 
That is, a decision is sought about the value of the *population* mean body temperature.
This value will never be known: we would need to measure the internal body temperature of every person alive .. and even those not yet born.

Define the parameter as $\mu$, the population mean internal body temperature (in ${}^\circ\text{C}$).
A *sample* of people can be taken to determine whether or not there is evidence that the *population* mean internal body temperature is $37.0^\circ\text{C}$.

To make this decision, the decision-making process (Sect. \@ref(DecisionMaking)) is used.
*Assume* that $\mu = 37.0$ (there is no evidence that this accepted standard is wrong), and the evidence is examined to determine if the evidence supports this claim or not.
The RQ is:

> Is the *population* mean internal body temperature $37.0^\circ\text{C}$?


## Statistical hypotheses and notation

The decision making process begins by *assuming* that the null hypothesis is true: that $\mu = 37.0$.
Because every sample is different, $\bar{x}$ will vary, and the *sample* mean $\bar{x}$ probably won't be exactly $37.0$, *even if* the population mean $\mu$ is $37.0$.
Two broad reasons could explain why:

1. The *population* mean body temperature *is* $37.0^\circ\text{C}$.  
   However, $\bar{x}$ isn't exactly $37.0$ due to sampling variation; 
2. The *population* mean body temperature *is not* $37.0$.  
   The *sample* mean body temperature reflects this.

These hypotheses can be written more formally as:

1. The *null hypothesis* ($H_0$): $\mu = 37.0^\circ\text{C}$; and
2. The *alternative hypothesis* ($H_1$): $\mu \ne 37.0^\circ\text{C}$.

The RQ asks if $\mu$ is $37.0$ or some other value (either smaller or larger than $37.0$).
Two possibilities are considered, so the alternative hypothesis is *two-tailed*.


## Sampling distribution for $\bar{x}$ {#SamplingDistSampleMeanHT}
\index{Sampling distribution!one mean}

To answer this RQ, data were collected by @data:Shoemaker1996:Temperature
`r if (knitr::is_latex_output()) {
   '(Table\\ \\@ref(tab:DataBodyTemp)).'
} else {
   '(Fig.\\ \\@ref(fig:DataBodyTemp)).'
}`
A graphical summary (Fig.\ \@ref(fig:BodyTempHist)) and a numerical summary, using jamovi (Fig.\ \@ref(fig:BodyTempjamovi)), shows that:

* the *sample* mean is $\bar{x} = 36.8052^\circ$C,
* the *sample* standard deviation is $s = 0.4073^\circ$C, and
* the sample size is $n = 130$.

The sample mean $\bar{x}$ is *less* than the assumed value of  $\mu = 37$... but *why*?
Can the difference reasonably be explained by sampling variation?
The approximate $95$% CI for $\mu$ is from $36.73$ to $36.88$.
This CI is narrow, implying $\mu$ has been estimated with precision, so detecting even small deviations of $\mu$ from $37.0^\circ$ should be possible.



<!-- ```{r NotationOneMeanHT} -->

<!-- OneMeanNotation <- array( dim = c(4, 2)) -->

<!-- OneMeanNotation[1, ] <- c("Describe individual in the population", -->
<!--                           "Vary with mean $\\mu$ and standard deviation $\\sigma$") -->
<!-- OneMeanNotation[2, ] <- c("Describe individual in a sample", -->
<!--                           "Vary with mean $\\bar{x}$ and standard deviation $s$") -->
<!-- OneMeanNotation[3, ] <- c("Describe sample means ($\\bar{x}$)", -->
<!--                           "Vary with approx. normal distribution (under certain conditions):") -->
<!-- OneMeanNotation[4, ] <- c("across all possible samples", -->
<!--                           "sampling mean $\\mu$; standard deviation $\\text{s.e.}(\\bar{x})$") -->


<!-- if( knitr::is_latex_output() ) { -->
<!--   kable( OneMeanNotation, -->
<!--          format = "latex", -->
<!--          booktabs = TRUE, -->
<!--          longtable = FALSE, -->
<!--          escape = FALSE, -->
<!--          caption = "The notation used for describing means, and the sampling distribution of the sample means", -->
<!--          align = c("r", "l"), -->
<!--          linesep = c("\\addlinespace", -->
<!--                      "\\addlinespace", -->
<!--                      ""), -->
<!--          col.names = c("Quantity", -->
<!--                        "Description") ) %>% -->
<!-- 	row_spec(0, bold = TRUE) %>% -->
<!--   kable_styling(font_size = 10) -->
<!-- } else { -->
<!--   OneMeanNotation[3, 1] <- paste(OneMeanNotation[3, 1],  -->
<!--                                  OneMeanNotation[4, 1]) -->
<!--   OneMeanNotation[3, 2] <- paste(OneMeanNotation[3, 2],  -->
<!--                                  OneMeanNotation[4, 2]) -->
<!--   OneMeanNotation[4, ] <- NA -->

<!--     kable( OneMeanNotation, -->
<!--          format = "html", -->
<!--          booktabs = TRUE, -->
<!--          longtable = FALSE, -->
<!--          escape = FALSE, -->
<!--          caption = "The notation used for describing means, and the sampling distribution of the sample means", -->
<!--          align = c("r", "l"), -->
<!--          linesep = c("\\addlinespace", -->
<!--                      "\\addlinespace", -->
<!--                      ""), -->
<!--          col.names = c("Quantity", -->
<!--                        "Description") ) %>% -->
<!-- 	row_spec(0, bold = TRUE)  -->
<!-- } -->
<!-- ``` -->


\begin{figure}
\begin{minipage}{0.42\textwidth}
\captionof{table}{The body temperature data: The first twenty of the 130 observations\label{tab:DataBodyTemp}}
\fontsize{8}{12}\selectfont
```{r}
if( knitr::is_latex_output() ) {
  
  BodyTdata <- array( dim = c(5, 4))
  BodyTdata[, 1] <- round(BodyTemp$BodyTempC[1:5],  2)
  BodyTdata[, 2] <- round(BodyTemp$BodyTempC[6:10],  2)
  BodyTdata[, 3] <- round(BodyTemp$BodyTempC[11:15],  2)
  BodyTdata[, 4] <- round(BodyTemp$BodyTempC[16:20],  2)
    
  kable( pad(BodyTdata,
             surroundMaths = TRUE,
             targetLength = c(5, 5, 5, 5),
             digits = 2),
         format = "latex",
         booktabs = TRUE,
         longtable = FALSE,
         escape = FALSE,
         linesep = "",
         #caption = "The body temperature data: The lowest five and the highest five of the 130 observations",
         align = "c",
         table.env = "@empty") %>%
    row_spec(0, bold = TRUE) %>%
    add_header_above(header = c("Body temperature (in ${}^{\\\\circ}$C)" = 4),
                     escape = FALSE,
                     bold = TRUE)
  
}
```
\end{minipage}
\hspace{0.05\textwidth}
\begin{minipage}{0.51\textwidth}%
\centering
```{r, out.width='95%', fig.width=4.75, fig.height = 3}
hist( BodyTemp$BodyTempC,
	xlab = "Body temperature, in degrees C",
	ylab = "Frequency",
	main = "Histogram of body temperatures",
	las = 1,
	breaks = seq(35, 39, by = 0.25),
	col = plot.colour)
box()
```
\caption{The histogram of the body temperature data\label{fig:BodyTempHist}}
\end{minipage}
\end{figure}



```{r DataBodyTemp, fig.align="center", fig.cap="The body temperature data"}
if( knitr::is_html_output() ) {
  DT::datatable( BodyTemp,
                 #fillContainer=FALSE, # Make more room, so we don't just have ten values
                 #filter="top", 
                 #selection="multiple", 
                 #escape=FALSE,
                 options = list(searching = FALSE)) # Remove searching: See: https://stackoverflow.com/questions/35624413/remove-search-option-but-leave-search-columns-option
}
```

<!-- The figure for LaTeX is in the minipage (combined with data table), so only need show it for the HTML -->
`r if (knitr::is_latex_output()) '<!--'`
```{r BodyTempHist, fig.show="hold", fig.cap="The histogram of the body temperature data", fig.align="center", out.width="50%", fig.width=4, fig.height=3}
hist( BodyTemp$BodyTempC,
	xlab = "Body temperature, in degrees C",
	ylab = "Frequency",
	main = "Histogram of body temperatures",
	las = 1,
	breaks = seq(35, 39, by = 0.25),
	col = plot.colour)
box()
```
`r if (knitr::is_latex_output()) '-->'`



```{r BodyTempjamovi, fig.cap="The jamovi summary of the body temperature data", fig.align="center", fig.height=3.0, out.width='65%'}
knitr::include_graphics( "jamovi/BodyTemp/BodyTemp-Summary.png")
```


The sampling distribution of $\bar{x}$ was given in Sect.\ \@ref(SamplingDistSampleMean).



::: {.definition #DEFSamplingDistributionXBar name="Sampling distribution of a sample mean"}
The *sampling distribution of the sample mean* is (when certain conditions are met; Sect.\ \@ref(ValiditySampleMeanTest)) described by

* an approximate normal distribution,
* centred around the sampling mean, whose value is $\mu$ (from $H_0$),
* with a standard deviation (called the *standard error* of $\bar{x}$) of  
\begin{equation}
   \text{s.e.}(\bar{x}) = \frac{s}{\sqrt{n}},
   (\#eq:StdErrorXbarTest)
\end{equation}
where $n$ is the size of the sample, and $s$ is the standard deviation of the data.
:::



Hence, if $\mu$ really was $37.0$, the possible values of the sample means across all possible samples can be described using:

* an approximate normal distribution,
* with a sampling mean whose value is $\mu = 37.0$ (from $H_0$),
* with a standard deviation of $\text{s.e.}(\bar{x}) = s/\sqrt{n} = 0.4073/\sqrt{130} = 0.0357$.


<!-- ::: {.importantBox .important data-latex="{iconmonstr-warning-8-240.png}"} -->
<!-- The notation $\text{s.e.}(\bar{x})$ is the *standard error of the sample mean*, and denotes 'the standard deviation of the means computed from all the possible samples'. -->
<!-- ::: -->


A picture of this sampling distribution (Fig.\ \@ref(fig:BodyTempSamplingDist)) shows how the sample mean varies when $n = 130$, for all possible samples, simply due to sampling variation when $\mu = 37.0$.
This enables questions to be asked about the likely values of $\bar{x}$ that would be found in the sample, when the population mean is $\mu = 37.0$.
For example, the value of $\bar{x}$ will be *larger* than $37.0357^\circ$C, if $\mu$ really is $37.0$, about $16$% of the time (using the $68$--$95$--$99.7$ rule).


```{r BodyTempSamplingDist, fig.cap="The distribution of sample mean body temperatures, if the population mean is $37.0^\\circ$C and $n = 130$. The grey vertical lines are $1$, $2$ and $3$ standard deviations from the mean.", fig.align="center", fig.height=2.5, fig.width=9, out.width='95%'}
mn <- 37.0

# These taken from Shoemaker's JSE data file
s <- 0.40732
n <- 130

se <- s/sqrt(n)
par(mar = c(3, 0.5, 1, 0.5))

out <- plotNormal(mn, 
                  s / sqrt(n),
                  xlab = "Sample means from sample of size 130 (deg C)",
                  round.dec = 4,
                  ylim = c(0, 16.25),
                  cex.tickmarks = 0.9)

arrows(x0 = mn,
       x1 = mn,
       y0 = 14, 
       y1 = max(out$y),
       length = 0.1,
       angle = 15)
text(x = mn,
     y = 14,
     pos = 3,
     labels = expression(Sampling~mean~mu) )


arrows(x0 = mn,
       x1 = mn + se,
       y0 = 3, 
       y1 = 3,
       length = 0.1,
       code = 3, # Arrow both ends
       angle = 15)
text(x = mn + (se / 2),
     y = 3,
     #pos = 3,
     labels = expression( atop(Std.~error,
                               s.e.(bar(italic(x)))==0.0357)) )
```




<iframe src="https://learningapps.org/watch?v=p28qr801322" style="border:0px;width:100%;height:600px" allowfullscreen="true" webkitallowfullscreen="true" mozallowfullscreen="true"></iframe>


## Computing the value of the test statistic: $t$-scores {#Tscores}
\index{Hypothesis testing!one mean}

The sampling distribution describes how the sample means varies; that is, what to *expect* from the sample means,  *assuming* $\mu = 37.0$.
The *observed* value of $\bar{x}$ is $\bar{x} = 36.8052^\circ$C.
How likely is it that such a value at least this small could occur in our sample by chance (i.e., by sampling variation)?

The value of the observed sample mean can be located on the sampling distribution (Fig.\ \@ref(fig:BodyTempSamplingDistT)).
The value $\bar{x} = 36.8052^\circ\text{C}$ is *extremely* small: a sample mean this low is very unlikely from a sample of $n = 130$ when $\mu = 37.0$.
How many standard deviations is $\bar{x}$ away from $\mu = 37.0$?


```{r BodyTempSamplingDistT, fig.cap="The sample mean of $\\bar{x} = 36.8041^\\circ$C is very unlikely to have been observed if the population mean really was $37.0^\\circ$C, and $n = 130$. The standard deviation of this normal distribution is $\\text{s.e.}(\\bar{x}) = 0.035724$.", fig.align="center", fig.width=8, fig.height=2.75, out.width='80%'}
mn <- 37.0
#These taken from Shoemaker's JSE data file
s <- 0.40732
n <- 130
xbar <- 36.8

se <- s/sqrt(n)

z36 <- (36.8 - mn)/(s / sqrt(n))

 
out <- plotNormal(mn,
                  s / sqrt(n),
                  xlab = "Sample mean temperatures (in deg C)",
                  round.dec = 3,
                  showX = seq(-6, 3, by = 1) * s/sqrt(n) + mn,
                  xlim.hi = mn + 3.5 * se, 
                  xlim.lo = mn - 6.5 * se,
                  cex.tickmarks = 0.9)

arrows(xbar, 
       0.8 * max(out$y), 
       xbar, 
       0, 
       length = 0.15, 
       angle = 15)
text(xbar, 
     0.8 * max(out$y), 
     "36.8 deg.",
     pos = 4)
```

Relatively speaking, the *distance* that the observed sample mean (of $\bar{x} = 36.8052$) is from the mean of the sampling distribution (Fig.\ \@ref(fig:BodyTempSamplingDistT)) is found by computing *how many* standard deviations the value of $\bar{x}$ is from the mean of the distribution:  
\[
   \frac{36.8052 - 37.0}{0.035724} = -5.453.
\]
This is *like* a $z$-score, but is actually called a $t$-score.
Both $t$ and $z$ scores measure *the number of standard deviations that a value is from the mean*.
Here we have a $t$-score, though, because the *population* standard deviation $\sigma$ is unknown, and the *sample* standard deviation is used to compute $\text{s.e.}(\bar{x})$.


::: {.tipBox .tip data-latex="{iconmonstr-info-6-240.png}"}
Like $z$-scores, $t$-scores measure the number of standard deviations that a value is from the mean.
Both measure the number of *standard errors* that a value is from the mean.
The difference between $t$- and $z$-scores is that:

* $t$-scores use a standard error involving sample estimates (as in this chapter, where $s$ is used).
* $z$-scores use a standard error *not* involving any sample estimates (as with a test for proportions (Chap. \@ref(TestOneProportion)).
:::


The calculation is therefore:  
\[
   t = \frac{36.8052 - 37.0}{0.035724} = -5.453;
\]
the observed sample mean is *more than five standard deviation below the population mean*, which is *highly* unusual based on the $68$--$95$--$99.7$ rule (Fig.\ \@ref(fig:BodyTempSamplingDistT)).

In general, a $t$-score in hypothesis testing is  
\begin{equation}
   t 
   = 
   \frac{\text{sample statistic} - \text{mean of the sampling distribution}}
        {\text{standard error of the sampling distribution}}
   =
   \frac{\bar{x} - \mu}{\text{s.e.}(\bar{x})}.
   (\#eq:tscore)
\end{equation}


<iframe src="https://learningapps.org/watch?v=pi8jnzhu322" style="border:0px;width:100%;height:500px" allowfullscreen="true" webkitallowfullscreen="true" mozallowfullscreen="true"></iframe>



## Determining $P$-values {#Pvalues}

As seen in Sect.\ \@ref(OnePropTestP), a $P$-value quantifies how unusual the observed sample statistic is, after assuming the null hypothesis is true.
Since $t$-scores and $z$-scores are very similar, the $P$-value can be *approximated* using the $68$--$95$--$99.7$ rule and a diagram (Sect.\ \@ref(ApproxP)), or using tables (`r if ( knitr::is_html_output()) { 'Appendix\\ \\@ref(ZTablesOnline).'} else {'Appendices\\ \\@ref(ZTablesNEG) and\\ \\@ref(ZTablesPOS)'}`).
Commonly, software is used to compute the $P$-value (Sect.\ \@ref(SoftwareP)).
$P$-values are very similar when $t$-scores and $z$-scores have the same value, except for small sample sizes.


### Approximate $P$-values  {#ApproxP}
\index{68@$68$--$95$--$99.7$ rule}

Since $t$-scores are similar to $z$-scores, the ideas in Sect.\ \@ref(OnePropTestP) can be used to *approximate* a $P$-value for a $t$-score.
In addition, tables of $z$-scores (`r if ( knitr::is_html_output()) { 'Appendix\\ \\@ref(ZTablesOnline).'} else {'Appendices\\ \\@ref(ZTablesNEG) and\\ \\@ref(ZTablesPOS)'}`) can be used to approximate the $P$-values for $t$-scores also (Sect.\ \@ref(OnePropTestPTables)).

Both methods produce approximate $P$-values only, since the approximations are based on using $z$-scores rather than $t$-scores.
Usually, software is used to determine $P$-values for $t$-scores.


### Exact $P$-values using software {#SoftwareP}

Software computes the $t$-score and a precise $P$-value (Fig.\ \@ref(fig:BodyTempTestjamovi)).
The output (in jamovi, under the heading `p`) shows that the $P$-value is indeed very small: less than $0.001$ (written as $P < 0.001$).


::: {.tipBox .tip data-latex="{iconmonstr-info-6-240.png}"}
Some software reports a $P$-value of `0.000`, which really means (and we should write) $P < 0.001$: that is, the $P$-value is *smaller* than $0.001$.
:::


This $P$-value means that, if $\mu = 37.0$, a sample mean as low as $36.8052$ would be *very* unusual to observe (from a sample size of $n = 130$). 
And yet... we did.
Using the decision-making process, this implies that the initial assumption (the null hypothesis) is contradicted by the data: we observed something extremely unlikely if $\mu = 37.0$.
That is, the evidence suggests that the *population* mean body temperature is *not* $37.0^\circ\text{C}$.


```{r BodyTempTestjamovi, fig.cap="jamovi output for conducting the $t$-test for the body temperature data", fig.align="center", out.width="65%"}
knitr::include_graphics("jamovi/BodyTemp/BodyTempTtest.png")
```


::: {.importantBox .important data-latex="{iconmonstr-warning-8-240.png}"}
For *one-tailed tests*, the $P$-value is *half* the value of the two-tailed $P$-value.
:::



### Making decisions with $P$-values

As seen in Sect.\ \@ref(OnePropTestDecisions), $P$-values measure the probability of observing the sample statistic (or something more extreme), assuming the population parameter is the value given in $H_0$.
For the body-temperature data then, where $P < 0.001$, the $P$-value is *very* small, so *very strong evidence* exists that the population mean body temperature is not $37.0^\circ\text{C}$.


## Writing conclusions

Communicating the results of any hypothesis test requires an *answer to the RQ*, a summary of the *evidence* used to reach that conclusion (such as the $t$-score and $P$-value, stating if it is a one- or two-tailed $P$-value), and some *sample summary information* (including a CI).
So for the body-temperature example, write:

> The sample provides very strong evidence ($t = -5.45$; two-tailed $P<0.001$) that the population mean body temperature is *not* $37.0^\circ\text{C}$ ($\bar{x} = 36.81$; $n = 130$; $95$% CI\ from 36.73$^\circ$C to 36.88$^\circ$C).

This statement contains the three components:

* The *answer to the RQ*.
  The sample provides very strong evidence...  that the population mean body temperature is not $37.0^\circ\text{C}$.
  The alternative hypothesis is two-tailed, so the conclusion is worded in terms of the population mean body temperature *not* being $37.0^\circ\text{C}$.
* The *evidence* used to reach the conclusion: $t = -5.45$; two-tailed  $P < 0.001$.
* Some *sample summary information* (including a CI, using details in Chap.\ \@ref(OneMeanConfInterval)): $\bar{x} = 36.81$; $n = 130$; $95$% CI from $36.73^\circ$C to $36.88^\circ$C.


::: {.importantBox .important data-latex="{iconmonstr-warning-8-240.png}"}
Since the *null* hypothesis is initially assumed to be true, the onus is on the evidence to refute the null hypothesis.
Hence, conclusions are worded in terms of how strongly the evidence (i.e., sample data) support the alternative hypothesis. 

The alternative hypothesis *may* or *may not* be true... but the evidence  (data) available here strongly supports the alternative hypothesis.
:::



## Process overview {#TestSummary}

Let's recap the decision-making process, in this context about body temperatures:

* *Step 1: Assumption*: 
  Write the *null hypothesis* about the parameter (based on the RQ): $H_0$: $\mu = 37.0$. 
  In addition, write the *alternative hypothesis*: $H_1$: $\mu \ne 37.0$. 
  (This alternative hypothesis is two-tailed.)
* *Step 2: Expectation*: 
  The *sampling distribution* describes what to expect from the statistic *if* the null hypothesis is true.
* *Step 3: Observation*: 
  Compute the $t$-score: $t = -5.45$. 
  The $t$-score can be computed by software, or using the general equation in Eq.\ \@ref(eq:tscore).
* *Step 4: Decision*: 
  Determine if the data are *consistent* with the assumption, by computing the $P$-value.
  Here, the $P$-value is much smaller than $0.001$.
  The $P$-value can be computed by software, or approximated using the $68$--$95$--$99.7$ rule.
  The *conclusion* is that there is very strong evidence that $\mu$ is not $37.0$.


## Statistical validity conditions {#ValiditySampleMeanTest}
\index{Statistical validity!one mean}

All hypothesis tests have underlying conditions  to be met so that the results are statistically valid; that is, the $P$-values can be found accurately because the sampling distribution is an approximate normal distribution.
For a hypothesis test for one mean, these conditions are the same as for the CI for one mean (Sect.\ \@ref(ValiditySampleMean)).

Statistical validity can be assessed using these criteria:

* When $n > 25$, the test is statistically valid provided the distribution of data is *not* highly skewed.
* When $n \le 25$, the test is statistically valid only if the data come from a *population* with a normal distribution.

The sample size of $25$ is a rough figure; some books give other values (such as $30$).
This condition ensures that the *distribution of the sample means has an approximate normal distribution* (so that, for example, the $68$--$95$--$99.7$ rule can be used).
Provided the sample size is larger than about $25$, this will be approximately true *even if* the distribution of the individuals in the population does not have a normal distribution.
That is, when $n > 25$ the sample means generally have an approximate normal distribution, even if the data themselves do not have a normal distribution.


::: {.example #StatisticalValidityTemps name="Statistical validity"}
The hypothesis test regarding body temperature is statistically valid since the sample size is larger tahn $25$ ($n = 130$).
(The data do not need come from a population with a normal distribution.)
:::


## Example: student IQs {#IQstudents}

Standard IQ scores are designed to have a mean in the general population of $100$.
@reilly2022gender studied $n = 224$ students at Griffith University, and found the sample mean IQ was $111.19$, with a standard deviation of $14.21$. 
Is this evidence that students at Griffith University (GU) have a *higher* mean IQ than the general population?

The RQ is:

> For students at Griffith University, is the mean IQ higher than $100$?
  
The parameter is $\mu$, the population mean IQ for students at GU.
The statistical hypotheses are:  
\[
   \text{$H_0$: $\mu = 100 \qquad \text{and} \qquad H_1$: $\mu > 100$.}
\]
This test is *one-tailed*, since the RQ asks if the IQ of GU students is *greater* than $100$.
(Writing $H_0$: $\mu\le 100$ is also correct (and equivalent), though the test still proceeds as if $\mu = 100$.)

We do not have the original data, but the summary data are sufficient: $\bar{x} = 111.19$ with $s = 14.21$ from a sample of size $n = 224$. 
The *sample* mean is higher than $100$, but we know sample mean vary.
The sample means vary with a normal distribution, with mean $100$ and a standard deviation of  
\[
   \text{s.e.}(\bar{x}) = \frac{s}{\sqrt{n}} = \frac{14.21}{\sqrt{224}} = 0.94945.
\]
The $t$-score is  
\[
   t = \frac{\bar{x} - \mu_{\bar{x}}}{\text{s.e.}(\bar{x})} = \frac{111.19 - 100}{0.94945} = 11.786.
\]

This $t$-score is *huge*: a sample mean as large as $111.19$ would be highly unlikely to occur simply by sampling variation in a sample of size $n = 224$ if the population mean really was $100$.
Since the alternative hypothesis is *one-tailed*, and specifically asking if $\mu > 100$, the $P$-value is the area in the right-side tail of the distribution (Fig.\ \@ref(fig:IQSamplingDistribution)); it will be extremely small.

   
```{r IQSamplingDistribution, fig.cap="The sampling distribution for the IQ data", fig.align="center", fig.width=10, fig.height=3, out.width='90%'}
mn.IQ <- 100
sd.IQ <- 14.21
sigma.IQ <- 15
n.IQ <- 100
se.IQ <- sd.IQ/sqrt(n.IQ)
xbar <- 111.19

z <- (xbar - mn.IQ)/se.IQ

out <- plotNormal(mn.IQ,
                  se.IQ, 
                  main = "The sampling distribution of the sample mean IQ",
                  xlab = "",
                  xlim.hi = 112,
                  las = 2,
                  ylim = c(0, 0.4),
                  round.dec = 2) 
# xlab, placed better
mtext(text = expression(Sample~mean~IQ),
      at = 107,
      side = 1,
      line = 1)
shadeNormal(out$x,
            out$y,
            lo = xbar,
            hi = 110,
            col = plot.colour)

arrows( x0 = xbar,
        x1 = xbar,
        y0 = 0.9 * max(out$y),
        y1 = 0,
        length = 0.1,
        angle = 15)
text( x = xbar, 
      y = 0.9 * max(out$y), 
      expression( paste(bar(italic(x)) == "111.19")),
      pos = 3)
mtext( text = expression( italic(z) == 11.78 ),
       at = xbar, 
      side = 1,
      line = 1)
points(x = xbar,
       y = 0,
       pch = 19)

arrows(x0 = 101,
       x1 = 105,
       y0 = 0.30,
       y1 = 0.30,
       angle = 15,
       length = 0.1)
text(x = 103,
     y = 0.30,
     #pos = 1,
     labels = expression( atop(The~one*"-"*tailed~italic(P)*"-"*value~is~the,
                               area~"in"~this~tail~only)))


```


We conclude (where the CI is found using the ideas in Sect.\ \@ref(OneMeanCI)):

> Very strong evidence exists in the sample ($t = 11.78$; one-tailed $P < 0.001$) that the population mean IQ in students at Griffith University is greater than $100$ (mean $111.19$; $n = 224$; $95$% CI from $109.29$ to $113.09$).

The test is about the *mean* IQ; *individual* students may have IQs less than $100$.

Since the sample size is much large than $25$, this conclusion is *statistically valid*.
The sample is not a true random sample from the population of all GU students (the students are mostly first-year students, and most were enrolled in an undergraduate psychological science degree). 
However, these students may be  somewhat representative of all GU student; those in the sample are probably not that different to students not in the sample.
That is, the sample *may* be externally valid.

The difference between the general population IQ of $100$ and the sample mean IQ of GU students is only small: about $11$ IQ units (less than one standard deviation).
Possibly, this difference has very little practical importance,\index{Practical importance} even though the statistical evidence suggests that the difference cannot be explained by chance.

IQ scores are designed to have a standard deviation of $\sigma = 15$ in the general population.
If we accept that this applies for university students too (we do not know if it does), the standard error is $\text{s.e.} = \sigma/\sqrt{n} = 15/\sqrt{130} = 1.0022$, and the test-statistic is a $z$-score:  
\[
  z = \frac{\bar{x} - \mu}{\text{s.e.}(\bar{x})} = \frac{111.19 - 100}{1.0022} = 11.87;
\]
the conclusions do not change.


## Chapter summary {#Chap27-Summary}

To test a hypothesis about a population mean $\mu$:

* Write the null hypothesis ($H_0$) and the alternative hypothesis ($H_1$).
* Initially *assume* the value of $\mu$ in the null hypothesis to be true.
* Then, describe the *sampling distribution*, which describes what to *expect*  from the sample mean based on this assumption: under certain statistical validity conditions, the sample mean varies with:
   *  an approximate normal distribution,
   *  with sampling mean whose value is the value $\mu$ (from $H_0$), and
   *  having a standard deviation of $\displaystyle \text{s.e.}(\bar{x}) =\frac{s}{\sqrt{n}}$.
* Compute the value of the *test statistic*:  
\[
   t = \frac{ \bar{x} - \mu}{\text{s.e.}(\bar{x})},
\]
where $\mu$ is the hypothesised value given in the null hypothesis.
* The $t$-value is like a $z$-score, and so an approximate *$P$-value* can be estimated using the $68$--$95$--$99.7$ rule, or found using software.


`r if (knitr::is_html_output()){
  'The following short video may help explain some of these concepts:'
}`


<iframe width="560" height="315" src="https://www.youtube.com/embed/ZbJ58Ag22Mw" frameborder="0" allow="accelerometer; encrypted-media; gyroscope; picture-in-picture"></iframe>


## Quick review questions  {#Chap27-QuickReview}


::: {.webex-check .webex-box}
The usual engineering recommendation is that the safe gap between travelling vehicles in traffic (a 'headway') is *at least* $1.9$\ s (often conveniently rounded to $2$\ s).
@majeed2014field studied $n = 28$ streams of traffic in Birmingham, Alabama found the mean headway was $1.1915$\ s, with a standard deviation of $0.231$\ s.
The researchers wanted to test if the mean headway in Birmingham was *less than* the recommended $1.9$\ s.

1. True or false? The test is *one-tailed*. \tightlist
`r if( knitr::is_html_output() ) {torf(answer=TRUE)}`
1. What is the standard error of the mean?
`r if( knitr::is_html_output() ) {
   paste("(Use *five* decimal places.) ", fitb(num=TRUE, tol=0.00001, answer=0.04365) )
}`
1. What is the null hypothesis?
`r if( knitr::is_html_output() ) {longmcq( c(
  "The sample mean headway is 1.9s",
  "The sample mean is 1.1915s",
  answer = "The population mean is 1.9s",
  "The population mean is 1.1915s")  )}`
1. What is the value of the test statistic?
`r if( knitr::is_html_output() ) {
   paste("(Use *two* decimal places.) ", fitb(num=TRUE, tol=0.01, answer=-16.23) )
}`
1. What is the value of the one-tailed $P$-value?
`r if( knitr::is_html_output() ) {mcq( c(
  answer = "Small",
  "Big",
  "0.05",
  "0.34")  )}`
1. True or false? There is no evidence to accept the *alternative* hypothesis (that the mean headway is less than $1.9$\ s).
`r if( knitr::is_html_output() ) {torf(answer=TRUE)}`
:::




<!-- ::: {.webex-check .webex-box} -->
<!-- A study [@imtiaz2017assessment] compared the nutritional intake of $n = 50$ anaemic infants in Lahore (Pakistan) with the recommended daily intake (of 13g). -->
<!-- The mean daily protein intake in the sample was 14g, with a standard deviation of 3g. -->
<!-- The researchers wanted to see if the mean intake met (or exceeded) the recommendation, or not. -->
<!-- ::: -->



## Exercises {#TestOneMeanAnswerExercises}

Answers to odd-numbered exercises are available in App.\ \@ref(Answers).


::: {.exercise #OneTSpeed}
@azwari2021evaluating studied driving speeds in Malaysia, and recorded the speeds of vehicles on various roads.
One RQ is whether the mean speed of cars on one road was the posted speed limit of $90$\ km.h^-1^, or whether it was *higher*.

The researchers recorded the speed of $n = 400$ vehicles on this road, and found the mean and standard deviation of the speeds of individual vehicles were $\bar{x} = 96.56$ and $s = 13.874$.

1. Define the parameter of interest.
2. Write the statistical hypotheses.
3. Compute the standard error of the sample mean.
4. Sketch the sampling distribution of the sample mean.
5. Compute the test statistic, a $t$-score.
6. Determine the $P$-value.
7. Write a conclusion.
8. Is the test statistically valid?
:::


::: {.exercise #TestOneMeanExercisesToothbrushing}
Most dental associations^[Such as the *American Dental Association* and the *Australian Dental Association*.] recommend brushing teeth for *at least* two minutes. 
@data:Macgregor1979:BrushingDurationKids studied the brushing time for $85$ uninstructed school children from England ($11$ to $13$ years old) and found the mean brushing time was $60.3$\ s, with a standard deviation of $23.8$\ s.

1. Define the parameter of interest.
2. Write the statistical hypotheses.
3. Compute the standard error of the sample mean.
4. Sketch the sampling distribution of the sample mean.
5. Compute the test statistic, a $t$-score.
6. Determine the $P$-value.
7. Write a conclusion.
8. Is the test statistically valid?
:::


::: {.exercise #TestOneMeanExercisesAutomatedVehicles}
@data:greenlee2018:vehicles conducted a study of human--automation interaction with automated vehicles.
They were interested in whether the average mental demand of 'drivers' of automated vehicles was *higher* than the average mental demand for ordinary tasks.

In the study, the $n = 22$ participants 'drove' (in a simulator) an automated vehicle for $40$\ mins. 
While driving, the drivers monitored the road for hazards.
The researchers assessed the 'mental demand' placed on these drivers, where scores of $50$ over 'typically indicate substantial levels of workload' (p. 471).
For the sample, the mean score was $84.00$ with a standard deviation of $22.05$.

Is there evidence of a 'substantial workload' associated with monitoring roadways while 'driving' automated vehicles?
:::



:::{.exercise #TestOneMeanWaterTemp}
Health departments recommend that hot water be stored at $60^\circ$C or higher, to kill *legionella* bacteria (for example, 
`r if (knitr::is_latex_output()) {
   'Health and Safety Executive, UK).'
} else {
   '[Health and Safety Executive, UK](https://www.hse.gov.uk/legionnaires/things-to-consider.htm)).'
}`
@alary1991risk studied $n = 178$ Quebec homes with electric water heaters to see if the water temperature was less than $60^\circ$C (i.e., at risk).

They found the mean temperature was $56.6^\circ$C, with a standard error of $0.4^\circ$C. 
Is there evidence the mean water temperature in Quebec is too low?
:::



::: {.exercise #TestOneMeanExercisesCherryRipes}
A *Cherry Ripe* is a popular chocolate bar in Australia.
In 2017, 2018 and 2019, I sampled some *Cherry Ripe* Fun Size bars.
The packaging claimed that the Fun Size bars weigh $14$\ g (on average).
Use the jamovi summary of the data (Fig.\ \@ref(fig:CherryRipes201720182019)) to perform a hypothesis test to determine if the mean weight really is $14$\ g or not.
:::


```{r CherryRipes201720182019, fig.cap="jamovi output for the Cherry Ripes data", fig.align="center", out.width="65%"}
knitr::include_graphics( "jamovi/CherryRipe/CherryRipe-Descriptives.png")
```




::: {.exercise #TestOneMeanBloodLoss}
(This study was also seen in Exercise\ \@ref(exr:CIOneMeanBloodLoss).)
@data:Williams2007:BloodLoss] asked $n = 199$ paramedics to estimate the amount of blood on four different surfaces.
When the actual amount of blood spilt on concrete was $1000$\ ml, the mean guess was $846.4$\ ml (with a standard deviation of $651.1$\ ml).

Is there evidence that the mean guess really is $1000$\ ml (the true amount)?
Is this test statistically valid?
:::

```{r}
data(BloodLoss) ### Exercise

DataAndTargets <- array(dim = c(3, 4))
DataAndTargets[1, ]<- c(64.31, 19.24, 64.97, 19.40)
DataAndTargets[2, ]<- c(1.700, 0.588, 1.029, 0.413)
DataAndTargets[3, ]<- c(64.22, 19.01, 65.05, 19.45)

rownames(DataAndTargets) <- c("Mean of data", 
                              "Std. dev. of data", 
                              "Pre-determined target")
colnames(DataAndTargets) <- c("High level", 
                              "Mid level", 
                              "High level", 
                              "Mid level")
```

::: {.exercise #TestOneMeanExercisesSleep}
@lin2021sleep compared the average sleep times of Taiwanese pre-school children to the recommendation (of *at least* $10$ hours per night).
The summary of the data for weekend sleep-times is shown in Table\ \@ref(tab:SleepingSummary), for both boys and girls.
On average, do girls get *at least* $10$ hours of sleep per night?
Do boys?
:::

```{r SleepingSummary}
SleepArray <- array( dim = c(2, 3))

SleepArray[1, ] <- c(47,
                     8.50,
                     0.48)
SleepArray[2, ] <- c(39,
                     8.64,
                     0.37)
rownames(SleepArray) <- c("Boys",
                          "Girls")

if( knitr::is_latex_output() ) {
  kable(pad(SleepArray,
            surroundMaths = TRUE,
            targetLength = c(2, 4, 4),
            digits = c(0, 2, 2)),
        format = "latex",
        align = "c",
        col.names = c("Sample size",
                      "Sample mean",
                      "Sample std. dev."),
        booktabs = TRUE,
        escape = FALSE,
        caption = "Summary information for the Taiwanese pre-schoolers sleep times (in hours)") %>%
    row_spec(0, bold = TRUE) %>% 
    kable_styling(font_size = 8)  
}
if( knitr::is_html_output() ) {
  kable(pad(SleepArray,
            surroundMaths = TRUE,
            targetLength = c(2, 4, 4),
            digits = c(0, 2, 2)),
        format = "html",
        align = "c",
        col.names = c("Sample size",
                      "Sample mean",
                      "Sample std. dev."),
        booktabs = TRUE,
        caption = "Summary information for the Taiwanese pre-schoolers sleep times (in hours)") %>%
    row_spec(0, bold = TRUE)
}
```

::: {.exercise #TestOneMeanQualityControl}
[*Dataset*: `BloodLoss`]
A quality-control study [@feng2017application] assessed the accuracy of two instruments from a clinical laboratory, by comparing the reported luteotropichormone (LH) concentrations to known pre-determined values (Table\ \@ref(tab:QualityControlDataSummary)).

Perform a series of tests to determine how well the two instruments perform, for both high- and mid-level LH concentrations (from the data in
`r if (knitr::is_latex_output()) {
   'Table \\@ref(tab:QualityControlData)).'
} else {
   'below.'
}`

:::


```{r QualityControlData}
if( knitr::is_latex_output() ) {
  
  T1info <- rbind( head(BloodLoss[1:2], 5),
                   c("$\\vdots$", "$\\vdots$") )
  
  T1 <- kable( pad(T1info,
                   surroundMaths = TRUE,
                   targetLength = c(4, 4),
                   digits = c(2, 2)),
        format = "latex",
        align = "c",
        escape = FALSE,
        linesep = "",
        col.names = c("High level", 
                      "Mid level"),
        booktabs = TRUE,
        longtable = FALSE) %>%
    row_spec(row = 0,
             bold = TRUE) %>%
    add_header_above(header = c("Instrument 1" = 2), 
                   bold = TRUE, 
                   align = "c")

  T2info <- rbind( head(BloodLoss[, 3:4], 5),
                   c("$\\vdots$", "$\\vdots$") )
    
    T2 <- kable( pad(T2info,
                   surroundMaths = TRUE,
                   targetLength = c(4, 4),
                   digits = c(2, 2)),
        format = "latex",
        align = "c",
        linesep = "",
        col.names = c("High level", 
                      "Mid level"),
        booktabs = TRUE,
        escape = FALSE,
        longtable = FALSE) %>%
    add_header_above(header = c("Instrument 2" = 2), 
                   bold = TRUE, 
                   align = "c") %>%
    row_spec(row = 0,
             bold = TRUE)

  out <- knitr::kables(list(T1, T2),
                       format = "latex",
                       label = "QualityControlData",
                       caption = "The quality-control data: LH levels (in mIU/mL) for two instruments (only the first five observations shown)") %>% 
    kable_styling(font_size = 8)
  
  prepareSideBySideTable(out)

} 

if( knitr::is_html_output() ) {
  DT::datatable(BloodLoss,
               caption = "The quality-control data: LH levels (in mIU/mL) for two instruments",
               colnames = colnames(DataAndTargets),
               options = list(searching = FALSE), # Remove searching: See: https://stackoverflow.com/questions/35624413/remove-search-option-but-leave-search-columns-option
               filter = "none")
}
```


```{r QualityControlDataSummary}
if( knitr::is_latex_output() ) {
  kable( pad(DataAndTargets,
             surroundMaths = TRUE,
             targetLength = 6,
             digits = c(3, 3, 3, 3) ),
        format = "latex",
        caption = "Summary of the quality-control data for LH levels (in mIU/mL) for two instruments",
        booktabs = TRUE,
        align = "c",
        #align = c("p{17mm}", "p{17mm}","p{17mm}","p{17mm}"),
        escape = FALSE,
        longtable = FALSE) %>%
    row_spec(row = 0,  
             bold = TRUE) %>%
    row_spec(row = 3,  
             italic = TRUE) %>%
    kable_styling(font_size = 8) %>%
    add_header_above(header = c("", 
                                "Instrument 1" = 2, 
                                "Instrument 2" = 2), 
                   bold = TRUE, 
                   align = "c")
}
if( knitr::is_html_output() ) {
  kable(pad(DataAndTargets,
             surroundMaths = TRUE,
             targetLength = 6,
             digits = c(3, 3, 3, 3) ),
        format = "html",
        align = "c",
        caption = "Summary of the quality-control data for LH levels (in mIU/mL) for two instruments",
        booktabs = TRUE,
        longtable = FALSE)
}
```




<!-- QUICK REVIEW ANSWERS -->
`r if (knitr::is_html_output()) '<!--'`
::: {.EOCanswerBox .EOCanswer data-latex="{iconmonstr-check-mark-14-240.png}"}
\textbf{Answers to \textit{Quick Revision} questions:}
**1.** True.
**2.** $0.0436$.
**3** The population mean headway is $1.9$\ s.
**4.** $0.34$.
**5.** Big
**7.** There is \emph{no evidence} to support the alternative hypothesis.
:::
`r if (knitr::is_html_output()) '-->'`

