
# More about hypothesis testing {#MoreAboutTests}


<!-- Introductions; easier to separate by format -->
```{r, child = if (knitr::is_html_output()) {'./introductions/33-Testing-More-HTML.Rmd'} else {'./introductions/33-Testing-More-LaTeX.Rmd'}}
```


## Introduction {#Chap28-Intro}

In Chaps.\ \@ref(TestOneProportion) and\ \@ref(TestOneMean), hypothesis tests for one proportion and one mean were studied.
Later chapters discuss hypothesis tests in other contexts, too.

However, the general approach to hypothesis testing is the same for *any* hypothesis test.
This chapter discusses some general ideas in testing:

* the *assumptions* and forming hypotheses (Sect.\ \@ref(AboutHypotheses)).
* the *expectations* described by the sampling distribution (Sect.\ \@ref(SamplingDistributionsExpectation)).
* the *observations* and the test statistic (Sect.\ \@ref(TestStatistic)).
* quantifying the *consistency* between the statistic and parameter values: computing $P$-values (Sect.\ \@ref(AboutFindingPvalues)).
* interpreting $P$-values (Sect.\ \@ref(AboutPvalues)).
* making mistakes when making conclusions (Sect.\ \@ref(TypeErrors)).
* wording *conclusions* (Sect.\ \@ref(WordingConclusion)).
* practical importance and statistical significance (Sect.\ \@ref(PracticalSignificance)).
* statistical validity in hypothesis testing (Sect.\ \@ref(ValidityHTs)).


::: {.importantBox .important data-latex="{iconmonstr-warning-8-240.png}"}
Hypothesis testing starts by assuming the null hypothesis is true.
The onus is on the data to provide evidence to refute this default position.
:::



## About hypotheses and assumptions {#AboutHypotheses}

Two *statistical* hypotheses are made about the population parameter: the null hypothesis $H_0$, and the alternative hypothesis $H_1$.


### Null hypotheses {#HypothesisNull}
\index{Hypotheses!null}

Statistical hypotheses *are always about a population parameter* (which can take different forms, depending on the context).
Hypothesising, for example, that the *sample* mean body temperature is equal to $37.0^\circ\text{C}$ is silly, because it clearly isn't: the *sample* mean is $36.8052^\circ\text{C}$ (Chap.\ \@ref(TestOneMean)).
Besides, the RQ is about the unknown *population*: the **P** in **P**OCI stands for **P**opulation.

The *null hypothesis* $H_0$ offers one possible reason why the value of the sample statistic (such as the sample mean) is not the same as the value of the proposed population parameter (such as the population mean): *sampling variation*.
Every sample is different, and we have data from just one of many possible samples.
The *sample statistic* will vary from sample to sample; the statistic may not be equal to the *population parameter*, just because of the sample obtained

Null hypotheses always contain an 'equals', because (as part of the decision making process) a specific value must be assumed for the population parameter, so we can describe what we might expect from the sample.
For example: the population mean *equals* $100$, is *less than or equal to* $100$, or is *more than or equal to* $100$.


::: {.importantBox .important data-latex="{iconmonstr-warning-8-240.png}"}
Defining the parameter carefully is important!

For example, if a parameter is about the difference between two means (say, in Group\ A and Group\ B), then the parameter description must clarify if the parameter is the 'Group\ A mean minus the Group\ B mean', or the 'Group\ B mean minus the Group\ A mean'.
Either is fine (though one may be easier to understand), but the direction used must be clearly stated.
:::


The null hypothesis always assumes the difference between the statistic and the assumed value of the parameter is due to sampling variation.
This may mean:

* there is *no difference* between the parameter value in two (or more) groups;
* there is *no change* in the parameter value (i.e., compared to an established or accepted value); or
* there is *no relationship* as measured by a parameter value.


::: {.importantBox .important data-latex="{iconmonstr-warning-8-240.png}"}
The *null hypothesis* always has the form 'no difference, no change, no relationship' regarding the population parameter.
:::


::: {.definition #NullHypothesis name="Null hypothesis"}
The **null hypothesis** proposes that *sampling variation* explains the difference between the proposed value of the parameter, and the observed value of the statistic.\index{Sampling variation}
:::


### Alternative hypotheses {#HypothesisAlternative}
\index{Hypotheses!alternative}

The other statistical hypothesis is called the *alternative hypothesis* $H_1$.
The alternative hypothesis offers another possible reason why the value of the sample statistic (such as the sample proportion) is not the same as the value of the proposed population parameter (such as the population proportion): the value of the population parameter really is not the value claimed in the null hypothesis (but does not explain *why*).


::: {.definition #AltHypothesis name="Alternative hypothesis"}
The *alternative hypothesis* proposes that the difference between the proposed value of the parameter and the observed value of the statistic cannot be explained by *sampling variation*: It proposes that the value of the parameter is not the value claimed in the null hypothesis.
:::


Alternative hypotheses can be *one-tailed* or *two-tailed*.\index{Hypotheses!one-tailed}\index{Hypotheses!two-tailed}
A *two*-tailed alternative hypothesis means, for example, that the population mean could be either smaller *or* larger than what is claimed.
A *one*-tailed alternative hypothesis admits only one of those two possibilities.
Most (but certainly not all) hypothesis tests are two-tailed.

The decision about whether the alternative hypothesis is one- or two-tailed is made by reading the RQ (*not* by looking at the data).
*The RQ and hypotheses should (in principle) be formed before the data are obtained*, or at least before looking at the data if the data are already collected.

The idea of hypothesis testing is the same whether the alternative hypothesis is one- or two-tailed: based on the data and the sample statistic, a decision is to be made about whether the alternative hypotheses is supported by the data.


::: {.example #AltHypothesisBodyTemp name="Alternative hypotheses"}
For the body-temperature study, the alternative hypothesis is *two-tailed*: The RQ asks if the population mean is $37.0^\circ\text{C}$ or *not*. 
Two possibilities are considered: that $\mu$  could be either larger *or* smaller than $37.0^\circ\text{C}$.

A *one-tailed alternative hypothesis* would be appropriate if the RQ asked: 'Is the *population* mean internal body temperature *greater* than $37.0^\circ\text{C}$?',
or 'Is the *population* mean internal body temperature *smaller* than $37.0^\circ\text{C}$?'.
:::


::: {.importantBox .important data-latex="{iconmonstr-warning-8-240.png}"}
Important points about forming hypotheses:

* Hypotheses always concern a *population* parameter.
* Null hypothesis always have the form 'no difference, no change, no relationship'.
* Alternative hypothesis are one- or two-tailed, depending on the RQ.
* Null hypotheses always contain an 'equals'.
* Hypotheses emerge from the RQ (not the data): The RQ and the hypotheses could be written down *before* collecting the data.
:::


<iframe src="https://learningapps.org/watch?v=pdvd1n72j22" style="border:0px;width:100%;height:500px" allowfullscreen="true" webkitallowfullscreen="true" mozallowfullscreen="true"></iframe>


## About sampling distributions and expectations {#SamplingDistributionsExpectation}
\index{Sampling distribution}

The *sampling distribution* describes, approximately, how all possible values of the sample statistic (such as $\hat{p}$ or $\bar{x}$) vary across all possible samples, when $H_0$ is true:
it describes the *sampling variation*.\index{Sampling variation}
Under certain circumstances, many sampling distributions have an approximate normal distribution.

When the sampling distribution is described by a normal distribution, the *mean* of the normal distribution is the parameter value given in the *assumption* ($H_0$), and the *standard deviation* of the normal distribution is called the *standard error*.\index{Standard error}
In some cases, the sample statistic does not have a normal distribution, but a quantity easily derived from the sample statistic does have a normal distribution
(for example, in the case of odds ratios^[In this case, the *logarithm* of the odds ratio has an approximate normal distribution.] in Chap.\ \@ref(TestsOddsRatio)).


## About observations and the test statistic {#TestStatistic}

The sampling distribution describes what values the statistic can take over all possible samples of given size.
Since the sampling distribution often has an approximate normal distribution, the observed value of the sample statistic can be expressed as $z$-score or a $t$-score:\index{test statistic}  
\[
   \text{test statistic} = 
   \frac{\text{sample statistic} - \text{centre of the sampling distribution}}
        {\text{standard deviation of the sampling distribution}}.
\]
The $z$-scores and $t$-scores are called *test statistics*, since their values are based on sample data ('a statistic') and used in a hypothesis test.
Other test statistics are used too (as in Chap.\ \@ref(TestsOddsRatio)).


::: {.tipBox .tip data-latex="{iconmonstr-info-6-240.png}"}
\index{test statistic!t@$t$-score}\index{test statistic!z@$z$-score}
A $t$-score and $z$-scores both measure the number of standard deviations an observation is from the mean:
\[
   \frac{\text{a specific value of something that varies} - \text{the mean of the distribution}}
        {\text{the standard deviation of the distribution}}.
\]
Then:

* If the 'quantity that varies' is an *individual* observation $x$, the measure of variation is the standard deviation of the individual observations. 
* If the 'quantity that varies' is a *sample statistic*, the measure of variation is a *standard error*, which measures the variation in the sample statistic. 

In both cases, the *test statistic* is a $t$-score if the measure of variation uses *sample* estimates.
:::


## About finding $P$-values {#AboutFindingPvalues}
\index{P@$P$-values}

As demonstrated in Sect.\ \@ref(OnePropTestP), $P$-values can be *approximated* using Tables or the $68$--$95$--$99.7$ rule when the sampling distribution has a normal distribution.
The $P$-value is the area *more extreme* than the calculated $z$- or $t$-score; the $68$--$95$--$99.7$ rule can be used to approximate this tail area.

For *two-tailed* tests, the $P$-value is the *combined* area in the left and right tails.\index{Hypotheses!one-tailed}\index{Hypotheses!two-tailed}\index{P@$P$-values!one-tailed}\index{P@$P$-values!two-tailed}
For *one-tailed* tests, the $P$-value is the area in just the left or right tail (as appropriate, according to the alternative hypothesis; see Sect.\ \@ref(IQstudents)).


::: {.importantBox .important data-latex="{iconmonstr-warning-8-240.png}"}
When software reports *two-tailed $P$-values*, a one-tailed $P$ is found by *halving the two-tailed $P$-value*.
:::


More accurate estimates of the $P$-value can be found using tables.
For precise $P$-values, use the $P$-values from software output.


::: {.softwareBox .software data-latex="{iconmonstr-laptop-4-240.png}"}
When using software to obtain $P$-values, if the software reports one- or two-tailed $P$-values.\index{P@$P$-values!reporting very small}
Some software (such as SPSS) always reports two-tailed $P$-values.
:::


## About interpreting $P$-values {#AboutPvalues}
\index{P@$P$-values!interpretation}\index{Hypothesis testing!interpretation}

A $P$-value is the probability of observing the value of the sample statistic (or something even more extreme) over repeated sampling, under the assumption that the null hypothesis is true.
Since the null hypothesis is initially assumed true, *the onus is on the data to present evidence to the contrary*.


::: {.importantBox .important data-latex="{iconmonstr-warning-8-240.png}"}
Conclusion are *always* about the population parameters.
$P$-values are needed to determine what we learn about the unknown *population* parameters, based on what we observed from one of the many possible values of the *sample* statistic.
:::


Commonly, a $P$-value smaller than $5$% is considered 'small', but this is *arbitrary* and sometimes the threshold is discipline-dependent.
More reasonably, $P$-values should be interpreted as giving varying degrees of evidence in support of the alternative hypothesis (Table\ \@ref(tab:PvaluesInterpretation)), but these are only guidelines.


```{r PvaluesInterpretation}
Meaning.latex <- c("\\emph{Insufficient} evidence to support $H_1$",
                "\\emph{Slight} evidence to support $H_1$",
                "\\emph{Moderate} evidence to support $H_1$",
                "\\emph{Strong} evidence to support $H_1$",
                "\\emph{Very strong} evidence to support $H_1$")
Meaning.html <- c("*Insufficient* evidence to support $H_1$",
                "*Slight* evidence to support $H_1$",
                "*Moderate* evidence to support $H_1$",
                "*Strong* evidence to support $H_1$",
                "*Very strong* evidence to support $H_1$")


Pvals <- c(0.5, 0.075, 0.025, 0.005, 0)

if( knitr::is_html_output() ) {
   PVTable <- data.frame( Value = c("Larger than 0.10", 
                                    "Between 0.05 and 0.10", 
                                    "Between 0.01 and 0.05",
                                    "Between 0.001 and 0.01",
                                    "Smaller than 0.001"),
                          Meaning = Meaning.html)

      PVTable %>%
      mutate_if(is.numeric, function(x) {
                cell_spec(x,
                          color = spec_color(x, end = 0.9),
                          font_size = spec_font_size(x))
     }) %>%
     mutate(Meaning = cell_spec(
                 Meaning, 
                 color = "white", 
                 bold = TRUE,
                 background = spec_color(1:5, 
                                         begin = 0.2, 
                                         end = 0.9, 
                                         option = "D", 
                                         direction = -1) # FOR COLORS, see  https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html
            )) %>%
     kable(escape = FALSE, 
           align = c("r", "l"), 
           format = "html",
           caption = "A guideline for interpreting \\(P\\)-values. \\(P\\)-values should be interpreted in context.",
           col.names = c("If the $P$-value is...", 
                         "Write the conclusion as...")) %>%
     kable_styling(c("striped", "condensed"), 
                   full_width = F)    
}
 
  
  
if( knitr::is_latex_output() ) {
   PVTable <- data.frame(Value = c("Larger than $0.10$", 
                                   "Between $0.05$ and $0.10$", 
                                   "Between $0.01$ and $0.05$",
                                   "Between $0.001$ and $0.01$",
                                   "Smaller than $0.001$"),
                          Meaning = Meaning.latex
                         )

   kable(PVTable,
         format = "latex",
         booktabs = TRUE,
         escape = FALSE,
         longtable = FALSE,
         col.names = c("If the $P$-value is...", "Write the conclusion as..."),
         caption = "A guideline for interpreting \\(P\\)-values. \\(P\\)-values should be interpreted in context.",
         align = c("r", "l")   ) %>%
     row_spec(0, bold = TRUE) %>%
     kable_styling(font_size = 8) 
  
}

```

::: {.definition #Pvalue name="$P$-value"}
A $P$-value is the likelihood of observing the sample statistic (or something more extreme) over repeated sampling, under the assumption that the null hypothesis about the population parameter is true.
:::


::: {.importantBox .important data-latex="{iconmonstr-warning-8-240.png}"}
$P$-values are never *exactly* zero.
jamovi reports very small $P$-values as '$P < 0.001$' (i.e., the $P$-value is smaller than $0.001$).
:::


$P$-values are commonly used in research, but must be used and interpreted correctly [@greenland2016statistical].
Specifically:

* A $P$-value *is not* the probability that the null hypothesis is true.
* A $P$-value *does not prove* anything (only one possible samples was studied).
* A big $P$-value *does not* mean the null hypothesis $H_0$ is true, or that $H_1$ is false.
* A small $P$-value *does not* mean the null hypothesis $H_0$ is false, or that $H_1$ is true.
* A small $P$-value *does not* mean the results are practically important (Sect.\ \@ref(PracticalSignificance)).
* A small $P$-value does not mean a large difference between the statistic and parameter; it means that the difference (whether large or small) could not reasonably be attributed to *sampling variation* (chance). 


::: {.importantBox .important data-latex="{iconmonstr-warning-8-240.png}"}
When the results of a study are reported as being *statistically significant*,\index{Statistical significance} this usually means that the $P$-value is less than $0.05$... though a different $P$-value is sometimes used as the 'threshold', so check!
To avoid confusion, the word 'significant' should be avoided in writing about research unless 'statistical significance' is what is actually what is meant.
In other situations, consider using words like 'substantial'.
:::


<iframe src="https://learningapps.org/watch?v=p04ya6w8t22" style="border:0px;width:100%;height:550px" allowfullscreen="true" webkitallowfullscreen="true" mozallowfullscreen="true"></iframe>


## About making mistakes in conclusions {#TypeErrors}

Hypothesis testing is about making a decision about a *population* using a *sample*.
Since the sample is just one of countless possible samples that could have been observed, making an incorrect conclusion is always a possibility.

Two mistakes can be made:

* *Incorrectly* concluding that evidence supports the alternative hypothesis.
  Of course, the researchers *do not know they are incorrect*... but the possible of making this mistake is always present when concluding that the evidence supports the alternative hypothesis.
  This is a *false positive*, or a *Type\ I error*.\index{Type\ I error}
* *Incorrectly* concluding that there is *no* evidence to support the alternative hypothesis.
  Of course, the researchers *do not know they are incorrect*... but the possible of making this mistake is always present when concluding that there is no evidence to support the alternative hypothesis.
  This is a *false negative*, or a *Type\ II error*.\index{Type\ II error}

Ideally, neither of these errors would be made; however, sampling variation that neither can ever be completely eliminated.
In practice, hypothesis testing begins by assuming the null hypothesis is true, and hence places the onus on the data to provide compelling evidence in favour of the alternative hypothesis.
This means researchers usually aim to minimising the chance of a Type\ I error.

A Type\ I error is like declaring an innocent person guilty (recall innocence is presumed).
Similarly, a Type\ II error is like declaring a guilty person innocent.
The law generally sees a Type\ I error as more grievous that a Type\ II error, just as in research.



## About writing conclusions {#WordingConclusion}
\index{Hypothesis testing!writing conclusions}
In general, communicating the results of any hypothesis test requires:

1. the *answer to the RQ*;
2. the *evidence* used to reach that conclusion (such as the $t$-score and $P$-value, clarifying if the $P$-value is *one-tailed* or *two-tailed*); and
3. *sample summary statistics* (such as sample means and sample sizes), including a CI (indicating the precision with which the parameter has been estimated).

Since we assume the null hypothesis is true, conclusions are worded (in context) in terms of how strongly the evidence supports the alternative hypothesis.


::: {.importantBox .important data-latex="{iconmonstr-warning-8-240.png}"}
Since the null hypothesis is initially assumed to be true, the onus is on the data to provide evidence in support of the alternative hypothesis.
Hence, conclusions are always worded in terms of how much evidence supports the *alternative* hypothesis.
:::


::: {.importantBox .important data-latex="{iconmonstr-warning-8-240.png}"}
'No evidence of a difference' is **not** the same as 'evidence of no difference'. 

That is, suppose we conclude that 'there is no evidence that the mean IQ is greater than $100$ in race-car drivers'.
This **does not** mean there is evidence that evidence that the mean IQ for race-car drivers is $100$ (or is lower than $100$).
:::


<iframe src="https://learningapps.org/watch?v=p13paniqk22" style="border:0px;width:100%;height:750px" allowfullscreen="true" webkitallowfullscreen="true" mozallowfullscreen="true"></iframe>


## About practical importance and statistical significance {#PracticalSignificance}
\index{Practical importance}\index{Statistical significance}

Hypothesis tests assess *statistical significance*, which answers the question: 'Can sampling variation explain the difference between the value of the statistic and the value of the assumed parameter?'
Even very small differences between the sample statistic and the population parameter can be *statistically* different if the sample size is sufficiently large. 

In contrast, *practical importance* answers the question: 'Is the conclusion of any importance *in practice*?'
Whether a results is of practical importance depends upon the context: what the data are being used for, and for what purpose.
'Practical importance' and 'statistical significance' are two separate issues.


::: {.example #PracticalImportance name="Practical importance"}
In the body-temperature study (Sect.\ \@ref(BodyTemperature)), very strong evidence exists that the mean body temperature had changed ('statistical significance').
But the change was so small that, for most purposes, it has no practical importance.
(In other (e.g., medical) situations, it *may* have practical importance however.)
:::


::: {.example #PracticalImportanceHerbal name="Practical importance"}
A study of some herbal medicines [@maunder2020effectiveness] for weight loss found that the intervention:

> ... resulted in a statistically significant weight loss compared to placebo, although this was not considered clinically significant. 

This means that the difference in weight loss between placebo and intervention was unlikely to be explained by chance ($P < 0.001$; i.e., 'statistical significant'), but the difference was so small (a mean weight loss of $1.61$\ kg) that it was unlikely to be of any use in practice ('practical importance').
In this context, the researchers decided that a weight loss of at least $2.5$\ kg was of practical importance.
:::


<iframe src="https://learningapps.org/watch?v=p6yimkdn522" style="border:0px;width:100%;height:500px" allowfullscreen="true" webkitallowfullscreen="true" mozallowfullscreen="true"></iframe>


## About statistical validity {#ValidityHTs}
\index{Statistical validity}

When performing hypothesis tests, certain *statistical validity conditions* must be true to ensure that the mathematics behind computing the $P$-value is sound.
For instance, if the sampling distribution has an approximate normal distribution, the statistical validity conditions ensure that the approximation is sufficiently accurate for the $68$--$95$--$99.7$ rule to apply^[Not all sample statistics have normal distributions, but all the sample statistics in this book are either normally distributed or are closely related to normal distributions.].
If these conditions are *not* met, the sampling distribution may not be sufficiently close to an exact normal distribution, so the $P$-values (and hence conclusions) maybe inappropriate.
These statistical validity conditions are usually the same as, or similar to, those for the corresponding CIs (Sect.\ \@ref(ValidityCIs)).

In addition to the statistical validity condition, the *internal validity* and *external validity* of the study should be discussed (Fig.\ \@ref(fig:ValiditiesCI)).


## Chapter summary {#Chap28-Summary}

Hypothesis testing formalises the decision-making process.
Starting with an *assumption* about a population parameter of interest, a description of what values the sample statistic might take  (based on this assumption) is produced: this describes what values the statistic is *expected* to take over all possible samples.
This sampling distribution is often a normal distribution, or related to a normal distribution.

The sample statistic (the *estimate*) is then *observed*, and a *test statistic*, which often is a $z$- or  $t$-score,  is computed to describe this sample statistic.
Using a $P$-value, a decision is made about whether the sample evidence supports or contradicts the initial assumption, and hence a *conclusion* is made.
Since $t$-scores are like $z$-scores, $P$-values can often be approximated using the $68$--$95$--$99.7$ rule.


<iframe src="https://learningapps.org/watch?v=p9epa2fpj22" style="border:0px;width:100%;height:500px" allowfullscreen="true" webkitallowfullscreen="true" mozallowfullscreen="true"></iframe>


## Quick review questions {#Chap28-QuickReview}

::: {.webex-check .webex-box}
1. True or false? \tightlist
   When a $P$-value is very small, a very large difference exists between the statistic and parameter.  
`r if( knitr::is_html_output() ) { torf( answer=FALSE )}`
1. True or false?
The alternative hypothesis is one-tailed if the sample statistic is larger than the hypothesised population mean.  
`r if( knitr::is_html_output() ) { torf( answer=FALSE )}`
1. What is wrong (if anything) with this null hypothesis: $H_0 = 37$?  
`r if( knitr::is_html_output() ) { mcq( c(
  "There is nothing wrong",
  answer = "There is no parameter",
  "This is the alternative (not the null) hypothesis") )}`
1. True or false: When the sampling distribution is a normal distribution, the standard deviation of this normal distribution is called the *standard error*.  
`r if( knitr::is_html_output() ) { torf( answer=TRUE )}`
1. True or false? 
   Both $z$-scores and $t$-scores can be test statistics.  
`r if( knitr::is_html_output() ) { torf( answer=TRUE )}`
1. True or false?  $P$-values can never be exactly zero.  
`r if( knitr::is_html_output() ) { torf( answer=TRUE )}`
1. True or false? A $P$-value is the probability
   that the null hypothesis is true.  
`r if( knitr::is_html_output() ) { torf( answer=FALSE )}`
:::



## Exercises {#MoreAboutTestsExercises}

Answers to odd-numbered exercises are available in App.\ \@ref(Answers).


::: {.exercise #MoreAboutExercisesApproximatingPValues}
Assuming the tests are statistically valid, use the $68$--$95$--$99.7$ rule to approximate the *two*-tailed $P$-value if:

:::::: {.cols data-latex=""}

:::: {.col data-latex="{0.4\textwidth}"}
1. the $t$-score is $3.4$.
2. the $t$-score is $-2.9$.
::::

:::: {.col data-latex="{0.05\textwidth}"}
\ 
<!-- an empty Div (with a white space), serving as
a column separator -->
::::

:::: {.col data-latex="{0.5\textwidth}"}
3. the $t$-score is $-2.1$.
4. the $t$-score is $-6.7$.
::::
::::::
:::


::: {.exercise #MoreAboutExercisesApproximatingPValues2}
Assuming the tests are statistically valid, use the $68$--$95$--$99.7$ rule to approximate the *two*-tailed $P$-value if:

:::::: {.cols data-latex=""}
:::: {.col data-latex="{0.4\textwidth}"}
1. the $t$-score is $1.05$.
4. the $t$-score is $-1.3$.
::::

:::: {.col data-latex="{0.05\textwidth}"}
\ 
<!-- an empty Div (with a white space), serving as
a column separator -->
::::

:::: {.col data-latex="{0.5\textwidth}"}
3. the $t$-score is $6.7$.
4. the $t$-score is $0.1$.
::::
::::::
:::



::: {.exercise #MoreAboutExercisesApproximatingPValuesOneTailed}
Consider the $t$-scores in Exercise \@ref(exr:MoreAboutExercisesApproximatingPValues).
Use the $68$--$95$--$99.7$ rule to approximate the *one*-tailed $P$-values in each case.
:::



::: {.exercise #MoreAboutExercisesApproximatingPValuesOneTailed2}
Consider the $t$-scores in Exercise \@ref(exr:MoreAboutExercisesApproximatingPValues2).
Use the $68$--$95$--$99.7$ rule to approximate the *one*-tailed $P$-values in each case.
:::



::: {.exercise #MoreAboutTestsInterpretingResults}
Suppose a hypothesis test results in a $P$-value of $0.0501$.
What would we conclude?
What if the $P$-value was $0.0499$?
:::



::: {.exercise #MoreAboutTestsInterpretingResults2}
Suppose a hypothesis test results in a $P$-value of $0.011$.
What would we conclude?
What if the $P$-value was $0.009$?
:::



::: {.exercise #MoreAboutTestsInterpretingHypotheses}
Consider the study to determine the mean body temperature (Chap.\ \@ref(TestOneMean)), where $\bar{x} = 36.8052^{\circ}\text{C}$.
What, if anything, is wrong with these hypotheses?
Explain.

1. $H_0$: $\bar{x} = 37$; $H_1$: $\bar{x} \ne 37$.
2. $H_0$: $\mu = 37$; $H_1$: $\mu > 37$.
3. $H_0$: $\mu = 37$; $H_1$: $\mu = 36.8052$.
:::


::: {.exercise #MoreAboutTestsInterpretingHypotheses2}
Consider the study to determine the mean body temperature (Chap.\ \@ref(TestOneMean)), where $\bar{x} = 36.8052^{\circ}\text{C}$.
What, if anything, is wrong with these hypotheses?
Explain.

1. $H_0$: $\bar{x} = 36.8052$; $H_1$: $\bar{x} > 36.8052$.
2. $H_0$: $\mu = 36.8052$; $H_1$: $\mu \ne 36.8052$.
3. $H_0$: $\mu > 37$; $H_1$: $\bar{x} > 37$.
:::


::: {.exercise #MoreAboutTestsConclusions}
The recommended daily energy intake for women is $7725$\ kJ (for a particular cohort, in a particular country; @data:Altman1991:PracticalStats).
The daily energy intake for 11 women was measured to see if this is being adhered to.
The RQ was 'Is the population mean daily energy intake $7725$\ kJ?'

The test produced $P = 0.018$.
What, if anything, is wrong with these conclusions after completing the hypothesis test?

1. There is moderate evidence ($P = 0.018$) that the energy intake is not meeting the recommended daily energy intake.
1. There is moderate evidence ($P = 0.018$) that the sample mean energy intake is not meeting the recommended daily energy intake.
1. There is moderate evidence ($P = 0.018$) that the population energy intake is not meeting the recommended daily energy intake.
1. The study proves that the population energy intake is not meeting the recommended daily energy intake ($P = 0.018$).
:::


```{r}
data(Battery)

out <- t.test(Time ~ Brand, 
              data = subset(Battery, Voltage == 1.1) )
```


::: {.exercise #MoreAboutTestsBatteries}
A study compared ALDI batteries to another brand of battery.
In one test comparing the length of time it takes for $1.5$\ volt AA batteries to reach $1.1$\ volts, the ALDI brand battery took $5.73$\ hours, and the other brand (Energizer) took $5.44$\ hours [@mypapers:Dunn:BatteryData].

1. What is the null hypothesis for the test?
1. The $P$-value for comparing these two means is about $P = 0.70$. What does this mean?
1. Is this difference likely to be of any practical importance? Explain.
1. What would be a correct conclusion for ALDI to report from the study? 
   Explain.
1. What else would be useful to know in comparing the two brands of batteries?
:::



 

<!-- QUICK REVIEW ANSWERS -->
`r if (knitr::is_html_output()) '<!--'`
::: {.EOCanswerBox .EOCanswer data-latex="{iconmonstr-check-mark-14-240.png}"}
\textbf{Answers to \textit{Quick Revision} questions:}
**1.** False.
**2.** False.
**3.** There is no parameter.
**4.** True.
**5.** True.
**6.** True.
**7.** False.
:::
`r if (knitr::is_html_output()) '-->'`


