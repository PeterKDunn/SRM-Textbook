
# CIs for one mean {#OneMeanConfInterval}


::: {.objectivesBox .objectives data-latex="{iconmonstr-target-4-240.png}"}
So far,
you have learnt to
ask a RQ, 
identify different ways of obtaining data,
design the study,
collect the data
describe the data,
summarise data graphically and numerically,
and
understand the tools of inference.

**In this chapter**,
you will learn about *confidence intervals* for one mean.
You will learn to:

* produce confidence intervals for one mean.
* determine whether the conditions for using the 
  confidence intervals apply in a given situation.
* compute sample size estimates in this situation.
:::






```{r echo=FALSE, fig.cap="", fig.align="center", fig.width=3, out.width="35%"}
SixSteps(5, "CIs: One mean")
```




## Sampling distribution: One mean with population standard deviation known

```{r echo=FALSE}
S.fun <- 
structure(list(Year = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 
4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 
4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 
4L, 4L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 6L, 6L, 6L, 
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 
9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 
9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 
9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 
9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L), .Label = c("Before 2011", "2011", 
"2012", "2013", "2014", "2015", "2016", "2017", "2018"), class = "factor"), 
    Pink = c(3, 1, 1, 4, 2, 1, 3, 4, 3, 3, 4, NA, NA, NA, 1, 
    2, 1, 1, 1, 3, 2, 3, 2, 0, 1, 2, 1, 3, 1, 3, 1, 2, 1, 2, 
    2, 2, 3, 2, 2, 0, 1, 1, 1, 4, NA, NA, NA, NA, NA, NA, NA, 
    NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 3, 3, 
    3, 0, 4, 5, 0, 2, 3, 1, 1, 4, 2, 1, 3, 4, 3, 3, 4, 3, 2, 
    2, 3, 2, 3, 2, 2, 2, 2, 1, 2, 1, 2, 1, 4, 1, 2, 3, 3, 3, 
    0, 2, 0, 5, 2, 3, 2, 4, 1, 0, 1, 1, 2, 1, 2, 3, 2, 0, 1, 
    0, 2, 2, 3, 1, 1, 2, 3, 4, 3, 3, 2, 3, 1, 1, 2, 2, 1, 1, 
    1, 2, 2, 3, 1, 1, 0, 3, 3, 1, 2, 2, NA, NA, NA, NA, NA, NA, 
    NA, NA, NA, NA, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 2, 2, 0, 1, 
    1, 3, 2, 2, 4, 6, 1, 3, 0, 1, 2, 2, 2, 2, 1, 1, 1, NA, NA, 
    NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
    NA, NA, NA, 1, 1, 0, 3, 2, 1, 0, 1, 0, 2, 2, 2, 0, 2, 2, 
    1, 2, 2, 2, 4, 3, 3, 1, 0, 1, 1, 4, 5, 1, 1, 5, 4, 4, 2, 
    2, 3, 0, 3, 3, 2, 1, 1, 2, 1, 3, 2, 3, 1, 1, 3, 2, 1, 0, 
    2, 1, 1, 0, 5, 0, 1, 2, 2, 0, 1, 4, 4, 0, 2, 2, 0, 2, 3, 
    1, 1, 4, 2, 0, 4, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
    NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 0, 0, 1, 
    1, 1, 2, 3, 1, 4, 1, 3, 0, 1, 2, 3, 1, 3, 3, 0, 3), Number = c(13, 
    11, 14, 13, 13, 11, 13, 11, 15, 13, 13, 14, 12, 13, 12, 12, 
    12, 9, 13, 11, 12, 12, 12, 10, 13, 13, 13, 11, 11, 12, 11, 
    10, 13, 12, 11, 12, 12, 10, 12, 13, 9, 13, 13, 12, 12, 13, 
    13, 12, 13, 12, 13, 12, 14, 13, 14, 11, 11, 12, 14, 12, 12, 
    13, 14, 14, 14, 12, 12, 13, 13, 13, 14, 12, 13, 11, 14, 13, 
    13, 11, 13, 11, 15, 13, 13, 12, 13, 14, 13, 14, 13, 13, 15, 
    13, 15, 13, 12, 13, 13, 13, 16, 14, 14, 14, 13, 14, 13, 12, 
    11, 12, 13, 16, 13, 13, 13, 12, 13, 12, 12, 12, 10, 13, 11, 
    12, 11, 9, 12, 12, 9, 12, 12, 11, 13, 14, 12, 13, 12, 15, 
    13, 12, 13, 12, 15, 12, 13, 12, 14, 14, 14, 14, 13, 15, 11, 
    12, 12, 13, 14, 12, 12, 11, 13, 14, 12, 13, 15, 13, 14, 13, 
    13, 13, 12, 14, 14, 13, 13, 13, 11, 14, 13, 12, 12, 15, 13, 
    12, 13, 13, 14, 14, 12, 13, 14, 11, 14, 13, 10, 13, 12, 12, 
    12, 13, 13, 13, 13, 12, 12, 14, 13, 12, 12, 13, 13, 12, 13, 
    12, 13, 14, 11, 12, 12, 12, 13, 11, 13, 11, 12, 12, 11, 11, 
    11, 11, 12, 12, 12, 12, 13, 12, 11, 12, 11, 12, 12, 12, 12, 
    11, 13, 13, 12, 12, 12, 11, 12, 13, 13, 13, 12, 13, 12, 12, 
    11, 12, 12, 13, 12, 11, 12, 11, 13, 13, 12, 12, 11, 12, 12, 
    12, 13, 14, 12, 13, 12, 13, 12, 13, 13, 13, 14, 15, 13, 14, 
    13, 14, 14, 13, 13, 12, 15, 13, 13, 13, 11, 13, 13, 12, 12, 
    14, 13, 13, 13, 12, 14, 13, 13, 13, 13, 13, 11, 13, 13, 13, 
    13, 14, 13, 15, 13, 14, 11, 14, 14, 14, 13, 13, 12, 13, 13, 
    13, 13, 14, 14), filter_. = structure(c(1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L
    ), .Label = c("Not Selected", "Selected"), class = "factor"), 
    Prop = c(0.230769230769231, 0.0909090909090909, 0.0714285714285714, 
    0.307692307692308, 0.153846153846154, 0.0909090909090909, 
    0.230769230769231, 0.363636363636364, 0.2, 0.230769230769231, 
    0.307692307692308, NA, NA, NA, 0.0833333333333333, 0.166666666666667, 
    0.0833333333333333, 0.111111111111111, 0.0769230769230769, 
    0.272727272727273, 0.166666666666667, 0.25, 0.166666666666667, 
    0, 0.0769230769230769, 0.153846153846154, 0.0769230769230769, 
    0.272727272727273, 0.0909090909090909, 0.25, 0.0909090909090909, 
    0.2, 0.0769230769230769, 0.166666666666667, 0.181818181818182, 
    0.166666666666667, 0.25, 0.2, 0.166666666666667, 0, 0.111111111111111, 
    0.0769230769230769, 0.0769230769230769, 0.333333333333333, 
    NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
    NA, NA, NA, NA, NA, 0.214285714285714, 0.25, 0.25, 0, 0.307692307692308, 
    0.384615384615385, 0, 0.166666666666667, 0.230769230769231, 
    0.0909090909090909, 0.0714285714285714, 0.307692307692308, 
    0.153846153846154, 0.0909090909090909, 0.230769230769231, 
    0.363636363636364, 0.2, 0.230769230769231, 0.307692307692308, 
    0.25, 0.153846153846154, 0.142857142857143, 0.230769230769231, 
    0.142857142857143, 0.230769230769231, 0.153846153846154, 
    0.133333333333333, 0.153846153846154, 0.133333333333333, 
    0.0769230769230769, 0.166666666666667, 0.0769230769230769, 
    0.153846153846154, 0.0769230769230769, 0.25, 0.0714285714285714, 
    0.142857142857143, 0.214285714285714, 0.230769230769231, 
    0.214285714285714, 0, 0.166666666666667, 0, 0.416666666666667, 
    0.153846153846154, 0.1875, 0.153846153846154, 0.307692307692308, 
    0.0769230769230769, 0, 0.0769230769230769, 0.0833333333333333, 
    0.166666666666667, 0.0833333333333333, 0.2, 0.230769230769231, 
    0.181818181818182, 0, 0.0909090909090909, 0, 0.166666666666667, 
    0.166666666666667, 0.333333333333333, 0.0833333333333333, 
    0.0833333333333333, 0.181818181818182, 0.230769230769231, 
    0.285714285714286, 0.25, 0.230769230769231, 0.166666666666667, 
    0.2, 0.0769230769230769, 0.0833333333333333, 0.153846153846154, 
    0.166666666666667, 0.0666666666666667, 0.0833333333333333, 
    0.0769230769230769, 0.166666666666667, 0.142857142857143, 
    0.214285714285714, 0.0714285714285714, 0.0714285714285714, 
    0, 0.2, 0.272727272727273, 0.0833333333333333, 0.166666666666667, 
    0.153846153846154, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
    0.0714285714285714, 0.0769230769230769, 0.0769230769230769, 
    0.0769230769230769, 0.166666666666667, 0.142857142857143, 
    0.0714285714285714, 0.0769230769230769, 0.153846153846154, 
    0.0769230769230769, 0.181818181818182, 0.142857142857143, 
    0, 0.0833333333333333, 0.0833333333333333, 0.2, 0.153846153846154, 
    0.166666666666667, 0.307692307692308, 0.461538461538462, 
    0.0714285714285714, 0.214285714285714, 0, 0.0769230769230769, 
    0.142857142857143, 0.181818181818182, 0.142857142857143, 
    0.153846153846154, 0.1, 0.0769230769230769, 0.0833333333333333, 
    NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
    NA, NA, NA, NA, NA, 0.0833333333333333, 0.0833333333333333, 
    0, 0.230769230769231, 0.181818181818182, 0.0769230769230769, 
    0, 0.0833333333333333, 0, 0.181818181818182, 0.181818181818182, 
    0.181818181818182, 0, 0.166666666666667, 0.166666666666667, 
    0.0833333333333333, 0.166666666666667, 0.153846153846154, 
    0.166666666666667, 0.363636363636364, 0.25, 0.272727272727273, 
    0.0833333333333333, 0, 0.0833333333333333, 0.0833333333333333, 
    0.363636363636364, 0.384615384615385, 0.0769230769230769, 
    0.0833333333333333, 0.416666666666667, 0.333333333333333, 
    0.363636363636364, 0.166666666666667, 0.153846153846154, 
    0.230769230769231, 0, 0.25, 0.230769230769231, 0.166666666666667, 
    0.0833333333333333, 0.0909090909090909, 0.166666666666667, 
    0.0833333333333333, 0.230769230769231, 0.166666666666667, 
    0.272727272727273, 0.0833333333333333, 0.0909090909090909, 
    0.230769230769231, 0.153846153846154, 0.0833333333333333, 
    0, 0.181818181818182, 0.0833333333333333, 0.0833333333333333, 
    0, 0.384615384615385, 0, 0.0833333333333333, 0.153846153846154, 
    0.166666666666667, 0, 0.0833333333333333, 0.307692307692308, 
    0.307692307692308, 0, 0.142857142857143, 0.133333333333333, 
    0, 0.142857142857143, 0.230769230769231, 0.0714285714285714, 
    0.0714285714285714, 0.307692307692308, 0.153846153846154, 
    0, 0.266666666666667, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
    NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 0, 0, 
    0.0714285714285714, 0.0769230769230769, 0.0666666666666667, 
    0.153846153846154, 0.214285714285714, 0.0909090909090909, 
    0.285714285714286, 0.0714285714285714, 0.214285714285714, 
    0, 0.0769230769230769, 0.166666666666667, 0.230769230769231, 
    0.0769230769230769, 0.230769230769231, 0.230769230769231, 
    0, 0.214285714285714)), .Names = c("Year", "Pink", "Number", 
"filter_.", "Prop"), row.names = c(NA, -335L), class = "data.frame", variable.labels = structure(c("", 
"Number of pink Smarties", "Number of Smarties", "(Year = 2018) (FILTER)", 
""), .Names = c("Year", "Pink", "Number", "filter_$", "Prop")), codepage = 65001L)
```



```{r echo=FALSE, results='hide'}
# Find the first ten in each class
S.fun <- subset(S.fun,  
                Year !="Before 2011")
S.fun$Year <- as.numeric( as.character(S.fun$Year) )

Yr.min <- 2013 #  min(S.fun$Year)
Yr.max <- max( S.fun$Year )

nn <- 10

new.df <- rep(-1, nn)

yrs <- NULL

col.number <- 1 # Recall col 1 is the dummy data!

for (i in (Yr.min:Yr.max) ) {
  S.sub <-  S.fun[S.fun$Year == i, ]
  
  n.sub <- length(S.sub$Number)
  
  n.groups.of.ten <- n.sub%/%nn
  
  if (n.groups.of.ten > 0 ) {
  
    for (j in (1:n.groups.of.ten)){
      
      col.number <- col.number + 1

      new.col <- S.sub$Number[((j - 1) * nn + 1) : (j * nn)]

      new.col.name <- paste(i, "-", j, sep="")
      new.df <- cbind(new.df, 
                      new.col.name = new.col)
      colnames( new.df ) [col.number] <- new.col.name

      yrs <- c(yrs, i)
    }
  }
}
# Now remove the dummy first col
new.df <- new.df[, -1]

# Add years
new.df2 <- data.frame( "Mn" = colMeans(new.df), 
                       "Year" = yrs) 
Num.Bags <-  dim(new.df)[2]

# Each year's data:
for (i in (Yr.min:Yr.max)){
  tmp.mns <- new.df2$Mn[new.df2$Year == i ]
  
  VALS <- paste(tmp.mns, 
                collapse = ", ")
   eval(
   	parse( 
   		text = paste("YR", i, " <- c(", VALS, ")", sep = "" )  
   		) 
   	)
}



# Means
mns <- colMeans(new.df)
```



<div style="float:right; width: 222x; border: 1px; padding:10px">
<img src="Illustrations/pexels-skitterphoto-705171.jpg" width="200px"/>
</div>



`r if (knitr::is_html_output()) '<!--'`
\begin{wrapfigure}{R}{.25\textwidth}
  \begin{center}
    \includegraphics[width=.20\textwidth]{Illustrations/pexels-skitterphoto-705171.jpg}
  \end{center}
\end{wrapfigure}
`r if (knitr::is_html_output()) '-->'`




In this chapter,
we study the situation where a population mean $\mu$ (the parameter)
is estimated by a sample mean $\bar{x}$ (the statistic).

Of course,
every sample
is likely to be different,
and is likely to produce a different sample mean $\bar{x}$.
That is,
the value of the sample mean will vary from sample to sample
and exhibit *sampling variation*
(which can be quantified using the *standard error*).

Consider rolling dice again.
Suppose a die is rolled $n=25$ times, and
the *mean* of the 25 numbers that are rolled is recorded.
What will be the sample mean of the numbers in the 25 rolls?

```{r echo=FALSE}
die.mn <- sum( (1:6) * (1/6))  # 3.5 as expected


die.vr <-  sum( ((1:6) - 3.5)^2 * (1/6))
die.se <- sqrt( die.vr/25)
```

The sample mean will vary from sample to sample,
(*sampling variation*).
Since every face of the die is equally likely to appear
on any one roll,
the population mean of all possible rolls is $\mu=3.5$
(in the middle of the numbers on the faces of the die,
which is also the *median*).

An example of the mean after repeatedly rolling a die 25 times is
`r if (knitr::is_latex_output()) {
   'shown in Fig. \\@ref(fig:RollDiceMeanFig) for 10 sets of 25 rolls.  (The online version has an animation.)'
} else {
   'shown in the animation below for 10 sets of 25 rolls.'
}`
The mean of the 25 rolls clearly varies.
In the simulation,
the sample mean of 25 rolls was as low as 3.08 and as high as 3.76.




```{r RollDiceMeanHTML, animation.hook="gifski", echo=FALSE, interval=0.5, dev=if (is_latex_output()){"pdf"}else{"png"}}
if (knitr::is_html_output()){
  set.seed(99999)
    num.rolls <- 25
    num.sims <- 10
    x.loc <- 1:(num.rolls)
    y.loc <- 1
    mean.even <- array(dim = num.sims)
    all.rolls <- array( dim = c(num.sims, num.rolls))
    for (i in 1:num.sims){
      plot( c(1, (num.rolls + 2)), c(1, num.sims),
            type = "n",
            las = 1,
            xlab = "",
            ylab = "",
            main = paste("Set number", i),
            axes = FALSE)
      
      roll <- sample(1:6, 
                     num.rolls, 
		     replace = TRUE)
      all.rolls[i, ] <- roll
      mean.even[i] <- mean(roll)
      
      col1 <- col2rgb("darkolivegreen2")
      col2 <- col2rgb("indianred2")
 
      for (j in 1:i){
        text(y = j, 
	     x = 1:num.rolls,
             labels = all.rolls[j, ])
        
        # Add some slight background colour under the sample proportions
        polygon( c(num.rolls + 1, num.rolls + 1, num.rolls + 3, num.rolls + 3),
                 c(j - 0.5, j + 0.5, j + 0.5, j - 0.5),
                 border = NA,
                 col = ifelse( mean.even[j] > 3.5, rgb( col1[1], col1[2], col1[3], alpha = 75, max = 255), 
                                                   rgb( col2[1], col2[2], col2[3], alpha = 75, max = 255) ) )

      }
      # Add x-bar heading
      mtext(expression(bar( italic(x) ) ), 
            side = 3, 
	    line = 0, 
	    at = num.rolls + 2 )
      # Add the roll number to the left-hand side
      axis(side = 2, 
           at = 1:i,
           las = 1,
           labels = paste("Set #", 1:i, sep = "") )
      
      # Add the sample mean to right-hand side 
      text(num.rolls + 2, 
           1:i, 
           labels = format(round(mean.even[1:i], 2), nsmall = 2) )
      #Add dividing line
      abline(v = num.rolls + 1, 
             col = "grey")
      # Add line dividing roll sets
      abline(h = seq(0.5, 10.5, by = 1), 
             col = "gray")
    }
  }
```






```{r RollDiceMeanFig, echo=FALSE, fig.align="center", cache=FALSE, fig.width=7, out.width="85%", fig.cap="Rolling dice: The average of 25 rolls, for 10 sets of 25 rolls" }
if (knitr::is_latex_output()){
  set.seed(99999)
    num.rolls <- 25
    num.sims <- 10
    x.loc <- 1:(num.rolls)
    y.loc <- 1
    mean.even <- array(dim = num.sims)
    all.rolls <- array( dim = c(num.sims, num.rolls))

      plot( c(1, (num.rolls + 2)), c(1, num.sims),
            type = "n",
            las = 1,
            xlab = "",
            ylab = "",
            main = "The sample mean from 25 rolls\nover 10 simulations",
            axes = FALSE)

    for (i in 1:num.sims){
      roll <- sample(1:6, 
                     num.rolls, 
		     replace = TRUE)
      all.rolls[i, ] <- roll
      mean.even[i] <- mean(roll)
      for (j in 1:i){
        text(y = j, 
	     x = 1:num.rolls,
             labels = all.rolls[j, ])
      }
      # Add x-bar heading
      mtext(expression(bar( italic(x) ) ), 
            side = 3, 
	    line = 0, 
	    at = num.rolls + 2 )
      # Add the roll number to the left-hand side
      axis(side = 2, 
           at = 1:i,
           las = 1,
           labels = paste("Set #", 1:i, sep = "") )
      
      # Add the sample mean to right-hand side 
      text(num.rolls + 2, 1:i, 
           labels = format(round(mean.even[1:i], 1), nsmall = 1) )

      #Add dividing line
      abline(v = num.rolls + 1, 
             col = "grey")
	     
      # Add line dividing roll sets
      abline(h = seq(0.5, 10.5, by = 1), 
             col = "gray")
    }
}
```


The mean for any single *sample* of $n = 25$ rolls 
will sometimes be higher than $\mu = 3.5$,
and sometimes lower than $\mu = 3.5$,
but most of the time the mean should be close to 3.5.

If many people made a set of 25 rolls, 
and computed the mean for their set,
every person would have a sample mean for their set of 25 rolls,
and we could produce a histogram of all these sample means;
`r if (knitr::is_latex_output()) {
   'see Fig. \\@ref(fig:RollDiceHistMeanFig).  (The online version has an animation.)'
} else {
   'see the animation below.'
}`





```{r RollDiceHistMeanHTML, cache=FALSE, echo=FALSE, animation.hook="gifski", interval=0.4, dev=if (is_latex_output()){"pdf"}else{"png"}}
if (knitr::is_html_output()){
  set.seed(123456789)
    num.sims <- 1000
    num.rolls <- 25
    
    
    print_Histo <- rep(FALSE, num.sims)
    print_Histo[ c( 1:10,
                    seq(24, num.sims, 25) + 1,
                    num.sims) ] <- TRUE
    
    die.mn <- sum( (1:6) * (1/6))  # 3.5 as expected
    die.vr <-  sum( ((1:6) - 3.5)^2 * (1/6))
    
    meanList <- array( dim = num.sims)
    
    for (i in 1:num.sims){
      meanSingleRoll <- mean(sample(1:6, 
                                    num.rolls, 
			            replace = TRUE))
      meanList[i] <- meanSingleRoll
      
      
      #Print every nth histogram only
      if (print_Histo[i]){
        out <- hist(meanList,
                    xlab = "Mean of 25 rolls",
                    ylab = "Number of times observed",
                    main = paste("Histogram of the mean of 25 rolls:\nSimulation number:", i),
                    sub = paste("(For this set: mean roll is ", 
                                format(round(meanList[i], 2), nsmall = 2), 
                                ")", 
                                sep = "" ),
                    col = plot.colour,
                    las = 1,
                    xlim = c(1.5, 5.5), 
                    right = FALSE,
                    ylim = c(0, 300),
                    breaks = seq(2, 5, by = 0.25)
        )
        
        points(meanList[i], 0,
               pch = 19,
               col = plot.colour0)
        
        xx <- seq(1, 6, length=500)
        yy <- dnorm(xx, 
                    mean = die.mn, 
                    sd = sqrt(die.vr/num.rolls) )
        yy <- yy/max(yy) * max(out$count)
        
        lines(yy ~ xx, 
              col = "grey", 
              lwd = 2)  
      }
    }
  }
```



```{r RollDiceHistMeanFig, echo=FALSE, fig.align="center", cache=FALSE, fig.width=5, fig.cap="Rolling dice: The mean of 25 rolls, for 1000 repetitions" }
if (knitr::is_latex_output()){
  set.seed(123456789)
    num.sims <- 1000
    num.rolls <- 25
    
    die.mn <- sum( (1:6) * (1/6))  # 3.5 as expected
    die.vr <-  sum( ((1:6) - 3.5)^2 * (1/6))
    
    meanList <- array( dim=num.sims)
    
    for (i in 1:num.sims){
      meanSingleRoll <- mean(sample(1:6, 
                                    num.rolls, 
				    replace = TRUE))
      meanList[i] <- meanSingleRoll
    }

    out <- hist(meanList,
                  xlab = "Mean of 25 rolls",
                  ylab = "Number of times observed",
                  main = paste("Histogram of the mean of 25 rolls:\nafter",
                                num.sims, "simulations"),
                  col = plot.colour,
                  las = 1,
                  xlim = c(1.5, 5.5), 
		  right = FALSE,
                  ylim = c(0, 300),
                  breaks = seq(2, 5, by = 0.25)
      )
      xx <- seq(1, 6, 
                length = 500)
      yy <- dnorm(xx, 
                  mean = die.mn, 
		  sd = sqrt(die.vr/num.rolls) )
      yy <- yy/max(yy) * max(out$count)

      lines(yy ~ xx, 
            col = "grey", 
	    lwd = 2)  
}      
```



`r if (knitr::is_latex_output()) {
   'From Fig. \\@ref(fig:RollDiceHistMeanFig),'
} else {
   'From the animation above,'
}`
the sample means appear to vary 
with an approximate normal distribution
(as we saw with the sample proportions).
This normal distribution is centred around the population mean $\mu$.
The standard deviation of the normal distribution is
the *standard error* of the sample mean $\bar{x}$,
written as
$\text{s.e.}(\bar{x})$.

When the *population* standard deviation $\sigma$ is *known*,
then

\[
  \text{s.e.}(\bar{x}) = \frac{\sigma}{\sqrt{n}}.
\]
So the possible values of the sample means 
have a *sampling distribution*
described by:

* an approximate normal distribution,
* with mean $\mu$, and
* a standard deviation, 
  called the standard error,
  of $\text{s.e.}(\bar{x}) = \sigma/\sqrt{n}$.

Usually the population mean and the population standard deviation
are *unknown*.
Nonetheless,
because the sampling distribution has an approximate normal distribution,
the 68--95--99.7 rule can be applied:
approximately 95% of the sample means are expected to be within
two standard errors of $\mu$.





## Sampling distribution: One mean with population standard deviation unknown {#SamplingDistSampleMean}

When a sample mean is used to estimate a population mean,
the sample mean will vary from sample to sample:
sampling variation exists,
as we saw in the previous section.

When we do not know the *population* standard deviation $\sigma$ (which is almost always the case),
we estimate it using the *sample* standard deviation $s$.
Then,
the *standard error* of the sample mean is
$\displaystyle\text{s.e.}(\bar{x}) = \frac{s}{\sqrt{n}}$.
With this information,
we can describe the *sampling distribution of the sample mean*\index{sampling distribution!sample mean}.


::: {.definition #DEFSamplingDistributionXbar name="Sampling distribution of a sample mean"}
When the *population* standard deviation is unknown,
the *sampling distribution of the sample mean* is described by:
  
* an approximate a normal distribution,
* centred around $\mu$,
* with a standard deviation (called the *standard error of the mean*)
of 

\begin{equation}
   \text{s.e.}(\bar{x}) = \frac{s}{\sqrt{n}},
   (\#eq:stderrorxbar)
\end{equation}
when [certain conditions are met](#ValiditySampleMean),
where $n$ is the size of the sample,
and $s$ is the standard deviation of the individual observations in the sample
(that is, the *sample* standard deviation). 
:::












## Confidence intervals: One mean {#OneMeanCI}

We\index{confidence interval!means}
don't know the value of $\mu$ (the paremeter),
the population mean,
but we have an *estimate*:
the value of $\bar{x}$, the sample mean (the statistic).
The actual value of $\mu$ might be a bit larger than $\bar{x}$, 
or a bit smaller than $\bar{x}$;
that is,
$\mu$ is probably about $\bar{x}$, give-or-take a bit.

Furthermore,
we have seen that the values of $\bar{x}$ vary from sample to sample
(*sampling variation*),
and noted that they vary with an approximate normal distribution.
So,
using the [68--95--99.7 rule](#def:EmpiricalRule),
we could create an approximate 95% interval 
for the plausible values of $\mu$ that may have given the observed values of the sample mean.
This is a *confidence interval*.

A confidence interval (CI) for the population mean is an interval surrounding a sample mean.
In general,
an approximate 95% confidence interval (CI) for $\mu$ is
$\bar{x}$ give-or-take about two standard errors.
In general,
the [confidence interval](#def:ConfidenceInterval) (CI) for $\mu$ is

\[
   \bar{x} \pm \overbrace{(\text{Multiplier}\times\text{s.e.}(\bar{x}))}^{\text{Called the `margin of error'}}.
\]
For an approximate 95% CI,
the multiplier is, as usual, about $2$
(since about 95% of values are within two standard deviations of the mean
from the [68--95--99.7 rule](#def:EmpiricalRule)).

We often find 95% CIs,
but we can find a CI  with *any* level of confidence:
we just need a different multiplier.
We'll just use a multiplier of $2$ (and hence find *approximate* 95% CIs),
and otherwise use software.
Commonly,
CIs\ are computed at 90%, 95% and 99% confidence levels.


::: {.tipBox .tip data-latex="{iconmonstr-info-6-240.png}"}
The multiplier of 2 is not a $z$-score here.
The multiplier would be a $z$-score *if* we knew the value of $\sigma$;
since we don't,
the multiplier is a $t$-score and not a $z$-score.

The $t$- and $z$-multipliers are very similar,
and (except for very small sample sizes) 
using an approximate multiplier of 2 is reasonable for computing
*approximate* 95% CIs in either case.

We'll let software handle the specifics.
:::


If we collected many samples of a specific size,
$\bar{x}$ and $s$ would be different for each sample,
so the calculated
CI would be different for each.
Some CIs would straddle the population mean $\mu$,
and some would not;
and we never know if the CI computed from our single sample straddles $\mu$ or not.

Loosely speaking,
there is a 95% chance that our 95% CI  straddles $\mu$.
For a CI computed from a single sample,
we don't know if our CI includes the value of $\mu$ or not.
The CI could also be interpreted as the range of plausible values of $\mu$
that could have produced the observed value of $\bar{x}$.


<div style="float:right; width: 222x; border: 1px; padding:10px">
<img src="Illustrations/pexels-pixabay-207697.jpg" width="200px"/>
</div>



`r if (knitr::is_html_output()) '<!--'`
\begin{wrapfigure}{R}{.25\textwidth}
  \begin{center}
    \includegraphics[width=.20\textwidth]{Illustrations/pexels-pixabay-207697.jpg}
  \end{center}
\end{wrapfigure}
`r if (knitr::is_html_output()) '-->'`


::: {.example #BagsCI name="School bags"}
A study of the school bags that 586 children (in Grades 6--8 in Tabriz, Iran) take to school
found that the mean weight was $\bar{x} = 2.8$&nbsp;kg
with a standard deviation of $s=0.94$&nbsp;kg
[@data:Dianat2014:schoolbags].

The parameter is the population mean weight of school bags for Iranian children in Grades 6--8.

Of course,
another sample of 586 children would produce a different sample mean:
the sample mean varies from sample to sample.

The *standard error* of the sample mean is

\[
   \text{s.e.}(\bar{x}) = \frac{s}{\sqrt{n}} = \frac{0.94}{\sqrt{586}} = 0.03883;
\]
see Fig. \@ref(fig:BagsNormal).
The approximate 95% CI for the population mean school-bag weight is

\[
  2.8\pm(2 \times 0.03883),  
\]
or $2.8\pm0.07766$.
(The *margin of error* is 0.07766.)
This is equivalent to an approximate 95% CI from 2.72&nbsp;kg to 2.88&nbsp;kg.
This CI has a 95% chance of straddling the population mean bag weight.
:::


::: {.thinkBox .think data-latex="{iconmonstr-light-bulb-2-240.png}"}
Would a 99% CI for $\mu$ be *wider* or *narrower* than the 95% CI?  Why?         

(Answer is here^[A *wider* interval is needed to be *more* confident that the interval contains the population mean.].)
:::







```{r BagsNormal, echo=FALSE, fig.cap="The normal distribution, showing how the sample mean bag weight varies in samples of size $n=586$", fig.align="center", fig.width=5, fig.height=3}
mn <- 2.8
n <- 586
stdd <- 0.94

se <- stdd/sqrt(n)

plot.norm(mu = mn, 
          sd = se, 
          xlab.name = "Sample mean bag weight, in kg (sample size: 586)", 
          round.dec = 3,
          shade.lo.x = -10, 
          shade.hi.x = -10,
           axis.labels = c( 	
                expression( paste("    ", mu, " - 0.114")),
                expression( paste("    ", mu, " - 0.076")), 
                expression( paste("    ", mu, " - 0.038")), 
                expression( mu ),
                expression( paste("    ", mu, " + 0.038")), 
                expression( paste("    ", mu, " + 0.076")), 
                expression( paste("    ", mu, " + 0.114")) ),
                cex.tickmarks = 0.8,
               srt = -25 # Rotate axis labels so they fit better
)
```



::: {.example #OneMeanCIBears name="Black bears"}
A study of American black bears [@bartareau2017estimating]
found that the mean weight of the $n = 185$ male bears in their study was 
$\bar{x} = 84.9$&nbsp;kg, with a standard deviation of $s = 51.1$&nbsp;kg.


The *parameter* of interest is the population mean weight of an American black bear, $\mu$.

Using the sample information,
the standard error of the mean is

\[
  \text{s.e.}(\bar{x}) = \frac{s}{\sqrt{n}} = \frac{51.1}{\sqrt{185}} = 3.756947,
\]
so the approximate 95% CI is from 
\[
  84.9 - (2 \times 3.756947) = 77.38611
\]
to
\[
  84.9 + (2 \times 3.756947) = 92.41389.
\]
The *approximate* 95% CI is from 77.4 to 92.4&nbsp;kg.
We would say:
  
> We are approximately 95% confidence that the *population mean* weight of 
> male American black bears is between 77.4 and 92.4&nbsp;kg.


The article gives the *exact* CI as 77.4 to 99.5&nbsp;kg, agreeing with the CI we calculated.
:::
         





## Statistical validity conditions: One mean {#ValiditySampleMean}

As with any inference procedure,
the underlying mathematics requires 
[certain conditions to be met](#exm:StatisticalValidityAnalogy)
so that the
results are statistically valid.
The CI  for one mean,
will be *statistical valid* if
*one* of these is true:

1. The sample size is at least 25, *or*
2. The sample size is smaller than 25
		  *and*
		  the *population* data has an approximate normal distribution.

The sample size of 25 is a rough figure here, and some books give other (similar) values (such as 30).
This condition
ensures that the *distribution of the sample means has an approximate normal distribution*
so that the [68--95--99.7 rule](#def:EmpiricalRule) can be used.

Provided the sample size is larger than about 25,
this will be approximately true
*even if* the distribution of the individuals in the
population does not have a normal distribution.
That is,
when $n > 25$
the sample means generally have an approximate normal distribution,
even if the data themselves don't have a normal distribution.


::: {.importantBox .important data-latex="{iconmonstr-warning-8-240.png}"}
A mean or a median may be appropriate for describing the *data*.

However,
the CI is about the *mean*.

Since the sampling distribution for the sample mean
(under certain conditions)
has a *normal distribution*, the mean is appropriate for describing the sampling distribution.
:::


In addition to the statistical validity condition,
the CI will be 

* [**internally valid**](#def:InternalValidity)
  if the study was well designed; and
* [**externally validity**](#def:ExternalValidity) 
  if the
  the sample is a [simple random sample](#SRS)
  and is internally valid.


::: {.importantBox .important data-latex="{iconmonstr-warning-8-240.png}"}
When $n > 25$ approximately,
we do *not* require that the *data* has a normal distribution.

We require that the *sample means* have a normal distribution,
which is approximately true if the statistical validity condition is true.
:::


This is one reason why means are used to describe samples:
under certain conditions,
sample means have an approximate normal distribution
(so the 68--95--99.7 rule applies).
In contrast,
the distribution of sample medians is far more complicated to describe.

To determine if assuming the *population* has an approximate normal distribution
in the statistical validity condition,
the histogram of the *sample* can be constructed.
However,
we can't really be sure about the distribution of the *population* 
from the distribution of the *sample*.
All we can reasonably do
is to identify (from the sample)
populations that likely to be very non-normal
(when the CI would be not valid).



<div style="float:right; width: 222x; border: 1px; padding:10px">
<img src="Illustrations/pexels-anna-shvets-4225923.jpg" width="200px"/>
</div>



::: {.example #AssumptionsCT name="Assumptions"}
A study
[@data:silverman:CT; @data:zou:fluoroscopy]
to examine exposure to radiation
for CT scans in the abdomen assessed
$n = 17$ patients.
A histogram of the
total radiation dose received is shown in
Fig.&nbsp;\@ref(fig:CTscanHistogram);
the sample mean dose is 26.86&nbsp;rads.

A CI  for the mean radiation dose received could be formed.
However,
as the sample size is 'small' (less than 25),
the *population* must have a normal distribution
for the CI to be statistically valid.
Even though the histogram is from *sample* data,
it seems improbable that the data in the sample
would have come from a *population* with a normal distribution:
the histogram of the sample data doesn't look normally distributed at all.

Computing a CI for the mean of these data will probably be statistically invalid.
Other methods (beyond the scope of this course) are possible for computing
a confidence interval for the mean.
:::



```{r CTscanHistogram, echo=FALSE, fig.cap="The radiation doses from CT scans for 17 people", fig.align="center", fig.width=5, fig.height=3.5}

data(fluoro)

hist(fluoro$Dose,
	col = plot.colour,
	las = 1,
	xlab = "Radiation dose (in rads)",
	ylab = "Number of people",
	main = "Radiation dose for 17 people\nundergoing a CT scan")
box()
```




::: {.example #BagsCIConditions name="School bags"}
In Example \@ref(exm:BagsCI),
an approximate 95% CI was formed for the mean weight of school bags for Iranian children.

Since the sample size was $n = 586$, the CI is statistically valid.

We *do not* have to assume that the distribution of school bag weights
has a normal distribution in the population,
as the sample size is (much) larger than 25.
:::







::: {.example #OneMeanCIBearsConditions name="Black bears"}
In Example \@ref(exm:OneMeanCIBears),
the approximate 95% CI was formed from a sample of size $n = 185$ male bears.

This CI is statistically valid, since the sample size is much larger than 25.

We *do not* have to assume that the distribution of the weights of male black bears 
has a normal distribution in the population,
as the sample size is (much) larger than 25.
:::
         





<iframe src="https://learningapps.org/watch?v=ppetqnq4322" style="border:0px;width:100%;height:500px" allowfullscreen="true" webkitallowfullscreen="true" mozallowfullscreen="true"></iframe>





## Example: NHANES {#OneMeanCINHANES}


```{r echo=FALSE, cache=FALSE}
n.S <- length(NHANES$DirectChol) - sum( is.na(NHANES$DirectChol))

mean.S <- mean(NHANES$DirectChol, 
               na.rm = TRUE)
sd.S <- sd(NHANES$DirectChol, 
           na.rm = TRUE)

se.S <- sd.S / sqrt( n.S )

ci.lo.S <- mean.S - 2 * se.S
ci.hi.S <- mean.S + 2 * se.S


t99 <- abs( qt(0.005, df = n.S - 1) )
ci.lo.S99 <- mean.S - t99 * se.S
ci.hi.S99 <- mean.S + t99 * se.S
```



<div style="float:right; width: 222x; border: 1px; padding:10px">
<img src="Illustrations/hamburger-2253349_640.jpg" width="200px"/>
</div>


`r if (knitr::is_html_output()) '<!--'`
\begin{wrapfigure}{R}{.25\textwidth}
  \begin{center}
    \includegraphics[width=.20\textwidth]{Illustrations/hamburger-2253349_640.jpg}
  \end{center}
\end{wrapfigure}
`r if (knitr::is_html_output()) '-->'`



Previously,
we asked this question
about the
[NHANES data](#NHANESGraphs):

> Among Americans,
> is the mean direct HDL cholesterol 
> different for current smokers and non-smokers?

The response variable is direct HDL cholesterol concentration.
The parameter is $\mu$, the population mean HDL cholesterol concentration.

What is the *population* mean direct HDL cholesterol concentration?

From the data (using jamovi or SPSS),
the sample mean is 
$\bar{x} = `r round( mean.S, 4)`$ mmol/L;
the standard deviation is 
$s=`r round( sd.S, 5)`$ mmol/L;
and the sample size is $n= `r n.S`$.

The value of $\bar{x}$
will vary from sample to sample;
sampling variation exists.
The standard error is:
    
\[
	\text{s.e.}(\bar{x}) = 
  	\frac{s}{\sqrt{n}}
  	=
 	\frac{ `r round(sd.S, 5)`}
	{\sqrt{`r n.S`}} = 
   	`r round(se.S, 5)`\text{ mmol/L}.
\]
The approximate 95% CI uses a multiplier of $2$,
so the margin-of-error is  
\[
       2\times `r round(se.S, 4)`
       = 
       `r round(2 * se.S, 5)`.
\]
The approximate 95% CI is $`r round( mean.S, 3)`$, 
give-or-take $`r round(2 * se.S, 5)`$;
or
from $`r round( ci.lo.S, 3)`$ to $`r round( ci.hi.S, 3)`$&nbsp;mmol/L.

> Based on the sample of size $n= `r n.S`$, a 95% CI
> for the population mean direct HDL cholesterol levels of Americans 
> is between 
> $`r round( ci.lo.S, 3)`$ and $`r round( ci.hi.S, 3)`$ mmol/L.

If many samples of the same size were found in the same way,
and computed the CI from each,
about 95% of the CIs would contain $\mu$
(but *this* particular CI may or may not contain the value of $\mu$).
We could also say that the CI gives a range of plausible values for $\mu$,
or that we are about 95% confident that this CI straddles the value of $\mu$.

The statistical validity condition should also be checked
to ensure the CI is statistically valid.

Since the sample size is much larger than 25,
this CI for mean direct HDL cholesterol is statistically valid,
*even though* the histogram of direct HDL cholesterol for individuals is skewed right
(Fig.&nbsp;\@ref(fig:NHANEShistogram)).
Recall: 
the distribution of the *sample means* should be normally distributed,
*not* the distribution of the data.


```{r NHANEShistogram, echo=FALSE, cache=FALSE, fig.cap="Histogram of direct HDL cholesterol concentration", fig.align="center", fig.width=4.5, fig.height=3.5}
hist( NHANES$DirectChol, 
      xlab = "Direct HDL cholesterol concentration (mmol/L)",
      ylab = "Number of people",
      col = plot.colour,
      las = 1,
      xlim = c(0, 4.5),
      main = "Histogram of direct\nHDL cholesterol concentrations")
```






## Example: Cadmium in peanuts {#Cadmium-In-Peanuts}



<div style="float:right; width: 222x; border: 1px; padding:10px">
<img src="Illustrations/tom-hermans-ZPfd3ZobOc0-unsplash.jpg" width="200px"/>
</div>


`r if (knitr::is_html_output()) '<!--'`
\begin{wrapfigure}{R}{.25\textwidth}
  \begin{center}
    \includegraphics[width=.20\textwidth]{Illustrations/tom-hermans-ZPfd3ZobOc0-unsplash.jpg}
  \end{center}
\end{wrapfigure}
`r if (knitr::is_html_output()) '-->'`



A study of peanuts from the United States
[@data:Blair2017:Peanuts]
found the sample mean cadmium concentration was 0.0768&nbsp;ppm
with a standard deviation of 0.0460&nbsp;ppm,
from a sample of size 290 peanuts gathered from
a variety of regions at various times
(attempting to find a representative sample).

The parameter is $\mu$, the population mean cadmium concentration in peanuts.

Every sample of $n = 290$ peanuts is likely to produce
a different sample mean;
that is,
the sample means show *sampling variation*.
The sampling variation can be measured using the standard error:

\[
	\text{s.e.}(\bar{x}) = \frac{s}{\sqrt{n}} = \frac{0.0460}{\sqrt{290}} = 0.002701\text{ ppm}.
\]
The approximate 95% CI\ is
\[
	0.0768 \pm (2 \times 0.002701),
\]
or $0.0768 \pm 0.00540$,
which is from 0.0714 to 0.0822&nbsp;ppm.
(The *margin of error* is 0.00540.)

If we repeatedly took samples of size 290 from this population,
about 95% of the 95% CIs would contain the population mean
(but *this* CI may or may not contain the value of $\mu$).

The plausible values of $\mu$ that could have produced $\bar{x} = 0.0768$
are between 0.0714 and 0.0822ppm.
Alternatively,
we are about 95% confident that the CI of 0.0714 to 0.0822&nbsp;ppm
straddles the population mean.

Since the sample size is larger than $25$,
the CI is statistically valid.




```{r echo=FALSE}
data(lungcap)

LC.F11 <- subset(lungcap, 
                 Age == 11 & Gender == "F")
```








## Quick review questions {#Chap22-QuickReview}

1. True or false: The value of $\bar{x}$ varies from sample to sample.  
`r if( knitr::is_html_output() ) {torf(answer = TRUE )}`
1. True or false: A CI for $\mu$ is statistically valid only if the histogram of the *data* has an approximate normal distribution.  
`r if( knitr::is_html_output() ) {torf(answer = FALSE )}`
1. Suppose $s = 8$ and $n=20$. Which *one* of the following is **true**?  
`r if( knitr::is_html_output() ) {longmcq( c(
  answer = "The standard error is 1.7889",
  "The standard error is 8",
  "Since the sample size is less than 25, the standard error is invalid") )}`

::: {.progressBox .progress}
`r if (knitr::is_html_output()) {"**Progress:**"}`
`r webexercises::total_correct()`
:::




## Exercises {#OneMeanConfIntervalExercises}


Selected answers are available in
Sect.&nbsp;\@ref(OneMeanConfIntervalAnswer).

::: {.exercise #CIOneMeanLungCapacityInChildren}

A study
[@data:Tager:FEV; @BIB:data:FEV]
of the lung capacity of children in East Boston
measured the forced expiratory volume (FEV) of children in the area.

The sample contained 
$n = 45$ eleven-year-old girls.
For these children,
the mean lung capacity was 
$\bar{x} = 2.85$ litres
and the standard deviation was 
$s = 0.43$ litres.

Find an approximate 95% CI for the
population mean lung capacity 
of eleven-year-old females from East Boston.
:::


::: {.exercise #CIOneMeanEnvironmentalPollution}

A study of lead smelter emissions near children's public playgrounds
[@data:Taylor2013:Lead]
found the mean lead concentration at one playground 
(Memorial Park, Port Pirie, in South Australia) to be 
6956.41 micrograms per square metre, with a 
standard deviation of 7571.74 micrograms of lead per square metre,
from a sample of $n = 58$ wipes taken over a seven-day period.
(As a reference,
the Western Australian government recommends a maximum of 400 micrograms of lead per square metre.)

Find an approximate 95% CI for the mean lead concentration
at this playground.
Would these results apply to other playgrounds?
:::

::: {.exercise #CIOneMeanToothbrushing}

A study
[@data:Macgregor1985:ToothbrushinghYoungAdults]
of the brushing time for 60 young adults
(aged 18--22 years old) 
found the mean brushing time was 33.0 seconds,
with a standard deviation of 12.0 seconds.

Find an approximate 95% CI for the mean brushing time for young adults.
:::

::: {.exercise #CIOneMeanBloodLoss}
A study of paramedics
[@data:Williams2007:BloodLoss]
asked participants ($n = 199$) to estimate the amount of blood loss on four different surfaces.
When the actual amount of blood spill on concrete was 1000&nbsp;ml,
the mean guess was 846.4&nbsp;ml (with a standard deviation of 651.1&nbsp;ml).

1. What is the approximate 95% CI for the mean guess of blood loss?
1. Are the participants good at estimating the amount of blood loss on concrete?
1. Is this CI likely to be valid?
1. How many paramedics would be needed if the mean guess was to be estimated with an precision of give-or-take 50&nbsp;ml?
1. How many paramedics would be needed if the mean guess was to be estimated with an precision of give-or-take 25&nbsp;ml?
1. How many times greater does the sample size need to be to *halve* the width of the margin of error?

:::


::: {.exercise #OneMeanCINHANESInterpret}
In Sect.&nbsp;\@ref(OneMeanCINHANES),
the approximate 95% CI for the mean direct HDL cholesterol was given as
$1.356$ to $1.374$&nbsp;mmol/L.
Which (if any) of these interpretations are acceptable?
Explain why are the other interpretations are *incorrect*.

1. In the *sample*, about 95% of individuals have a direct HDL concentration between $1.356$ to $1.374$&nbsp;mmol/L.
1. In the *population*, about 95% of individuals have a direct HDL concentration between $1.356$ to $1.374$&nbsp;mmol/L.
1. About 95% of the *samples* are between $1.356$ to $1.374$&nbsp;mmol/L.
1. About 95% of the *populations* are between $1.356$ to $1.374$&nbsp;mmol/L.
1. The *population* mean varies so that it is between $1.356$ to $1.374$&nbsp;mmol/L about 95% of the time.
1. We are about 95% sure that *sample* mean is between $1.356$ to $1.374$&nbsp;mmol/L.
1. It is plausible that the *sample* mean is between $1.356$ to $1.374$&nbsp;mmol/L.

:::




::: {.exercise #OneMeanStdError}
An article 
[@data:Grabosky2016:Trees]
describes the diameter of *Quercus bicolor* trees planted in a lawn as
having a mean of 25.8 cm, with a standard error of 0.64 cm,
from a sample of 19 trees.
Which (if any) of the following is correct?
  
1. About 95% of the
trees in the *sample* will have a diameter between 
$25.8 - (2\times 0.64)$ and $25.8 + (2\times 0.64)$
(based on using the 68--95--99.7 rule).

1. About 95% of these types of trees in the *population*
will have a diameter between 
$25.8 - (2\times 0.64)$ and $25.8 + (2\times 0.64)$
(based on using the 68--95--99.7 rule)?

:::



