
# CIs for one mean {#OneMeanConfInterval}



```{r, child = if (knitr::is_html_output()) {'./introductions/22-CIs-OneMean-HTML.Rmd'} else {'./introductions/22-CIs-OneMean-LaTeX.Rmd'}}
```


## Describing the sampling distribution: $\sigma$ known


<div style="float:right; width: 222x; border: 1px; padding:10px">
<img src="Illustrations/pexels-skitterphoto-705171.jpg" width="200px"/>
</div>


In this chapter, we study the situation where a population mean $\mu$ (the parameter) is estimated by a sample mean $\bar{x}$ (the statistic).
The sample mean is computed from just one of the many possible samples, and each possible sample is likely to produce a different value of $\bar{x}$.
That is, the value of the sample mean varies from sample to sample, called *sampling variation* (which is quantified using the *standard error*).


::: {.importantBox .important data-latex="{iconmonstr-warning-8-240.png}"}
Remember: Studying a sample leads to the following observations:
\vspace{-2ex}

* Every sample is likely to be different.
* The observed sample is one of countless possible samples from the population.
* Every sample is likely to yield a different value for the sample statistic.
* We observe just one of the many possible values for the statistic.
\vspace{-2ex}

Since many values for the sample mean are possible, the possible values of the sample mean vary (called *sampling variation*) and have a *distribution* (called a *sampling distribution*).
:::


Consider rolling dice again.
Suppose a die is rolled $n = 25$ times, and the *mean* of the $25$ numbers that are rolled is recorded.
Since every face of the die is equally likely to appear on any one roll, the population mean of all possible rolls is $\mu = 3.5$ (in the middle of the numbers on the faces of the die, which is also the *median*).

What will be the sample mean of the numbers in the $25$ rolls?
We cannot be sure, as the sample mean will vary from sample to sample (*sampling variation*).
But suppose we roll a die $25$ times, to see how the sample mean varies in $25$ rolls
`r if (knitr::is_latex_output()) {
   '(see Fig.\\ \\@ref(fig:RollDiceMeanFig) for ten sets of $25$ rolls).'
} else {
   'as shown in the animation below for ten sets of $25$ rolls.'
}`
The mean of the $25$ rolls clearly varies, as expected.
In the simulation, the sample mean of $25$ rolls was as low as $3.08$ and as high as $3.76$.


```{r}
die.mn <- sum( (1:6) * (1/6) )  # 3.5 as expected
die.vr <-  sum( ((1:6) - 3.5)^2 * (1/6) ) #  2.916667
die.se <- sqrt( die.vr / 25)
```




```{r RollDiceMeanHTML, animation.hook="gifski", interval=0.5, dev=if (is_latex_output()){"pdf"}else{"png"}}
if (knitr::is_html_output()){
  set.seed(99999)
    num.rolls <- 25
    num.sims <- 10
    x.loc <- 1:(num.rolls)
    y.loc <- 1
    mean.even <- array(dim = num.sims)
    all.rolls <- array( dim = c(num.sims, num.rolls))
    for (i in 1:num.sims){
      plot( c(1, (num.rolls + 2)), c(1, num.sims),
            type = "n",
            las = 1,
            xlab = "",
            ylab = "",
            main = paste("Set number", i),
            axes = FALSE)
      
      roll <- sample(1:6, 
                     num.rolls, 
		     replace = TRUE)
      all.rolls[i, ] <- roll
      mean.even[i] <- mean(roll)
      
      col1 <- col2rgb("darkolivegreen2")
      col2 <- col2rgb("indianred2")
 
      for (j in 1:i){
        text(y = j, 
	     x = 1:num.rolls,
             labels = all.rolls[j, ])
        
        # Add some slight background colour under the sample proportions
        polygon( c(num.rolls + 1, num.rolls + 1, num.rolls + 3, num.rolls + 3),
                 c(j - 0.5, j + 0.5, j + 0.5, j - 0.5),
                 border = NA,
                 col = ifelse( mean.even[j] > 3.5, rgb( col1[1], col1[2], col1[3], alpha = 75, max = 255), 
                                                   rgb( col2[1], col2[2], col2[3], alpha = 75, max = 255) ) )

      }
      # Add x-bar heading
      mtext(expression(bar( italic(x) ) ), 
            side = 3, 
	    line = 0, 
	    at = num.rolls + 2 )
      # Add the roll number to the left-hand side
      axis(side = 2, 
           at = 1:i,
           las = 1,
           labels = paste("Set #", 1:i, sep = "") )
      
      # Add the sample mean to right-hand side 
      text(num.rolls + 2, 
           1:i, 
           labels = format(round(mean.even[1:i], 2), nsmall = 2) )
      #Add dividing line
      abline(v = num.rolls + 1, 
             col = "grey")
      # Add line dividing roll sets
      abline(h = seq(0.5, 10.5, by = 1), 
             col = "gray")
    }
  }
```



```{r RollDiceMeanFig, fig.align="center", fig.width=7, fig.height=4, out.width="85%", fig.cap="Rolling dice: The average of 25 rolls, for ten sets of 25 rolls" }
if (knitr::is_latex_output()){
  set.seed(99999)
  num.rolls <- 25
  num.sims <- 10
  x.loc <- 1:(num.rolls)
  y.loc <- 1
  mean.even <- array(dim = num.sims)
  all.rolls <- array( dim = c(num.sims, num.rolls))
  
  par(mar = c(0.5, 4, 5, 0.5) )
  plot( c(1, (num.rolls + 2)), c(1, num.sims),
        type = "n",
        las = 1,
        xlab = "",
        ylab = "",
        main = "The sample mean from 25 rolls\nover 10 simulations",
        axes = FALSE)
  
  for (i in 1:num.sims){
    roll <- sample(1:6, 
                   num.rolls, 
                   replace = TRUE)
    all.rolls[i, ] <- roll
    mean.even[i] <- mean(roll)
  }
  for (j in 1:num.sims){
    text(y = j, 
         x = 1:num.rolls,
         labels = all.rolls[j, ])
  }
  # Add x-bar heading
  mtext(expression(bar( italic(x) ) ), 
        side = 3, 
        line = 0, 
        at = num.rolls + 2 )
  # Add the roll number to the left-hand side
  axis(side = 2, 
       at = 1:i,
       las = 1,
       labels = paste("Set #", 1:i, sep = "") )
  
  # Add the sample mean to right-hand side 
  text(x = num.rolls + 2, 
       y = 1:i, 
       font = ifelse( mean.even > 3.5, 2, 1),
       labels = format(round(mean.even[1:i], 2), nsmall = 2) )
  
  #Add dividing line
  abline(v = num.rolls + 1, 
         col = "grey")
  
  # Add line dividing roll sets
  abline(h = seq(0.5, 10.5, by = 1), 
         col = "gray")
  #}
}
```


The mean for any single sample of $n = 25$ rolls will sometimes be higher than $\mu = 3.5$, and sometimes lower than $\mu = 3.5$, but most of the time the mean should be close to $3.5$.
If thousands of people made one set of $25$ rolls each, and computed the mean for their set, every person would have a sample mean for their set of $25$ rolls, and we could produce a histogram of all these sample means;
`r if (knitr::is_latex_output()) {
   'see Fig.\\ \\@ref(fig:RollDiceHistMeanFig).'
} else {
   'see the animation below.'
}`


```{r RollDiceHistMeanHTML, animation.hook="gifski", interval=0.4, dev=if (is_latex_output()){"pdf"}else{"png"}}
if (knitr::is_html_output()){
  set.seed(123456789)
  num.sims <- 1000
  num.rolls <- 25
  
  
  print_Histo <- rep(FALSE, num.sims)
  print_Histo[ c( 1:10,
                  seq(24, num.sims, 25) + 1,
                  num.sims) ] <- TRUE
  
  die.mn <- sum( (1:6) * (1/6))  # 3.5 as expected
  die.vr <-  sum( ((1:6) - 3.5)^2 * (1/6))
  
  meanList <- array( dim = num.sims)
  
  for (i in 1:num.sims){
    meanSingleRoll <- mean(sample(1:6, 
                                  num.rolls, 
                                  replace = TRUE))
    meanList[i] <- meanSingleRoll
    
    
    #Print every nth histogram only
    if (print_Histo[i]){
      out <- hist(meanList,
                  xlab = "Mean of 25 rolls",
                  ylab = "Number of times observed",
                  main = paste("Histogram of the mean of 25 rolls:\nSimulation number:", i),
                  sub = paste("(For this set: mean roll is ", 
                              format(round(meanList[i], 2), nsmall = 2), 
                              ")", 
                              sep = "" ),
                  col = plot.colour,
                  las = 1,
                  xlim = c(1.5, 5.5), 
                  right = FALSE,
                  ylim = c(0, 300),
                  breaks = seq(2, 5, by = 0.25)
      )
      
      points(meanList[i], 0,
             pch = 19,
             col = plot.colour0)
      
      xx <- seq(1, 6, length=500)
      yy <- dnorm(xx, 
                  mean = die.mn, 
                  sd = sqrt(die.vr/num.rolls) )
      yy <- yy/max(yy) * max(out$count)
      
      lines(yy ~ xx, 
            col = "grey", 
            lwd = 2)  
    }
  }
}
```


```{r RollDiceHistMeanFig, fig.align="center", fig.width=5, fig.height=2.5, fig.cap="Rolling dice: The mean of 25 rolls, for thousands of repetitions" }
if (knitr::is_latex_output()){
  set.seed(123456789)
  num.sims <- 2000
  num.rolls <- 25
  
  die.mn <- sum( (1:6) * (1/6))  # 3.5 as expected
  die.vr <-  sum( ((1:6) - 3.5)^2 * (1/6))
  
  meanList <- array( dim=num.sims)
  
  for (i in 1:num.sims){
    meanSingleRoll <- mean(sample(1:6, 
                                  num.rolls, 
                                  replace = TRUE))
    meanList[i] <- meanSingleRoll
  }
  
  out <- hist(meanList,
              xlab = "Mean of 25 rolls",
              #ylab = "Number of times observed",
              ylab = "",
              main = "Histogram of the mean of 25 rolls:\nafter thousands of simulations",
              #                             num.sims, "simulations"),
              col = plot.colour,
              las = 1,
              axes = FALSE,
              #xlim = c(1.5, 5.5),
              right = FALSE,
              #ylim = c(0, 300),
              breaks = seq(2, 5, by = 0.25) 
  )
  axis(side = 1,
       at = 1:6)
  xx <- seq(1, 6, 
            length = 500)
  yy <- dnorm(xx, 
              mean = die.mn, 
              sd = sqrt(die.vr/num.rolls) )
  yy <- yy/max(yy) * max(out$count)
  
  lines(yy ~ xx, 
        col = "grey", 
        lwd = 2)  
  
  points(x = 3.5,
         y = 0,
         pch = 19)
  mtext(expression(mu),
        side = 1, 
        line = 0.5)
}      
```


`r if (knitr::is_latex_output()) {
   'From Fig.\\ \\@ref(fig:RollDiceHistMeanFig),'
} else {
   'From the animation above,'
}`
the sample means vary with an approximate normal distribution (as we saw with the sample proportions).
This normal distribution is *not* describing the data; it is describing how the *values of sample means vary across all possible samples*.
Under certain conditions, the values of the sample means can vary with a normal distribution, and this normal distribution has a mean and a standard deviation.

This sampling distribution describes how *sample means* vary.
The mean of this distribution---the *sampling mean*---has the value $\mu$.
The standard deviation of this distribution is called the *standard error of the sample means*, denoted $\text{s.e.}(\bar{x})$.
When the *population* standard deviation $\sigma$ is *known*, the standard error happens to be  
\[
  \text{s.e.}(\bar{x}) = \frac{\sigma}{\sqrt{n}}.
\]
The possible values of the sample means have a *sampling distribution* described by:

* an approximate normal distribution,
* with a sampling mean whose value is $\mu$, and
* a standard deviation, called the standard error, of $\text{s.e.}(\bar{x}) = \sigma/\sqrt{n}$.

Since the sampling distribution has an approximate normal distribution, the 68--95--99.7 rule can be applied: approximately $95$% of the sample means are expected to be within two standard errors of $\mu$.
This information could be used to form a *sampling interval* (Sect.\ @ref(CIpKnownp)).
However, since the population standard deviation is rarely known, let's first consider the case where the value of $\sigma$ is unknown.


## Describing the sampling distribution: $\sigma$ unknown {#SamplingDistSampleMean}

When a sample mean is used to estimate a population mean, the sample mean varies from sample to sample: sampling variation exists, as we saw in the previous section.

However, the value of the *population* standard deviation $\sigma$ is almost never known, so the sample standard deviation $s$ to give an estimate of the standard error of the mean: $\text{s.e.}(\bar{x}) = s/\sqrt{n}$.
With this information, the *sampling distribution of the sample mean* can be described (see Table\ \@ref(tab:NotationOneMeanCI)).


::: {.definition #DEFSamplingDistributionXbar name="Sampling distribution of a sample mean"}
When the *population* standard deviation is unknown, the *sampling distribution of the sample mean* is described by:
  
* an approximate a normal distribution,
* centred around a sampling mean whose value is $\mu$,
* with a standard deviation (called the *standard error of the mean*) $\text{s.e.}(\bar{x})$, whose value is  
\begin{equation}
   \text{s.e.}(\bar{x}) = \frac{s}{\sqrt{n}},
   (\#eq:stderrorxbar)
\end{equation}
when certain conditions are met (Sect.\ \@ref(ValiditySampleMean)), where $n$ is the size of the sample, and $s$ is the sample standard deviation of the observations. 
:::


::: {.importantBox .important data-latex="{iconmonstr-warning-8-240.png}"}
A mean or a median may be appropriate for describing the *data*.
However, the *sampling distribution* for the sample mean (under certain conditions) has a *normal distribution*, and so the mean is appropriate for describing the sampling distribution.
:::


```{r NotationOneMeanCI}
OneMeanNotation <- array( dim = c(4, 2))

OneMeanNotation[1, ] <- c("Individual values in the population",
                          "Vary with mean $\\mu$ and standard deviation $\\sigma$")
OneMeanNotation[2, ] <- c("Individual values in a sample",
                          "Vary with mean $\\bar{x}$ and standard deviation $s$")
OneMeanNotation[3, ] <- c("Sample means ($\\bar{x}$) across",
                          "Vary with approx. normal distribution (under certain conditions):")
OneMeanNotation[4, ] <- c("all possible samples",
                          "sampling mean $\\mu$; standard deviation $\\text{s.e.}(\\bar{x})$")


if( knitr::is_latex_output() ) {
  kable( OneMeanNotation,
         format = "latex",
         booktabs = TRUE,
         longtable = FALSE,
         escape = FALSE,
         caption = "The notation used for describing means, and the sampling distribution of the sample means",
         align = c("r", "l"),
         linesep = c("\\addlinespace",
                     "\\addlinespace",
                     ""),
         col.names = c("Quantity",
                       "Description") ) %>%
	row_spec(0, bold = TRUE) %>%
  kable_styling(font_size = 10)
} else {
  OneMeanNotation[3, 1] <- paste(OneMeanNotation[3, 1], 
                                 OneMeanNotation[4, 1])
  OneMeanNotation[3, 2] <- paste(OneMeanNotation[3, 2], 
                                 OneMeanNotation[4, 2])
  OneMeanNotation[4, ] <- NA
  
    kable( OneMeanNotation,
         format = "html",
         booktabs = TRUE,
         longtable = FALSE,
         escape = FALSE,
         caption = "The notation used for describing means, and the sampling distribution of the sample means",
         align = c("r", "l"),
         linesep = c("\\addlinespace",
                     "\\addlinespace",
                     ""),
         col.names = c("Quantity",
                       "Description") ) %>%
	row_spec(0, bold = TRUE) 
}
```


## Computing confidence intervals {#OneMeanCI}

We don't know the value of $\mu$ (the parameter), but we have an *estimate*: the value of $\bar{x}$, the sample mean (the statistic).
The actual value of $\mu$ might be a bit larger than $\bar{x}$, or a bit smaller than $\bar{x}$; that is, the value of $\mu$ is probably about $\bar{x}$, give-or-take a bit.

Furthermore, the values of $\bar{x}$ vary from sample to sample (*sampling variation*), and vary with an approximate normal distribution.
So, using the [68--95--99.7 rule](#def:EmpiricalRule), an approximate $95$% interval could be constructed for the plausible values of $\mu$ that may have given the observed values of the sample mean.
This is a *confidence interval*.

A confidence interval (CI) for the population mean is an interval surrounding a sample mean.
An approximate $95$% confidence interval (CI) for $\mu$ is $\bar{x}$ give-or-take about two standard errors.
In general, an [confidence interval](#def:ConfidenceInterval) (CI) for $\mu$ is  
\[
   \bar{x} \pm \overbrace{(\text{Multiplier}\times\text{s.e.}(\bar{x}))}^{\text{The `margin of error'}}.
\]
For an approximate $95$% CI, the multiplier is about $2$ (since about $95$% of values are within two standard deviations of the mean, from the [68--95--99.7 rule](#def:EmpiricalRule)).

CIs are commonly $95$% CIs, but *any* level of confidence can be used, when a different multiplier is needed.
In this book, a multiplier of $2$ is used to create CIs manually (to find *approximate* $95$% CIs), and otherwise software is used.
Commonly, CIs are computed at $90$%, $95$% and $99$% confidence levels.


::: {.tipBox .tip data-latex="{iconmonstr-info-6-240.png}"}
The multiplier is not actually a $z$-score here.
The multiplier *would* be a $z$-score if the value of the *population* standard deviation was known.
We don't know the *population* standard deviation, and use the *sample* standard deviation instead.
In these situations, the multiplier is a $t$-score.

The values of $t$- and $z$-multipliers are *very* similar, and (except for small sample sizes) using an approximate multiplier of\ $2$ is reasonable for computing *approximate* $95$% CIs in either case.
:::


Pretend for the moment that the value of $\mu$ was unknown, and we tossed a die $25$ times, and found $\bar{x} = 3.2$ and $s = 2.5$.
Then,  
\[
   \text{s.e.}(\bar{x}) = \frac{s}{\sqrt{n}} = \frac{2.5}{\sqrt{25}} = 0.5.
\]
Then, the sample means will vary with an approximate normal distribution, centred around the unknown value of $\mu$, with a standard deviation of $\text{s.e.}(\bar{x}) = 0.5$ (Fig.\ \@ref(fig:DiceMeanNormal)).

Our estimate of $\bar{x} = 3.2$ may be a bit smaller than the value of $\mu$, or a bit larger than the value of $\mu$; that is, the value of $\mu$ is probably $\bar{x}$ give-or-take a bit.
A range of $\bar{x}$ values that are likely to straddle $\mu$ is given by a CI.
The *approximate* $95$% CI is from $3.2 - (2 \times 0.5)$ and $3.2 - (2 \times 0.5)$, or from $2.2$ to $4.2$ (see Def.\ \@ref(def:DEFSamplingDistributionXbar)).
Hence, values of $\mu$ between $2.2$ to $4.2$ could reasonably have produced a sample mean of $\bar{x} = 3.2$.


```{r CIrelationshipsMean, out.width='75%', fig.align="center", fig.cap="A CI gives a range of values of $\\mu$ for which it is reasonable to produce the observed value of $\\bar{x}$. The shaded regions under the normal distributions represent the regions containing 95\\% of the values of $\\bar{x}$ for each value of $\\bar{x}$.", fig.width=6.5, fig.height=8}
#source("R/showCIForVariousMeans.R")                
```


```{r DiceMeanNormal, fig.cap="The sampling distribution is a normal distribution; it shows how the sample mean of 25 die rolls varies in samples of size $n = 25$", fig.align="center", fig.width=9.5, fig.height=2.75, out.width='95%'}

mn <- 3.5
n <- 25
stdd <- 2.5

se <- stdd / sqrt(n)

par( mar = c(4, 0.5, 0.5, 0.5) )
out <- plotNormal(mn,
                  se,
                  xlab = "Sample mean die roll, from 25 rolls of a die", 
                  cex.axis = 0.95,
                  ylim = c(0, 1.3),
                  showXlabels = c( 	
                    expression( mu-0.5),
                    expression( mu-1), 
                    expression( mu-1.5), 
                    expression( mu ),
                    expression( mu+0.5), 
                    expression( mu+1.0), 
                    expression( mu+1.5) ) )

arrows(x0 = mn,
       x1 = mn,
       y0 = 1.4 * max(out$y),
       y1 = max(out$y),
       lwd = 2,
       length = 0.15,
       angle = 15)
text(x = mn,
     y = 1.4 * max(out$y),
     pos = 3,
     labels = expression(Sampling~mean) )

arrows(x0 = mn ,
       x1 = mn + se,
       y0 = 0.35 * max(out$y),
       y1 = 0.35 * max(out$y),
       lwd = 2,
       code = 3,
       length = 0.15,
       angle = 15)
text(x = mn + (se / 2),
     y = 0.3 * max(out$y),
     pos = 3,
     labels = expression(Std~error))
text(x = mn + (se / 2),
     y = 0.32 * max(out$y),
     pos = 1,
     labels = expression(plain(s.e.)(bar(italic(x)))))
```



<div style="float:right; width: 222x; border: 1px; padding:10px">
<img src="Illustrations/pexels-pixabay-207697.jpg" width="200px"/>
</div>


::: {.example #BagsCI name="School bags"}
A study of the school bags that $586$ children (in Grades 6--8 in Tabriz, Iran) take to school found the mean weight was $\bar{x} = 2.8$\ kg with a standard deviation of $s = 0.94$\ kg [@data:Dianat2014:schoolbags].
The parameter $\mu$ is the population mean weight of school bags for Iranian children in Grades 6--8.

Another sample of $586$ children would produce a different sample mean; sampling variation exists.
The *standard error* of the sample mean is  
\[
   \text{s.e.}(\bar{x}) = \frac{s}{\sqrt{n}} = \frac{0.94}{\sqrt{586}} = 0.03883.
\]
The approximate $95$% CI for the population mean school-bag weight is  
\[
  2.8\pm (2 \times 0.03883),  
\]
or $2.8\pm0.07766$.
(The *margin of error* is $0.07766$.)
This is an interval from $2.72$\ kg to $2.88$\ kg.
This CI has a $95$% chance of straddling the population mean bag weight.
:::


::: {.thinkBox .think data-latex="{iconmonstr-light-bulb-2-240.png}"}
Would a $99$% CI for $\mu$ be *wider* or *narrower* than the $95$% CI?
Why?\label{thinkBox:WiderNarrower}

`r if (knitr::is_latex_output()) '<!--'`
`r webexercises::hide()`
A *wider* interval is needed to be *more* confident that the interval contains the population mean.
`r webexercises::unhide()`
`r if (knitr::is_latex_output()) '-->'`
:::


<!-- ```{r BagsNormal, fig.cap="The sampling distribution is a normal distribution; it shows how the sample mean bag weight varies in samples of size $n = 586$", fig.align="center", fig.width=9.5, fig.height=2.75, out.width='95%'} -->

<!-- mn <- 2.8 -->
<!-- n <- 586 -->
<!-- stdd <- 0.94 -->

<!-- se <- stdd/sqrt(n) -->

<!-- par( mar = c(4, 0.5, 0.5, 0.5) ) -->
<!-- out <- plotNormal(mn, -->
<!--                   se, -->
<!--                   xlab = "Sample mean bag weight, in kg (sample size: 586)",  -->
<!--                   cex.axis = 0.95, -->
<!--                   showXlabels = c( 	 -->
<!--                     expression( mu-0.114), -->
<!--                     expression( mu-0.076),  -->
<!--                     expression( mu-0.038),  -->
<!--                     expression( mu ), -->
<!--                     expression( mu+0.038),  -->
<!--                     expression( mu+0.076),  -->
<!--                     expression( mu+0.114) ) ) -->
<!-- ``` -->



## Statistical validity conditions {#ValiditySampleMean}

As with any inference procedure, the underlying mathematics requires [certain conditions to be met](#exm:StatisticalValidityAnalogy) so that the results are statistically valid (i.e., the sampling distribution is sufficiently like a normal distribution).
The CI for one mean will be *statistical valid* if *one* of these is true:

1. The sample size is at least $25$, *or*
2. The sample size is smaller than $25$ *and* the *population* data has an approximate normal distribution.

The sample size of $25$ is a rough figure here, and some books give other (similar) values (such as $30$).

This condition ensures that the *sampling distribution of the sample means has an approximate normal distribution* (so that the [68--95--99.7 rule](#def:EmpiricalRule) can be used).
Provided the sample size is larger than about $25$, this will be approximately true *even if* the distribution of the individuals in the
population do not have a normal distribution.
That is, when $n > 25$ the sample means generally have an approximate normal distribution, even if the data themselves do not follow a normal distribution.


::: {.importantBox .important data-latex="{iconmonstr-warning-8-240.png}"}
When $n > 25$ approximately, we do *not* require that the *data* has a normal distribution.
The *sample means* need to have a normal distribution, which is approximately true if the statistical validity condition is true.
:::


To determine if assuming the *population* has an approximate normal distribution in the statistical validity condition, the histogram of the *sample* can be constructed.
However, we can't really be sure about the distribution of the *population* from the distribution of the *sample*.
All we can reasonably do is to identify (from the sample) populations that likely to be very non-normal (when the CI would be not valid).


<div style="float:right; width: 222x; border: 1px; padding:10px">
<img src="Illustrations/pexels-anna-shvets-4225923.jpg" width="200px"/>
</div>


::: {.example #AssumptionsCT name="Assumptions"}
A study [@data:silverman:CT; @data:zou:fluoroscopy] to examine exposure to radiation for CT scans in the abdomen assessed $n = 17$ patients.
As the sample size is 'small' (less than $25$), the *population data* must have a normal distribution for a CI for $\mu$ to be statistically valid.

A histogram of the total radiation dose received using the *sample* data (Fig.\ \@ref(fig:CTscanHistogram)) suggests this is very unlikely.
Even though the histogram is from *sample* data, it seems improbable that the data in the sample would have come from a *population* with a normal distribution.

Computing a CI for the mean of these data will probably be statistically invalid.
Other methods (beyond the scope of this course) are possible for computing a CI for the mean.
:::


```{r CTscanHistogram, fig.cap="The radiation doses from CT scans for 17 people", fig.align="center", fig.width=5, fig.height=3.5}
data(Fluoro)

hist(Fluoro$Dose,
	col = plot.colour,
	las = 1,
	xlab = "Radiation dose (in rads)",
	ylab = "Number of people",
	main = "Radiation dose for 17 people\nundergoing a CT scan")
box()
```


::: {.example #BagsCIConditions name="School bags"}
In Example\ \@ref(exm:BagsCI), an approximate $95$% CI was formed for the mean weight of school bags for Iranian children.
Since the sample size was $n = 586$, the CI is statistically valid.
We *do not* have to assume that the distribution of school bag weights has a normal distribution in the population, as the sample size is (much) larger than $25$.
:::


<iframe src="https://learningapps.org/watch?v=ppetqnq4322" style="border:0px;width:100%;height:500px" allowfullscreen="true" webkitallowfullscreen="true" mozallowfullscreen="true"></iframe>


## Example: cadmium in peanuts {#Cadmium-In-Peanuts}


<div style="float:right; width: 222x; border: 1px; padding:10px">
<img src="Illustrations/tom-hermans-ZPfd3ZobOc0-unsplash.jpg" width="200px"/>
</div>


A study of peanuts from the United States [@data:Blair2017:Peanuts] found the sample mean cadmium concentration was $0.076$\ ppm with a standard deviation of $0.0460$\ ppm, from a sample of $290$ peanuts gathered from a variety of regions at various times (attempting to find a representative sample).
The parameter is $\mu$, the population mean cadmium concentration in peanuts.

Every sample of $n = 290$ peanuts is likely to produce a different sample mean, so *sampling variation* exists and can be measured using the standard error:  
\[
	\text{s.e.}(\bar{x}) = \frac{s}{\sqrt{n}} = \frac{0.0460}{\sqrt{290}} = 0.002701\text{ ppm}.
\]
The approximate $95$% CI\ is $0.0768 \pm (2 \times 0.002701)$, or $0.0768 \pm 0.00540$, which is from $0.0714$ to $0.0822$\ ppm.
(The *margin of error* is $0.00540$.)

If we repeatedly took samples of size $290$ from this population, about $95$% of the $95$% CIs would contain the population mean (but *this* CI may or may not contain the value of $\mu$).
The plausible values of $\mu$ that could have produced $\bar{x} = 0.0768$ are between $0.0714$ and $0.0822$\ ppm.
Alternatively, we are about $95$% confident that the CI of $0.0714$ to $0.0822$\ ppm straddles the population mean.

Since the sample size is larger than $25$, the CI is statistically valid.


```{r}
data(LungCap)

LC.F11 <- subset(LungCap, 
                 Age == 11 & Gender == "F")
```



## Quick review questions {#Chap22-QuickReview}

::: {.webex-check .webex-box}
1. True or false: The value of $\bar{x}$ varies from sample to sample.\tightlist  
`r if( knitr::is_html_output() ) {torf(answer = TRUE )}`

1. True or false: A CI for $\mu$ is statistically valid only if the histogram of the *data* has an approximate normal distribution.  
`r if( knitr::is_html_output() ) {torf(answer = FALSE )}`

1. A sample of data produces $s = 8$ and $n = 20$. 
   Which *one* of the following is **true**?  
`r if( knitr::is_html_output() ) {longmcq( c(
  answer = "The standard error of the mean is 1.7889.",
  "The standard error of the mean is 8.",
  "Since the sample size is less than 25, the standard error is not statistically valid.") )}`
`r if( !knitr::is_html_output() ) {
'   * The standard error of the mean is 1.7889.
   * The standard error of the mean is 8.
   * Since the sample size is less than 25, the standard error is not statistically valid.'
}`
:::



## Exercises {#OneMeanConfIntervalExercises}

Selected answers are available in Sect.\ \@ref(OneMeanConfIntervalAnswer).


::: {.exercise #OneMeanCIBears}
A study of American black bears [@bartareau2017estimating] found the mean weight of the $n = 185$ male bears was  $\bar{x} = 84.9$\ kg, with a standard deviation of $s = 51.1$\ kg.

1. Write down the *parameter* of interest.
1. Compute the standard error of the mean.
1. Compute the approximate $95$% CI.
1. Write a conclusion.
1. Is the CI likely to be statistically valid?
:::
         


::: {.exercise #CIOneMeanLungCapacityInChildren}
A study of the lung capacity of children in East Boston [@data:Tager:FEV; @BIB:data:FEV] measured the forced expiratory volume (FEV) of children in the area.
The sample contained $n = 45$ eleven-year-old girls.
For these children, the mean lung capacity was $\bar{x} = 2.85$ litres and the standard deviation was $s = 0.43$ litres.
Find an approximate $95$% CI for the population mean lung capacity of eleven-year-old females from East Boston.
:::


::: {.exercise #CIOneMeanEnvironmentalPollution}
A study of lead smelter emissions near children's public playgrounds [@data:Taylor2013:Lead] found the mean lead concentration at one playground (Memorial Park, Port Pirie, in South Australia) to be $6956.41$ micrograms per square metre, with a standard deviation of $7571.74$ micrograms of lead per square metre, from a sample of $n = 58$ wipes taken over a seven-day period.
(As a reference, the Western Australian Government recommends a maximum of $400$ micrograms of lead per square metre.)

Find an approximate $95$% CI for the mean lead concentration at this playground.
Would these results apply to other playgrounds?
:::


::: {.exercise #CIOneMeanToothbrushing}
A study [@data:Macgregor1985:ToothbrushinghYoungAdults] of the brushing time for $60$ young adults (aged 18--22 years old) found the mean brushing time was $33.0$ seconds, with a standard deviation of $12.0$ seconds.
Find an approximate $95$% CI for the mean brushing time for young adults.
:::


::: {.exercise #CIOneMeanBloodLoss}
A study of paramedics [@data:Williams2007:BloodLoss] asked participants ($n = 199$) to estimate the amount of blood loss on four different surfaces.
When the actual amount of blood spill on concrete was $1000$\ ml, the mean guess was $846.4$\ ml (with a standard deviation of $651.1$\ ml).

1. What is the approximate $95$% CI for the mean guess of blood loss?
1. Are the participants good at estimating the amount of blood loss on concrete?
1. Is this CI likely to be valid?
:::


::: {.exercise #OneMeanCINHANESInterpret}
Using the NHANES data, the approximate $95$% CI for the mean direct HDL cholesterol is $1.356$ to $1.374$\ mmol/L.
Which (if any) of these interpretations are acceptable?
Explain *why* are the other interpretations are incorrect.

1. In the *sample*, about $95$% of individuals have a direct HDL concentration between $1.356$ to $1.374$\ mmol/L.
1. In the *population*, about $95$% of individuals have a direct HDL concentration between $1.356$ to $1.374$\ mmol/L.
1. About $95$% of the *samples* are between $1.356$ to $1.374$\ mmol/L.
1. About $95$% of the *populations* are between $1.356$ to $1.374$\ mmol/L.
1. The *population* mean varies so that it is between $1.356$ to $1.374$\ mmol/L about $95$% of the time.
1. We are about $95$% sure that *sample* mean is between $1.356$ to $1.374$\ mmol/L.
1. It is plausible that the *sample* mean is between $1.356$ to $1.374$\ mmol/L.
:::


::: {.exercise #OneMeanStdError}
An article [@data:Grabosky2016:Trees] describes the diameter of *Quercus bicolor* trees planted in a lawn as having a mean of $25.8$\ cm, with a standard error of $0.64$\ cm, from a sample of $19$ trees.
Which (if any) of the following is correct?
  
1. About $95$% of the trees in the *sample* will have a diameter between $25.8 - (2\times 0.64)$ and $25.8 + (2\times 0.64)$ (based on using the 68--95--99.7 rule).
1. About $95$% of these types of trees in the *population* will have a diameter between $25.8 - (2\times 0.64)$ and $25.8 + (2\times 0.64)$ (based on using the 68--95--99.7 rule)?
:::


::: {.exercise #ChewingTime}
In a study of $n = 30$ five-year-old children [@watanabe1995estimation], the mean time for the children to eat a cookie was $61.3$\ s, with a standard deviation of $29.4$\ s.

1. What is an approximate $95$% CI for the population mean time for a five-year-old child to eat a cookie? 
2. Is the CI likely to be statistically valid?
:::




<!-- QUICK REVIEW ANSWERS -->
`r if (knitr::is_html_output()) '<!--'`
::: {.EOCanswerBox .EOCanswer data-latex="{iconmonstr-check-mark-14-240.png}"}
**Answers to in-chapter questions:**

- Sect. \ref{thinkBox:WiderNarrower}: A *wider* interval is needed to be *more* confident that the interval contains the population mean.

- \textbf{\textit{Quick Revision} questions:}
**1.** True
**2.** False.
**3.** Standard error is $1.7889$.
:::
`r if (knitr::is_html_output()) '-->'`

