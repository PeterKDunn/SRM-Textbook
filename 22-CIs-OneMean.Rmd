
# CIs for one mean {#OneMeanConfInterval}



```{r, child = if (knitr::is_html_output()) {'./introductions/22-CIs-OneMean-HTML.Rmd'} else {'./introductions/22-CIs-OneMean-LaTeX.Rmd'}}
```


## Describing the sampling distribution: $\sigma$ known


<div style="float:right; width: 222x; border: 1px; padding:10px">
<img src="Illustrations/pexels-skitterphoto-705171.jpg" width="200px"/>
</div>


In this chapter, we study the situation where a population mean $\mu$ (the parameter) is estimated by a sample mean $\bar{x}$ (the statistic).
The sample mean comes from just one of the many possible samples, and each possible sample is likely to produce a different value $\bar{x}$.
That is, the value of the sample mean varies from sample to sample, called *sampling variation* (which can be quantified using the *standard error*).


::: {.importantBox .important data-latex="{iconmonstr-warning-8-240.png}"}
Studying a sample leads to the following observations:

* Each sample is likely to be different.
* Our sample is just one of countless possible samples from the population.
* Each sample is likely to produce a different value for the sample mean.
* Hence we only observe one of the many possible values for the sample mean.

Since many values for the sample mean are possible, the possible values of the sample mean vary (called *sampling variation*) and have a *distribution* (called a *sampling distribution*).
:::

Consider rolling dice again.
Suppose a die is rolled $n = 25$ times, and the *mean* of the 25 numbers that are rolled is recorded.
Since every face of the die is equally likely to appear on any one roll, the population mean of all possible rolls is $\mu = 3.5$ (in the middle of the numbers on the faces of the die, which is also the *median*).

What will be the sample mean of the numbers in the 25 rolls?
We cannot be sure, as the sample mean will vary from sample to sample (*sampling variation*).


```{r echo=FALSE}
die.mn <- sum( (1:6) * (1/6) )  # 3.5 as expected
die.vr <-  sum( ((1:6) - 3.5)^2 * (1/6) ) #  2.916667
die.se <- sqrt( die.vr / 25)
```


Suppose we try rolling a die 25 times, to see how much the sample mean varies in 25 rolls
`r if (knitr::is_latex_output()) {
   '(see Fig. \\@ref(fig:RollDiceMeanFig) for 10 sets of 25 rolls).  (The online version has an animation.)'
} else {
   'as shown in the animation below for 10 sets of 25 rolls.'
}`
The mean of the 25 rolls clearly varies, as expected.
In the simulation, the sample mean of 25 rolls was as low as 3.08 and as high as 3.76.


```{r RollDiceMeanHTML, animation.hook="gifski", echo=FALSE, interval=0.5, dev=if (is_latex_output()){"pdf"}else{"png"}}
if (knitr::is_html_output()){
  set.seed(99999)
    num.rolls <- 25
    num.sims <- 10
    x.loc <- 1:(num.rolls)
    y.loc <- 1
    mean.even <- array(dim = num.sims)
    all.rolls <- array( dim = c(num.sims, num.rolls))
    for (i in 1:num.sims){
      plot( c(1, (num.rolls + 2)), c(1, num.sims),
            type = "n",
            las = 1,
            xlab = "",
            ylab = "",
            main = paste("Set number", i),
            axes = FALSE)
      
      roll <- sample(1:6, 
                     num.rolls, 
		     replace = TRUE)
      all.rolls[i, ] <- roll
      mean.even[i] <- mean(roll)
      
      col1 <- col2rgb("darkolivegreen2")
      col2 <- col2rgb("indianred2")
 
      for (j in 1:i){
        text(y = j, 
	     x = 1:num.rolls,
             labels = all.rolls[j, ])
        
        # Add some slight background colour under the sample proportions
        polygon( c(num.rolls + 1, num.rolls + 1, num.rolls + 3, num.rolls + 3),
                 c(j - 0.5, j + 0.5, j + 0.5, j - 0.5),
                 border = NA,
                 col = ifelse( mean.even[j] > 3.5, rgb( col1[1], col1[2], col1[3], alpha = 75, max = 255), 
                                                   rgb( col2[1], col2[2], col2[3], alpha = 75, max = 255) ) )

      }
      # Add x-bar heading
      mtext(expression(bar( italic(x) ) ), 
            side = 3, 
	    line = 0, 
	    at = num.rolls + 2 )
      # Add the roll number to the left-hand side
      axis(side = 2, 
           at = 1:i,
           las = 1,
           labels = paste("Set #", 1:i, sep = "") )
      
      # Add the sample mean to right-hand side 
      text(num.rolls + 2, 
           1:i, 
           labels = format(round(mean.even[1:i], 2), nsmall = 2) )
      #Add dividing line
      abline(v = num.rolls + 1, 
             col = "grey")
      # Add line dividing roll sets
      abline(h = seq(0.5, 10.5, by = 1), 
             col = "gray")
    }
  }
```



```{r RollDiceMeanFig, echo=FALSE, fig.align="center", cache=FALSE, fig.width=7, fig.height=4, out.width="85%", fig.cap="Rolling dice: The average of 25 rolls, for some sets of 25 rolls" }
if (knitr::is_latex_output()){
  set.seed(99999)
    num.rolls <- 25
    num.sims <- 10
    x.loc <- 1:(num.rolls)
    y.loc <- 1
    mean.even <- array(dim = num.sims)
    all.rolls <- array( dim = c(num.sims, num.rolls))

      plot( c(1, (num.rolls + 2)), c(1, num.sims),
            type = "n",
            las = 1,
            xlab = "",
            ylab = "",
            main = "The sample mean from 25 rolls\nover 10 simulations",
            axes = FALSE)

    for (i in 1:num.sims){
      roll <- sample(1:6, 
                     num.rolls, 
		     replace = TRUE)
      all.rolls[i, ] <- roll
      mean.even[i] <- mean(roll)
    }
     for (j in 1:num.sims){
        text(y = j, 
	     x = 1:num.rolls,
             labels = all.rolls[j, ])
      }
      # Add x-bar heading
      mtext(expression(bar( italic(x) ) ), 
            side = 3, 
	    line = 0, 
	    at = num.rolls + 2 )
      # Add the roll number to the left-hand side
      axis(side = 2, 
           at = 1:i,
           las = 1,
           labels = paste("Set #", 1:i, sep = "") )
      
      # Add the sample mean to right-hand side 
      text(num.rolls + 2, 1:i, 
           labels = format(round(mean.even[1:i], 1), nsmall = 2) )

      #Add dividing line
      abline(v = num.rolls + 1, 
             col = "grey")
	     
      # Add line dividing roll sets
      abline(h = seq(0.5, 10.5, by = 1), 
             col = "gray")
    #}
}
```


The mean for any single *sample* of $n = 25$ rolls will sometimes be higher than $\mu = 3.5$, and sometimes lower than $\mu = 3.5$, but most of the time the mean should be close to 3.5.
If thousands of people made one set of 25 rolls each, and computed the mean for their set, every person would have a sample mean for their set of 25 rolls, and we could produce a histogram of all these sample means;
`r if (knitr::is_latex_output()) {
   'see Fig. \\@ref(fig:RollDiceHistMeanFig).  (The online version has an animation.)'
} else {
   'see the animation below.'
}`


```{r RollDiceHistMeanHTML, cache=FALSE, echo=FALSE, animation.hook="gifski", interval=0.4, dev=if (is_latex_output()){"pdf"}else{"png"}}
if (knitr::is_html_output()){
  set.seed(123456789)
    num.sims <- 1000
    num.rolls <- 25
    
    
    print_Histo <- rep(FALSE, num.sims)
    print_Histo[ c( 1:10,
                    seq(24, num.sims, 25) + 1,
                    num.sims) ] <- TRUE
    
    die.mn <- sum( (1:6) * (1/6))  # 3.5 as expected
    die.vr <-  sum( ((1:6) - 3.5)^2 * (1/6))
    
    meanList <- array( dim = num.sims)
    
    for (i in 1:num.sims){
      meanSingleRoll <- mean(sample(1:6, 
                                    num.rolls, 
			            replace = TRUE))
      meanList[i] <- meanSingleRoll
      
      
      #Print every nth histogram only
      if (print_Histo[i]){
        out <- hist(meanList,
                    xlab = "Mean of 25 rolls",
                    ylab = "Number of times observed",
                    main = paste("Histogram of the mean of 25 rolls:\nSimulation number:", i),
                    sub = paste("(For this set: mean roll is ", 
                                format(round(meanList[i], 2), nsmall = 2), 
                                ")", 
                                sep = "" ),
                    col = plot.colour,
                    las = 1,
                    xlim = c(1.5, 5.5), 
                    right = FALSE,
                    ylim = c(0, 300),
                    breaks = seq(2, 5, by = 0.25)
        )
        
        points(meanList[i], 0,
               pch = 19,
               col = plot.colour0)
        
        xx <- seq(1, 6, length=500)
        yy <- dnorm(xx, 
                    mean = die.mn, 
                    sd = sqrt(die.vr/num.rolls) )
        yy <- yy/max(yy) * max(out$count)
        
        lines(yy ~ xx, 
              col = "grey", 
              lwd = 2)  
      }
    }
  }
```


```{r RollDiceHistMeanFig, echo=FALSE, fig.align="center", fig.width=5, fig.height=3.5, fig.cap="Rolling dice: The mean of 25 rolls, for thousands of repetitions" }
if (knitr::is_latex_output()){
  set.seed(123456789)
    num.sims <- 2000
    num.rolls <- 25
    
    die.mn <- sum( (1:6) * (1/6))  # 3.5 as expected
    die.vr <-  sum( ((1:6) - 3.5)^2 * (1/6))
    
    meanList <- array( dim=num.sims)
    
    for (i in 1:num.sims){
      meanSingleRoll <- mean(sample(1:6, 
                                    num.rolls, 
				    replace = TRUE))
      meanList[i] <- meanSingleRoll
    }

    out <- hist(meanList,
                xlab = "Mean of 25 rolls",
                #ylab = "Number of times observed",
                ylab = "",
                main = "Histogram of the mean of 25 rolls:\nafter thousands of simulations",
#                             num.sims, "simulations"),
                col = plot.colour,
                las = 1,
                axes = FALSE,
                #xlim = c(1.5, 5.5),
                right = FALSE,
                #ylim = c(0, 300),
                breaks = seq(2, 5, by = 0.25) 
      )
    axis(side = 1,
         at = 1:6)
      xx <- seq(1, 6, 
                length = 500)
      yy <- dnorm(xx, 
                  mean = die.mn, 
		  sd = sqrt(die.vr/num.rolls) )
      yy <- yy/max(yy) * max(out$count)

      lines(yy ~ xx, 
            col = "grey", 
	    lwd = 2)  
}      
```


`r if (knitr::is_latex_output()) {
   'From Fig. \\@ref(fig:RollDiceHistMeanFig),'
} else {
   'From the animation above,'
}`
the sample means vary with an approximate normal distribution (as we saw with the sample proportions).
This normal distribution is *not* describing the data; it is describing how the *bvalues of sample means vary across all possible samples*.
Under certain conditions, the values of the sample means can vary with a normal distribution, and this normal distribution has a mean and a standard deviation.

Since this distribution is describing how *sample means* vary, the mean is denoted $\mu_{\bar{x}}$; it so happens that this is equal to the value of $\mu$.
The standard deviation is called the *standard error of the sample means*, denoted $\text{s.e.}(\bar{x})$.
When the *population* standard deviation $\sigma$ is *known*, the standard error happens to be

\[
  \text{s.e.}(\bar{x}) = \frac{\sigma}{\sqrt{n}}.
\]
So the possible values of the sample means have a *sampling distribution* described by:

* an approximate normal distribution,
* with mean $\mu_{\bar{x}} = \mu$, and
* a standard deviation, called the standard error, of $\text{s.e.}(\bar{x}) = \sigma/\sqrt{n}$.

However, almost always the population mean, and the population standard deviation, are *unknown* (if they were known, we wouldn't need to take a sample to estimate them).
Since the sampling distribution has an approximate normal distribution, the 68--95--99.7 rule can be applied: approximately 95% of the sample means are expected to be within two standard errors of $\mu$.


## Describing the sampling distribution: $\sigma$ unknown {#SamplingDistSampleMean}

When a sample mean is used to estimate a population mean, the sample mean varies from sample to sample: sampling variation exists, as we saw in the previous section.

When the *population* standard deviation $\sigma$ is unknown (which is almost always the case), it is estimated using the *sample* standard deviation $s$.
Then, the best we can do is use the estimate of the *standard error* of the sample mean: $\displaystyle\text{s.e.}(\bar{x}) = \frac{s}{\sqrt{n}}$.
With this information, we can describe the *sampling distribution of the sample mean* (see Table \@ref(tab:NotationOneMeanCI)).


::: {.definition #DEFSamplingDistributionXbar name="Sampling distribution of a sample mean"}
When the *population* standard deviation is unknown, the *sampling distribution of the sample mean* is described by:
  
* an approximate a normal distribution,
* centred around $\mu_{\bar{x}} = \mu$,
* with a standard deviation (called the *standard error of the mean*) of 

\begin{equation}
   \text{s.e.}(\bar{x}) = \frac{s}{\sqrt{n}},
   (\#eq:stderrorxbar)
\end{equation}
when certain conditions are met (Sect. \@ref(ValiditySampleMean)), where $n$ is the size of the sample, and $s$ is the standard deviation describing the variation in the individual observations in the sample (that is, the *sample* standard deviation). 
:::




```{r NotationOneMeanCI, echo=FALSE}

OneMeanNotation <- array( dim = c(4, 2))

OneMeanNotation[1, ] <- c("Individual values in the population",
                          "Vary with mean $\\mu$ and standard deviation $\\sigma$")
OneMeanNotation[2, ] <- c("Individual values in a sample",
                          "Vary with mean $\\bar{x}$ and standard deviation $s$")
OneMeanNotation[3, ] <- c("Sample means ($\\bar{x}$) across",
                          "Vary with approx. normal distribution (under certain conditions)")
OneMeanNotation[4, ] <- c("all possible samples",
                          "with mean $\\mu_{\\bar{x}}$ and standard deviation $\\text{s.e.}(\\bar{x})$")


if( knitr::is_latex_output() ) {
  kable( OneMeanNotation,
         format = "latex",
         booktabs = TRUE,
         longtable = FALSE,
         escape = FALSE,
         caption = "The notation used for describing means, and the sampling distribution of the sample means",
         align = c("r", "l"),
         linesep = c("\\addlinespace",
                     "\\addlinespace",
                     ""),
         col.names = c("Quantity",
                       "Description") ) %>%
	row_spec(0, bold = TRUE) %>%
  kable_styling(font_size = 10)
} else {
  OneMeanNotation[3, 1] <- paste(OneMeanNotation[3, 1], 
                                 OneMeanNotation[4, 1])
  OneMeanNotation[3, 2] <- paste(OneMeanNotation[3, 2], 
                                 OneMeanNotation[4, 2])
  OneMeanNotation[3, 3] <- paste(OneMeanNotation[3, 3], 
                                 OneMeanNotation[4, 3])
  OneMeanNotation[3, 4] <- paste(OneMeanNotation[3, 4], 
                                 OneMeanNotation[4, 4])
  OneMeanNotation[4, ] <- NA
  
    kable( OneMeanNotation,
         format = "html",
         booktabs = TRUE,
         longtable = FALSE,
         escape = FALSE,
         caption = "The notation used for describing means, and the sampling distribution of the sample means",
         align = c("r", "l"),
         linesep = c("\\addlinespace",
                     "\\addlinespace",
                     ""),
         col.names = c("Quantity",
                       "Description") ) %>%
	row_spec(0, bold = TRUE) 
}
```


## Computing confidence intervals {#OneMeanCI}

We don't know the value of $\mu$ (the parameter), the population mean, but we have an *estimate*: the value of $\bar{x}$, the sample mean (the statistic).
The actual value of $\mu$ might be a bit larger than $\bar{x}$, or a bit smaller than $\bar{x}$; that is, $\mu$ is probably about $\bar{x}$, give-or-take a bit.

Furthermore, the values of $\bar{x}$ vary from sample to sample (*sampling variation*), and they vary with an approximate normal distribution.
So, using the [68--95--99.7 rule](#def:EmpiricalRule), an approximate 95% interval could be constructed for the plausible values of $\mu$ that may have given the observed values of the sample mean.
This is a *confidence interval*.

A confidence interval (CI) for the population mean is an interval surrounding a sample mean.
An approximate 95% confidence interval (CI) for $\mu$ is $\bar{x}$ give-or-take about two standard errors.
In general, an [confidence interval](#def:ConfidenceInterval) (CI) for $\mu$ is

\[
   \bar{x} \pm \overbrace{(\text{Multiplier}\times\text{s.e.}(\bar{x}))}^{\text{The `margin of error'}}.
\]
For an approximate 95% CI, the multiplier is, as usual, about $2$ (since about 95% of values are within two standard deviations of the mean from the [68--95--99.7 rule](#def:EmpiricalRule)).

The most common CIs are 95% CIs, but *any* level of confidence can be used, when a different multiplier is needed.
In this book, a multiplier of $2$ is used to create CIs manually (to find *approximate* 95% CIs), and otherwise software is used.
Commonly, CIs are computed at 90%, 95% and 99% confidence levels.


::: {.tipBox .tip data-latex="{iconmonstr-info-6-240.png}"}
The multiplier of 2 is not a $z$-score here.
It *would* be a $z$-score if the value of the *population* standard deviation was known.
Since we don't know the *population* standard deviation, and use the *sample* standard deviation instead, the multiplier is actually a $t$-score.\smallskip

However, $t$- and $z$-multipliers are *very* similar, and (except for small sample sizes) using an approximate multiplier of 2 is reasonable for computing *approximate* 95% CIs in either case.
:::


If we collected many samples of a specific size, $\bar{x}$ and $s$ would be different for each sample, so the calculated CI would be different for each.
Some CIs would straddle the population mean $\mu$, and some would not.
We never know if the CI computed from our single sample straddles $\mu$ or not.

Loosely speaking, there is a 95% chance that our 95% CI straddles $\mu$.
For a CI computed from a single sample, we don't know if our CI includes the value of $\mu$ or not.
The CI could also be interpreted as the range of plausible values of $\mu$ that could have produced the observed value of $\bar{x}$.


```{r CIrelationshipsMean, echo=FALSE, out.width='85%', fig.align="center", fig.cap="A CI gives a range of possible values of $\\mu$ for which it is reasonable to produce the observed value of $\\bar{x}$. The shaded regions represent the regions containing 95\\% of the values of $\\bar{x}$ for each value of $p$.", fig.width=9, fig.height=6.5}
source("R/showCIForVariousMeans.R")                
```



<div style="float:right; width: 222x; border: 1px; padding:10px">
<img src="Illustrations/pexels-pixabay-207697.jpg" width="200px"/>
</div>



::: {.example #BagsCI name="School bags"}
A study of the school bags that 586 children (in Grades 6--8 in Tabriz, Iran) take to school found the mean weight was $\bar{x} = 2.8$&nbsp;kg with a standard deviation of $s = 0.94$&nbsp;kg [@data:Dianat2014:schoolbags].
The parameter $\mu$ is the population mean weight of school bags for Iranian children in Grades 6--8.

Another sample of 586 children would produce a different sample mean, so the sample mean varies from sample to sample; sampling variation exists.
The *standard error* of the sample mean is

\[
   \text{s.e.}(\bar{x}) = \frac{s}{\sqrt{n}} = \frac{0.94}{\sqrt{586}} = 0.03883;
\]
see Fig. \@ref(fig:BagsNormal).
The approximate 95% CI for the population mean school-bag weight is

\[
  2.8\pm(2 \times 0.03883),  
\]
or $2.8\pm0.07766$.
(The *margin of error* is 0.07766.)
This is an interval from 2.72&nbsp;kg to 2.88&nbsp;kg.
This CI has a 95% chance of straddling the population mean bag weight.
:::


::: {.thinkBox .think data-latex="{iconmonstr-light-bulb-2-240.png}"}
Would a 99% CI for $\mu$ be *wider* or *narrower* than the 95% CI?
Why?\label{thinkBox:WiderNarrower}

`r if (knitr::is_latex_output()) '<!--'`
`r webexercises::hide()`
A *wider* interval is needed to be *more* confident that the interval contains the population mean.
`r webexercises::unhide()`
`r if (knitr::is_latex_output()) '-->'`
:::


```{r BagsNormal, echo=FALSE, fig.cap="The samoking distribution is a normal distribution; it shows how the sample mean bag weight varies in samples of size $n = 586$", fig.align="center", fig.width=8.5, fig.height=2.5, out.width='90%'}
mn <- 2.8
n <- 586
stdd <- 0.94

se <- stdd/sqrt(n)

out <- plotNormal(mn,
                  se,
                  xlab = "Sample mean bag weight, in kg (sample size: 586)", 
                  cex.axis = 0.95,
                  showXlabels = c( 	
                    expression( mu-0.114),
                    expression( mu-0.076), 
                    expression( mu-0.038), 
                    expression( mu ),
                    expression( mu+0.038), 
                    expression( mu+0.076), 
                    expression( mu+0.114) ) )
```



## Statistical validity conditions {#ValiditySampleMean}

As with any inference procedure, the underlying mathematics requires [certain conditions to be met](#exm:StatisticalValidityAnalogy) so that the results are statistically valid.
The CI for one mean will be *statistical valid* if *one* of these is true:

1. The sample size is at least 25, *or*
2. The sample size is smaller than 25 *and* the *population* data has an approximate normal distribution.

The sample size of 25 is a rough figure here, and some books give other (similar) values (such as 30).
This condition ensures that the *sampling distribution of the sample means has an approximate normal distribution* (so that, for example, the [68--95--99.7 rule](#def:EmpiricalRule) can be used).

Provided the sample size is larger than about 25, this will be approximately true *even if* the distribution of the individuals in the
population does not have a normal distribution.
That is, when $n > 25$ the sample means generally have an approximate normal distribution, even if the data themselves don't have a normal distribution.


::: {.importantBox .important data-latex="{iconmonstr-warning-8-240.png}"}
A mean or a median may be appropriate for describing the *data*.
However, the CI is about the *mean* since the *sampling distribution* for the sample mean (under certain conditions) has a *normal distribution* and so the mean is appropriate for describing the sampling distribution.
:::


::: {.importantBox .important data-latex="{iconmonstr-warning-8-240.png}"}
When $n > 25$ approximately, we do *not* require that the *data* has a normal distribution.
The *sample means* need to have a normal distribution, which is approximately true if the statistical validity condition is true.
:::


This is one reason why means are used to describe samples: under certain conditions, sample means have an approximate normal distribution (so the 68--95--99.7 rule applies).
In contrast, the distribution of sample medians is far more complicated to describe.

To determine if assuming the *population* has an approximate normal distribution in the statistical validity condition, the histogram of the *sample* can be constructed.
However, we can't really be sure about the distribution of the *population* from the distribution of the *sample*.
All we can reasonably do is to identify (from the sample) populations that likely to be very non-normal (when the CI would be not valid).


<div style="float:right; width: 222x; border: 1px; padding:10px">
<img src="Illustrations/pexels-anna-shvets-4225923.jpg" width="200px"/>
</div>


::: {.example #AssumptionsCT name="Assumptions"}
A study [@data:silverman:CT; @data:zou:fluoroscopy] to examine exposure to radiation for CT scans in the abdomen assessed $n = 17$ patients.
A CI for the mean radiation dose received could be formed.
However, as the sample size is 'small' (less than 25), the *population data* must have a normal distribution for the CI to be statistically valid.

A histogram of the total radiation dose received using the *sample* data (Fig.&nbsp;\@ref(fig:CTscanHistogram)) suggests this is very unlikely.
Even though the histogram is from *sample* data, it seems improbable that the data in the sample would have come from a *population* with a normal distribution.

Computing a CI for the mean of these data will probably be statistically invalid.
Other methods (beyond the scope of this course) are possible for computing a CI for the mean.
:::


```{r CTscanHistogram, echo=FALSE, fig.cap="The radiation doses from CT scans for 17 people", fig.align="center", fig.width=5, fig.height=3.5}
data(fluoro)

hist(fluoro$Dose,
	col = plot.colour,
	las = 1,
	xlab = "Radiation dose (in rads)",
	ylab = "Number of people",
	main = "Radiation dose for 17 people\nundergoing a CT scan")
box()
```


::: {.example #BagsCIConditions name="School bags"}
In Example \@ref(exm:BagsCI), an approximate 95% CI was formed for the mean weight of school bags for Iranian children.
Since the sample size was $n = 586$, the CI is statistically valid.
We *do not* have to assume that the distribution of school bag weights has a normal distribution in the population, as the sample size is (much) larger than 25.
:::




<iframe src="https://learningapps.org/watch?v=ppetqnq4322" style="border:0px;width:100%;height:500px" allowfullscreen="true" webkitallowfullscreen="true" mozallowfullscreen="true"></iframe>



## Example: NHANES {#OneMeanCINHANES}


```{r echo=FALSE, cache=FALSE}
n.S <- length(NHANES$DirectChol) - sum( is.na(NHANES$DirectChol))

mean.S <- mean(NHANES$DirectChol, 
               na.rm = TRUE)
sd.S <- sd(NHANES$DirectChol, 
           na.rm = TRUE)

se.S <- sd.S / sqrt( n.S )

ci.lo.S <- mean.S - 2 * se.S
ci.hi.S <- mean.S + 2 * se.S


t99 <- abs( qt(0.005, df = n.S - 1) )
ci.lo.S99 <- mean.S - t99 * se.S
ci.hi.S99 <- mean.S + t99 * se.S
```


<div style="float:right; width: 222x; border: 1px; padding:10px">
<img src="Illustrations/hamburger-2253349_640.jpg" width="200px"/>
</div>


Previously, this RQ was asked about the [NHANES data](#NHANESGraphs):

> Among Americans, is the mean direct HDL cholesterol different for current smokers and non-smokers?

The response variable is direct HDL cholesterol concentration.
The parameter is $\mu$, the population mean HDL cholesterol concentration.
What is the *population* mean direct HDL cholesterol concentration?

From the data (using jamovi or SPSS), the *sample* mean is $\bar{x} = `r round( mean.S, 4)`$ mmol/L; the standard deviation is $s =`r round( sd.S, 5)`$ mmol/L; and the sample size is $n = `r n.S`$.
The value of $\bar{x}$ will vary from sample to sample; sampling variation exists.
The standard error is:
    
\[
	\text{s.e.}(\bar{x}) = 
  	\frac{s}{\sqrt{n}}
  	=
 	\frac{ `r round(sd.S, 5)`}
	{\sqrt{`r n.S`}} = 
   	`r round(se.S, 5)`\text{ mmol/L}.
\]
The approximate 95% CI uses a multiplier of $2$, so the margin-of-error is 
$2\times `r round(se.S, 4)` = `r round(2 * se.S, 5)`$.
The approximate 95% CI is $`r round( mean.S, 3)`$, give-or-take $`r round(2 * se.S, 5)`$; or from $`r round( ci.lo.S, 3)`$ to $`r round( ci.hi.S, 3)`$&nbsp;mmol/L:

> Based on the sample of size $n = `r n.S`$, a 95% CI for the population mean direct HDL cholesterol levels of Americans is between $`r round( ci.lo.S, 3)`$ and $`r round( ci.hi.S, 3)`$ mmol/L.

If many samples of the same size were found in the same way, and computed the CI from each, about 95% of the CIs would contain $\mu$ (but *this* particular CI may or may not contain the value of $\mu$).
Alternatively, the CI gives a range of plausible values for $\mu$, or that we are about 95% confident that this CI straddles the value of $\mu$.

Since the sample size is much larger than 25, this CI for mean direct HDL cholesterol is statistically valid, *even though* the histogram of direct HDL cholesterol for individuals is skewed right (Fig.&nbsp;\@ref(fig:NHANEShistogram)).
The distribution of the *sample means* should be normally distributed, *not* the distribution of the data.


```{r NHANEShistogram, echo=FALSE, cache=FALSE, fig.cap="Histogram of direct HDL cholesterol concentration", fig.align="center", fig.width=4.5, fig.height=3.0}
hist( NHANES$DirectChol, 
      xlab = "Direct HDL cholesterol concentration (mmol/L)",
      ylab = "Number of people",
      col = plot.colour,
      las = 1,
      xlim = c(0, 4.5),
      main = "Histogram of direct\nHDL cholesterol concentrations")
```


## Example: cadmium in peanuts {#Cadmium-In-Peanuts}


<div style="float:right; width: 222x; border: 1px; padding:10px">
<img src="Illustrations/tom-hermans-ZPfd3ZobOc0-unsplash.jpg" width="200px"/>
</div>


A study of peanuts from the United States [@data:Blair2017:Peanuts] found the sample mean cadmium concentration was 0.0768&nbsp;ppm with a standard deviation of 0.0460&nbsp;ppm, from a sample of 290 peanuts gathered from a variety of regions at various times (attempting to find a representative sample).
The parameter is $\mu$, the population mean cadmium concentration in peanuts.

Every sample of $n = 290$ peanuts is likely to produce a different sample mean, so *sampling variation* exists and can be measured using the standard error:

\[
	\text{s.e.}(\bar{x}) = \frac{s}{\sqrt{n}} = \frac{0.0460}{\sqrt{290}} = 0.002701\text{ ppm}.
\]
The approximate 95% CI\ is $0.0768 \pm (2 \times 0.002701)$, or $0.0768 \pm 0.00540$, which is from 0.0714 to 0.0822&nbsp;ppm.
(The *margin of error* is 0.00540.)

If we repeatedly took samples of size 290 from this population, about 95% of the 95% CIs would contain the population mean (but *this* CI may or may not contain the value of $\mu$).
The plausible values of $\mu$ that could have produced $\bar{x} = 0.0768$ are between 0.0714 and 0.0822ppm.
Alternatively, we are about 95% confident that the CI of 0.0714 to 0.0822&nbsp;ppm straddles the population mean.

Since the sample size is larger than $25$, the CI is statistically valid.


```{r echo=FALSE}
data(lungcap)

LC.F11 <- subset(lungcap, 
                 Age == 11 & Gender == "F")
```



## Quick review questions {#Chap22-QuickReview}

::: {.webex-check .webex-box}
1. True or false: The value of $\bar{x}$ varies from sample to sample.\tightlist  
`r if( knitr::is_html_output(exclude = "epub") ) {torf(answer = TRUE )}`

1. True or false: A CI for $\mu$ is statistically valid only if the histogram of the *data* has an approximate normal distribution.  
`r if( knitr::is_html_output(exclude = "epub") ) {torf(answer = FALSE )}`

1. Suppose $s = 8$ and $n=20$. Which *one* of the following is **true**?  
`r if( knitr::is_html_output(exclude = "epub") ) {longmcq( c(
  answer = "The standard error is 1.7889.",
  "The standard error is 8.",
  "Since the sample size is less than 25, the standard error is invalid.") )}`
`r if( !knitr::is_html_output(exclude = "epub") ) {
'   * The standard error is 1.7889.
   * The standard error is 8.
   * Since the sample size is less than 25, the standard error is invalid.'
}`
:::



## Exercises {#OneMeanConfIntervalExercises}

Selected answers are available in Sect.&nbsp;\@ref(OneMeanConfIntervalAnswer).


::: {.exercise #OneMeanCIBears}
A study of American black bears [@bartareau2017estimating] found the mean weight of the $n = 185$ male bears was  $\bar{x} = 84.9$&nbsp;kg, with a standard deviation of $s = 51.1$&nbsp;kg.

1. Write down the *parameter* of interest.
1. Compute the standard error of the mean.
1. Compute the approximate 95% CI.
1. Write a conclusion.
1. Is the CI likely to be statistically valid?
:::
         


::: {.exercise #CIOneMeanLungCapacityInChildren}
A study of the lung capacity of children in East Boston [@data:Tager:FEV; @BIB:data:FEV] measured the forced expiratory volume (FEV) of children in the area.
The sample contained $n = 45$ eleven-year-old girls.
For these children, the mean lung capacity was $\bar{x} = 2.85$ litres and the standard deviation was $s = 0.43$ litres.

Find an approximate 95% CI for the population mean lung capacity of eleven-year-old females from East Boston.
:::


::: {.exercise #CIOneMeanEnvironmentalPollution}
A study of lead smelter emissions near children's public playgrounds [@data:Taylor2013:Lead] found the mean lead concentration at one playground (Memorial Park, Port Pirie, in South Australia) to be 6956.41 micrograms per square metre, with a standard deviation of 7571.74 micrograms of lead per square metre, from a sample of $n = 58$ wipes taken over a seven-day period.
(As a reference, the Western Australian government recommends a maximum of 400 micrograms of lead per square metre.)

Find an approximate 95% CI for the mean lead concentration at this playground.
Would these results apply to other playgrounds?
:::


::: {.exercise #CIOneMeanToothbrushing}
A study [@data:Macgregor1985:ToothbrushinghYoungAdults] of the brushing time for 60 young adults (aged 18--22 years old) found the mean brushing time was 33.0 seconds, with a standard deviation of 12.0 seconds.
Find an approximate 95% CI for the mean brushing time for young adults.
:::


::: {.exercise #CIOneMeanBloodLoss}
A study of paramedics [@data:Williams2007:BloodLoss] asked participants ($n = 199$) to estimate the amount of blood loss on four different surfaces.
When the actual amount of blood spill on concrete was 1000&nbsp;ml, the mean guess was 846.4&nbsp;ml (with a standard deviation of 651.1&nbsp;ml).

1. What is the approximate 95% CI for the mean guess of blood loss?
1. Are the participants good at estimating the amount of blood loss on concrete?
1. Is this CI likely to be valid?
:::


::: {.exercise #OneMeanCINHANESInterpret}
In Sect.&nbsp;\@ref(OneMeanCINHANES), the approximate 95% CI for the mean direct HDL cholesterol was given as $1.356$ to $1.374$&nbsp;mmol/L.
Which (if any) of these interpretations are acceptable?
Explain *why* are the other interpretations are incorrect.

1. In the *sample*, about 95% of individuals have a direct HDL concentration between $1.356$ to $1.374$&nbsp;mmol/L.
1. In the *population*, about 95% of individuals have a direct HDL concentration between $1.356$ to $1.374$&nbsp;mmol/L.
1. About 95% of the *samples* are between $1.356$ to $1.374$&nbsp;mmol/L.
1. About 95% of the *populations* are between $1.356$ to $1.374$&nbsp;mmol/L.
1. The *population* mean varies so that it is between $1.356$ to $1.374$&nbsp;mmol/L about 95% of the time.
1. We are about 95% sure that *sample* mean is between $1.356$ to $1.374$&nbsp;mmol/L.
1. It is plausible that the *sample* mean is between $1.356$ to $1.374$&nbsp;mmol/L.
:::


::: {.exercise #OneMeanStdError}
An article [@data:Grabosky2016:Trees] describes the diameter of *Quercus bicolor* trees planted in a lawn as having a mean of 25.8 cm, with a standard error of 0.64 cm, from a sample of 19 trees.
Which (if any) of the following is correct?
  
1. About 95% of the trees in the *sample* will have a diameter between $25.8 - (2\times 0.64)$ and $25.8 + (2\times 0.64)$ (based on using the 68--95--99.7 rule).
1. About 95% of these types of trees in the *population* will have a diameter between $25.8 - (2\times 0.64)$ and $25.8 + (2\times 0.64)$ (based on using the 68--95--99.7 rule)?
:::


::: {.exercise #ChewingTime}
In a study of $n = 30$ five-year-old children [@watanabe1995estimation], the mean time for the children to eat a cookie was 61.3&nbsp;s, with a standard deviation of 29.4&nbsp;s.

1. What is an approximate 95% CI for the population mean time for a five-year-old child to eat a cookie? 
2. Is the CI likely to be statistically valid?
:::




<!-- QUICK REVIEW ANSWERS -->
`r if (knitr::is_html_output()) '<!--'`
::: {.EOCanswerBox .EOCanswer data-latex="{iconmonstr-check-mark-14-240.png}"}
**Answers to in-chapter questions:**

- Sect. \ref{thinkBox:WiderNarrower}: A *wider* interval is needed to be *more* confident that the interval contains the population mean.

- \textbf{\textit{Quick Revision} questions:}
**1.** True
**2.** False.
**3.** The standard error is 1.7889.
:::
`r if (knitr::is_html_output()) '-->'`

