
# Confidence intervals: one mean {#OneMeanConfInterval}



```{r, child = if (knitr::is_html_output()) {'./introductions/23-CIs-OneMean-HTML.Rmd'} else {'./introductions/23-CIs-OneMean-LaTeX.Rmd'}}
```


<!-- Define colours as appropriate -->
```{r, child = if (knitr::is_html_output()) {'./children/coloursHTML.Rmd'} else {'./children/coloursLaTeX.Rmd'}}
```



## Introduction {#CIOneMeanIntro}

<div style="float:right; width: 222x; border: 1px; padding:10px">
<img src="Illustrations/pexels-skitterphoto-705171.jpg" width="200px"/>
</div>


Consider rolling a fair, six-sided die $n = 25$ times.
Suppose we are interested in the *mean* of the numbers that are rolled.
Since every face of the die is equally likely to appear on any one roll, the population mean of all possible rolls is $\mu = 3.5$ (in the middle of the numbers on the faces of the die, so this is also the *median*).

What will be the sample mean of the numbers in the $25$\ rolls?
We don't know, as the sample mean varies from sample to sample (*sampling variation*).


::: {.importantBox .important data-latex="{iconmonstr-warning-8-240.png}"}
Remember: studying a sample leads to the following observations:
\vspace{-2ex}

* Every sample is likely to be different.
* We observe just one of the many possible samples.
* Every sample is likely to yield a different value for the statistic.
* We observe just one of the many possible values for the statistic.
\vspace{-2ex}

Since many values for the sample mean are possible, the values of the sample mean vary (called *sampling variation*) and have a *distribution* (called a *sampling distribution*).
:::



```{r}
die.mn <- sum( (1:6) * (1/6) )  # 3.5 as expected
die.vr <-  sum( ((1:6) - 3.5)^2 * (1/6) ) #  2.916667
die.se <- sqrt( die.vr / 25)
```




<!-- ```{r RollDiceMeanHTML, animation.hook="gifski", interval=0.5, dev=if (is_latex_output()){"pdf"}else{"png"}} -->
<!-- if (knitr::is_html_output()){ -->
<!--   set.seed(99999) -->
<!--     num.rolls <- 25 -->
<!--     num.sims <- 10 -->
<!--     x.loc <- 1:(num.rolls) -->
<!--     y.loc <- 1 -->
<!--     mean.even <- array(dim = num.sims) -->
<!--     all.rolls <- array( dim = c(num.sims, num.rolls)) -->

<!--     for (i in 1:num.sims){ -->

<!--       par( mar = c(5.1, 5.1, 4.1, 2.1)) -->

<!--       plot( c(1, (num.rolls + 2)), c(1, num.sims), -->
<!--             type = "n", -->
<!--             las = 1, -->
<!--             xlab = "", -->
<!--             ylab = "", -->
<!--             main = paste("Sample number", i), -->
<!--             axes = FALSE) -->

<!--       roll <- sample(1:6,  -->
<!--                      num.rolls,  -->
<!-- 		     replace = TRUE) -->
<!--       all.rolls[i, ] <- roll -->
<!--       mean.even[i] <- mean(roll) -->

<!--       col1 <- col2rgb("darkolivegreen2") -->
<!--       col2 <- col2rgb("indianred2") -->

<!--       for (j in 1:i){ -->
<!--         text(y = j,  -->
<!-- 	     x = 1:num.rolls, -->
<!--              labels = all.rolls[j, ]) -->

<!--         # Add some slight background colour under the sample proportions -->
<!--         polygon( c(num.rolls + 1, num.rolls + 1, num.rolls + 3, num.rolls + 3), -->
<!--                  c(j - 0.5, j + 0.5, j + 0.5, j - 0.5), -->
<!--                  border = NA, -->
<!--                  col = ifelse( mean.even[j] > 3.5, rgb( col1[1], col1[2], col1[3], alpha = 75, max = 255),  -->
<!--                                                    rgb( col2[1], col2[2], col2[3], alpha = 75, max = 255) ) ) -->

<!--       } -->
<!--       # Add x-bar heading -->
<!--       mtext(expression(bar( italic(x) ) ),  -->
<!--             side = 3,  -->
<!-- 	    line = 0,  -->
<!-- 	    at = num.rolls + 2 ) -->
<!--       # Add the roll number to the left-hand side -->
<!--       axis(side = 2,  -->
<!--            at = 1:i, -->
<!--            las = 1, -->
<!--            labels = paste("Sample #", 1:i, sep = "") ) -->

<!--       # Add the sample mean to right-hand side  -->
<!--       text(num.rolls + 2,  -->
<!--            1:i,  -->
<!--            labels = format(round(mean.even[1:i], 2), nsmall = 2) ) -->
<!--       #Add dividing line -->
<!--       abline(v = num.rolls + 1,  -->
<!--              col = "grey") -->
<!--       # Add line dividing roll sets -->
<!--       abline(h = seq(0.5, 10.5, by = 1),  -->
<!--              col = "gray") -->
<!--     } -->
<!--   } -->
<!-- ``` -->



<!-- ```{r RollDiceMeanFig, fig.align="center", fig.width=7, fig.height=4, out.width="85%", fig.cap="Rolling dice: The average of $25$ rolls, for ten samples" } -->
<!-- if (knitr::is_latex_output()){ -->
<!--   set.seed(99999) -->
<!--   num.rolls <- 25 -->
<!--   num.sims <- 10 -->
<!--   x.loc <- 1:(num.rolls) -->
<!--   y.loc <- 1 -->
<!--   mean.even <- array(dim = num.sims) -->
<!--   all.rolls <- array( dim = c(num.sims, num.rolls)) -->

<!--   par(mar = c(0.5, 5.5, 5, 0.5) ) -->
<!--   plot( c(1, (num.rolls + 2)), c(1, num.sims), -->
<!--         type = "n", -->
<!--         las = 1, -->
<!--         xlab = "", -->
<!--         ylab = "", -->
<!--         main = "The sample mean from a sample of 25 rolls\nfor each of 10 simulations", -->
<!--         axes = FALSE) -->

<!--   for (i in 1:num.sims){ -->
<!--     roll <- sample(1:6,  -->
<!--                    num.rolls,  -->
<!--                    replace = TRUE) -->
<!--     all.rolls[i, ] <- roll -->
<!--     mean.even[i] <- mean(roll) -->
<!--   } -->
<!--   for (j in 1:num.sims){ -->
<!--     text(y = j,  -->
<!--          x = 1:num.rolls, -->
<!--          labels = all.rolls[j, ]) -->
<!--   } -->
<!--   # Add x-bar heading -->
<!--   mtext(expression(bar( italic(x) ) ),  -->
<!--         side = 3,  -->
<!--         line = 0,  -->
<!--         at = num.rolls + 2 ) -->
<!--   # Add the roll number to the left-hand side -->
<!--   axis(side = 2,  -->
<!--        at = 1:i, -->
<!--        las = 1, -->
<!--        labels = paste("Sample #", 1:i, sep = "") ) -->

<!--   # Add the sample mean to right-hand side  -->
<!--   text(x = num.rolls + 2,  -->
<!--        y = 1:i,  -->
<!--        font = ifelse( mean.even > 3.5, 2, 1), -->
<!--        labels = format(round(mean.even[1:i], 2), nsmall = 2) ) -->

<!--   #Add dividing line -->
<!--   abline(v = num.rolls + 1,  -->
<!--          col = "grey") -->

<!--   # Add line dividing roll sets -->
<!--   abline(h = seq(0.5, 10.5, by = 1),  -->
<!--          col = "gray") -->
<!--   #} -->
<!-- } -->
<!-- ``` -->


## Sampling distribution for $\bar{x}$: for $\sigma$ known {#SamplingDistSampleMeanSigmaKnown}
\index{Sampling distribution!one mean ($\sigma$ known)}

Suppose thousands of people made one sample of $25$\ rolls each, and computed the mean for their sample.\index{Mean!sample}
Then, every person would have a sample mean for their sample, and we could produce a histogram of all these sample means
`r if (knitr::is_latex_output()) {
   '(Fig.\\ \\@ref(fig:RollDiceHistMeanFig)).'
} else {
   '(see the animation below).'
}`
The mean for any single sample of $n = 25$ rolls will sometimes be higher than $\mu = 3.5$, and sometimes lower than $\mu = 3.5$, but often close to\ $3.5$.


```{r RollDiceHistMeanHTML, animation.hook="gifski", interval=0.4, dev=if (is_latex_output()){"pdf"}else{"png"}}
if (knitr::is_html_output()){
  set.seed(123456789)
  num.sims <- 1000
  num.rolls <- 25
  
  
  print_Histo <- rep(FALSE, num.sims)
  print_Histo[ c( 1:10,
                  seq(24, num.sims, 25) + 1,
                  num.sims) ] <- TRUE
  
  die.mn <- sum( (1:6) * (1/6))  # 3.5 as expected
  die.vr <-  sum( ((1:6) - 3.5)^2 * (1/6))
  
  meanList <- array( dim = num.sims)
  
  for (i in 1:num.sims){
    meanSingleRoll <- mean(sample(1:6, 
                                  num.rolls, 
                                  replace = TRUE))
    meanList[i] <- meanSingleRoll
    
    
    #Print every nth histogram only
    if (print_Histo[i]){
      out <- hist(meanList,
                  xlab = "Mean of 25 rolls",
                  ylab = "Number of times observed",
                  main = paste("Histogram of the mean of 25 rolls:\nSimulation number:", i),
                  sub = paste("(For this sample: mean roll is ", 
                              format(round(meanList[i], 2), nsmall = 2), 
                              ")", 
                              sep = "" ),
                  col = plot.colour,
                  las = 1,
                  xlim = c(1.5, 5.5), 
                  right = FALSE,
                  ylim = c(0, 300),
                  breaks = seq(2, 5, by = 0.2)
      )
      
      points(meanList[i], 0,
             pch = 19,
             col = plot.colour0)
      xx <- seq(1, 6, length=500)
      yy <- dnorm(xx, 
                  mean = die.mn, 
                  sd = sqrt(die.vr/num.rolls) )
      yy <- yy/max(yy) * max(out$count)
      
      lines(yy ~ xx, 
            col = "grey", 
            lwd = 2)  
    }
  }
}
```


```{r RollDiceHistMeanFig, fig.align="center", fig.width=5, fig.height=2.5, fig.cap="Rolling dice: the mean of 25 rolls, for thousands of repetitions.  The solid line is the normal distribution used to model the sampling distribution." }
if (knitr::is_latex_output()){
  set.seed(123456789)
  num.sims <- 2000
  num.rolls <- 25
  
  die.mn <- sum( (1:6) * (1/6))  # 3.5 as expected
  die.vr <-  sum( ((1:6) - 3.5)^2 * (1/6))
  
  meanList <- array( dim=num.sims)
  
  for (i in 1:num.sims){
    meanSingleRoll <- mean(sample(1:6, 
                                  num.rolls, 
                                  replace = TRUE))
    meanList[i] <- meanSingleRoll
  }
  
  mar <- par()$mar
  par( mar = c( mar[1] - 1.3, 
                0, 
                mar[3] - 0.5, 
                0))
  out <- hist(meanList,
              xlab = "Mean of 25 rolls",
              #ylab = "Number of times observed",
              ylab = "",
              main = "Histogram of the mean of 25 rolls:\nafter thousands of simulations",
              #                             num.sims, "simulations"),
              col = plot.colour,
              las = 1,
              axes = FALSE,
              #xlim = c(1.5, 5.5),
              right = FALSE,
              #ylim = c(0, 300),
              breaks = seq(2, 5, by = 0.2) 
  )
  axis(side = 1,
       at = 1:6)
  xx <- seq(1, 6, 
            length = 500)
  yy <- dnorm(xx, 
              mean = die.mn, 
              sd = sqrt(die.vr/num.rolls) )
  yy <- yy/max(yy) * max(out$count)
  
  lines(yy ~ xx, 
        col = "grey", 
        lwd = 2)  
  
  points(x = 3.5,
         y = 0,
         pch = 19)
  mtext(expression(mu),
        side = 1, 
        line = 0.5)
}      
```


`r if (knitr::is_latex_output()) {
   'From Fig.\\ \\@ref(fig:RollDiceHistMeanFig),'
} else {
   'From the animation above,'
}`
the sample means vary with an approximate normal distribution (as with the sample proportions).
This normal distribution does *not* describe the data; it describes how the *values of the sample means vary across all possible samples*.
Under certain conditions (Sect.\ \@ref(ValiditySampleMean)), the values of the sample mean vary with a normal distribution, and this normal distribution has a mean and a standard deviation.

The mean of this sampling distribution---the *sampling mean*---has the value $\mu$.
The standard deviation of this sampling distribution is called the *standard error of the sample means*, denoted $\text{s.e.}(\bar{x})$.
When the *population* standard deviation\ $\sigma$ is *known*, the value of the standard error happens to be
$$
  \text{s.e.}(\bar{x}) = \frac{\sigma}{\sqrt{n}}.
$$
In summary, the values of the sample means have a *sampling distribution* described by:

* an approximate normal distribution,
* with a sampling mean whose value is\ $\mu$, and
* a standard deviation, called the standard error, of $\text{s.e.}(\bar{x}) = \sigma/\sqrt{n}$.

However, since the *population* standard deviation is rarely ever known, we will focus on the case where the value of\ $\sigma$ is unknown (and estimated by the *sample* standard deviation,\ $s$).


## Sampling distribution for $\bar{x}$: for $\sigma$ unknown {#SamplingDistSampleMean}
\index{Sampling distribution!one mean ($\sigma$ unknown)}

Since the value of the population standard deviation\ $\sigma$ is almost never known, the sample standard deviation\ $s$ is used to give an estimate of the standard error of the mean: $\text{s.e.}(\bar{x}) = s/\sqrt{n}$.
With this information, the *sampling distribution of the sample mean* can be described.


::: {.definition #DEFSamplingDistributionXbarCI name="Sampling distribution of a sample mean for $\sigma$ unknown"}
When the *population* standard deviation is unknown, the *sampling distribution of the sample mean* is (when certain conditions are met; Sect.\ \@ref(ValiditySampleMean)) described by:
  
* an approximate normal distribution,
* centred around a sampling mean whose value is $\mu$,
* with a standard deviation (called the *standard error of the mean*), denoted\ $\text{s.e.}(\bar{x})$, whose value is
\begin{equation}
   \text{s.e.}(\bar{x}) = \frac{s}{\sqrt{n}},
   (\#eq:stderrorxbarCI)
\end{equation}
where\ $n$ is the size of the sample, and\ $s$ is the sample standard deviation of the observations. 
:::


::: {.importantBox .important data-latex="{iconmonstr-warning-8-240.png}"}
A mean or a median may be appropriate for describing the *data*.
However, the *sampling distribution* for the sample mean (under certain conditions) has a *normal distribution*.
Hence, the mean is appropriate for describing the sampling distribution, even if not for describing the data.
:::


<!-- ```{r NotationOneMeanCI} -->
<!-- OneMeanNotation <- array( dim = c(4, 2)) -->

<!-- OneMeanNotation[1, ] <- c("Individual values in the population", -->
<!--                           "Vary with mean $\\mu$ and standard deviation $\\sigma$") -->
<!-- OneMeanNotation[2, ] <- c("Individual values in a sample", -->
<!--                           "Vary with mean $\\bar{x}$ and standard deviation $s$") -->
<!-- OneMeanNotation[3, ] <- c("Sample means ($\\bar{x}$) across", -->
<!--                           "Vary with approx. normal distribution (under certain conditions):") -->
<!-- OneMeanNotation[4, ] <- c("all possible samples", -->
<!--                           "sampling mean $\\mu$; standard deviation $\\text{s.e.}(\\bar{x})$") -->


<!-- if( knitr::is_latex_output() ) { -->
<!--   kable( OneMeanNotation, -->
<!--          format = "latex", -->
<!--          booktabs = TRUE, -->
<!--          longtable = FALSE, -->
<!--          escape = FALSE, -->
<!--          caption = "The notation used for describing means, and the sampling distribution of the sample means", -->
<!--          align = c("r", "l"), -->
<!--          linesep = c("\\addlinespace", -->
<!--                      "\\addlinespace", -->
<!--                      ""), -->
<!--          col.names = c("Quantity", -->
<!--                        "Description") ) %>% -->
<!-- 	row_spec(0, bold = TRUE) %>% -->
<!--   kable_styling(font_size = 10) -->
<!-- } else { -->
<!--   OneMeanNotation[3, 1] <- paste(OneMeanNotation[3, 1],  -->
<!--                                  OneMeanNotation[4, 1]) -->
<!--   OneMeanNotation[3, 2] <- paste(OneMeanNotation[3, 2],  -->
<!--                                  OneMeanNotation[4, 2]) -->
<!--   OneMeanNotation[4, ] <- NA -->

<!--     kable( OneMeanNotation, -->
<!--          format = "html", -->
<!--          booktabs = TRUE, -->
<!--          longtable = FALSE, -->
<!--          escape = FALSE, -->
<!--          caption = "The notation used for describing means, and the sampling distribution of the sample means", -->
<!--          align = c("r", "l"), -->
<!--          linesep = c("\\addlinespace", -->
<!--                      "\\addlinespace", -->
<!--                      ""), -->
<!--          col.names = c("Quantity", -->
<!--                        "Description") ) %>% -->
<!-- 	row_spec(0, bold = TRUE)  -->
<!-- } -->
<!-- ``` -->


## Confidence intervals for $\mu$ {#OneMeanCI}
\index{Confidence intervals!one mean}

In practice, we do not know the value of\ $\mu$.
After all, that's why we take a sample: to *estimate* the value of the unknown population mean.
Suppose, then, we did not know the value of\ $\mu$ for the die-rolling situation.

Then, we don't know the value of\ $\mu$ (the parameter), but we have an *estimate*: the value of\ $\bar{x}$, the sample mean (the statistic).
The value of\ $\bar{x}$ may be a bit smaller than\ $\mu$, or a bit larger than\ $\mu$ (but we don't know which, since we do not know the value of\ $\mu$).
In other words, the values of\ $\mu$ that may have produced the observed value\ $\bar{x}$ may be less than the value of\ $\bar{x}$, or greater than the value of\ $\bar{x}$.

Since the values of\ $\bar{x}$ vary from sample to sample (*sampling variation*) with an approximate normal distribution (Def.\ \@ref(def:DEFSamplingDistributionXbarCI)), the $68$--$95$--$99.7$ rule could be used to construct an approximate $95$%\ interval for the plausible values of\ $\mu$ that may have produced the observed values of the sample mean.\index{68@$68$--$95$--$99.7$ rule}
This is a *confidence interval*.

A confidence interval (CI) for the population mean is an interval surrounding a sample mean.
In general, a confidence interval for\ $\mu$ is
$$
   \bar{x} \pm \overbrace{\big(\text{multiplier}\times\text{s.e.}(\bar{x})\big)}^{\text{The `margin of error'}}.
$$
For an approximate $95$%\ CI, the multiplier is about\ $2$ (since about\ $95$% of values are within two standard deviations of the mean, from the $68$--$95$--$99.7$ rule).


::: {.definition #ConfidenceIntervalmu name="Confidence interval for $\mu$"}
A *confidence interval* (CI) for the unknown value of the population mean\ $\mu$ is
\begin{equation}
  \bar{x} \pm \big( \text{multiplier} \times \text{s.e.}(\bar{x})\big), 
  (\#eq:CImu)
\end{equation}
where $\big( \text{multiplier} \times \text{s.e.}(\hat{p})\big)$ is the *margin of error*, and
$$
   \text{s.e.}(\bar{x}) 
   =
   \frac{s}{\sqrt{n}}
$$
is the *standard error* of\ $\bar{x}$, where\ $\bar{x}$ is the sample mean, and\ $n$ is the sample size.
For an *approximate* $95$%\ CI, the multiplier is\ $2$.
:::


CIs are commonly $95$%\ CIs, but *any* level of confidence can be used (with the appropriate multiplier).
In this book, a multiplier of\ $2$ is used when *approximate* $95$%\ CIs are created manually, and otherwise software is used.
Commonly, CIs are computed at $90$%,\ $95$% and\ $99$% confidence levels.


::: {.tipBox .tip data-latex="{iconmonstr-info-6-240.png}"}
In Chap.\ \@ref(CIOneProportion), the multiplier was a $z$-score, and approximate values for the multiplier can be found using the $68$--$95$--$99.7$ rule.
When computing the CI for a sample mean, the multiplier is *not* a $z$-score (but is very *similar* to a $z$-score).
The multiplier is only a $z$-score when the value of the *population* standard deviation is known (e.g., the situation in Sect.\ \@ref(SamplingDistSampleMeanSigmaKnown)).
When\ $\sigma$ is unknown (almost always), and the *sample* standard deviation is used instead, the multiplier is a $t$-score (Sect.\ \@ref(Tscores)).

The values of\ $t$- and $z$-multipliers are *very* similar, and (except for small sample sizes) using an approximate multiplier of\ $2$ is reasonable for computing *approximate* $95$%\ CIs in either case.
:::


Pretend for the moment that the value of $\mu$ was unknown, and we tossed a die $25$\ times, and found $\bar{x} = 3.2$ and $s = 2.5$.
Then,
$$
   \text{s.e.}(\bar{x}) = \frac{s}{\sqrt{n}} = \frac{2.5}{\sqrt{25}} = 0.5.
$$
The sample means vary with an approximate normal distribution, centred around the unknown value of\ $\mu$, with a standard deviation of $\text{s.e.}(\bar{x}) = 0.5$ (Fig.\ \@ref(fig:DiceMeanNormal)).


```{r CIrelationshipsMean, out.width='75%', fig.align="center", fig.cap="A CI gives a range of values of $\\mu$ for which it is reasonable to produce the observed value of $\\bar{x}$. The shaded regions under the normal distributions represent the regions containing 95\\% of the values of $\\bar{x}$ for each value of $\\bar{x}$.", fig.width=6.5, fig.height=8}
#source("R/showCIForVariousMeans.R")                
```


```{r DiceMeanNormal, fig.cap="The sampling distribution is an approximate normal distribution; it shows a model of how the mean roll varies when a die is rolled $25$ times.", fig.align="center", fig.width=9.5, fig.height=2.75, out.width='95%'}

mn <- 3.5
n <- 25
stdd <- 2.5

se <- stdd / sqrt(n)

par( mar = c(4, 0.5, 0.5, 0.5) )
out <- plotNormal(mn,
                  se,
                  xlab = "Sample mean die roll, from 25 rolls of a die", 
                  cex.axis = 0.95,
                  ylim = c(0, 1.3),
                  showXlabels = c( 	
                    expression( mu-1.5),
                    expression( mu-1), 
                    expression( mu-0.5), 
                    expression( mu ),
                    expression( mu+0.5), 
                    expression( mu+1.0), 
                    expression( mu+1.5) ) )

arrows(x0 = mn,
       x1 = mn,
       y0 = 1.4 * max(out$y),
       y1 = max(out$y),
       lwd = 2,
       length = 0.15,
       angle = 15)
text(x = mn,
     y = 1.4 * max(out$y),
     pos = 3,
     labels = expression(Sampling~mean) )

arrows(x0 = mn ,
       x1 = mn + se,
       y0 = 0.35 * max(out$y),
       y1 = 0.35 * max(out$y),
       lwd = 2,
       code = 3,
       length = 0.15,
       angle = 15)
text(x = mn + (se / 2),
     y = 0.3 * max(out$y),
     pos = 3,
     labels = expression(Std~error))
text(x = mn + (se / 2),
     y = 0.32 * max(out$y),
     pos = 1,
     labels = expression(plain(s.e.)(bar(italic(x)))))


# Explanations at left and right
text(x = 2.2,
     y = 0.3 * max(out$y),
     pos = 3,
     labels = expression(Values~of~bar(italic(x))~smaller~than~mu))
text(x = 4.8,
     y = 0.3 * max(out$y),
     pos = 3,
     labels = expression(Values~of~bar(italic(x))~larger~than~mu) )

```


Our estimate of\ $\bar{x} = 3.2$ may be a bit smaller than the value of\ $\mu$, or a bit larger than the value of\ $\mu$; that is, the value of\ $\mu$ is\ $\bar{x}$ give-or-take a bit.
A range of\ $\mu$ values that are likely to straddle\ $\bar{x}$ is given by a CI.\spacex
An *approximate* $95$%\ CI is from
\begin{align*}
                 3.2 - (2 \times 0.5) &\qquad\text{(which is $2.2$)}\\
  \text{to}\quad 3.2 + (2 \times 0.5) &\qquad\text{(which is $4.2$)}.
\end{align*}
Hence, values of\ $\mu$ between\ $2.2$ to\ $4.2$ could reasonably have produced a sample mean of $\bar{x} = 3.2$. 
<!-- (Fig.\ \@ref(fig:CIrelationshipsMu)). -->

Using software, the exact $95$%\ CI is from\ $2.17$ to\ $4.23$, the same as the approximate CI to one decimal place.


<!-- ```{r, CIrelationshipsMu, out.width='90%', fig.align="center", fig.cap="Various values of $\\mu$ from which the observed value of $\\bar{x}$ could reasonably be observed. The intervals are the $95$\\% sampling intervals for the given value of $\\mu$. The confidence interval contains the values for which the sampling interval contains the observed value of $\\bar{x}$.", fig.width=8, fig.height=5} -->
<!-- source("R/rangeForMuCI.R")  -->
<!-- ``` -->

<!-- ```{r BagsNormal, fig.cap="The sampling distribution is a normal distribution; it shows how the sample mean bag weight varies in samples of size $n = 586$", fig.align="center", fig.width=9.5, fig.height=2.75, out.width='95%'} -->

<!-- mn <- 2.8 -->
<!-- n <- 586 -->
<!-- stdd <- 0.94 -->

<!-- se <- stdd/sqrt(n) -->

<!-- par( mar = c(4, 0.5, 0.5, 0.5) ) -->
<!-- out <- plotNormal(mn, -->
<!--                   se, -->
<!--                   xlab = "Sample mean bag weight, in kg (sample size: 586)",  -->
<!--                   cex.axis = 0.95, -->
<!--                   showXlabels = c( 	 -->
<!--                     expression( mu-0.114), -->
<!--                     expression( mu-0.076),  -->
<!--                     expression( mu-0.038),  -->
<!--                     expression( mu ), -->
<!--                     expression( mu+0.038),  -->
<!--                     expression( mu+0.076),  -->
<!--                     expression( mu+0.114) ) ) -->
<!-- ``` -->



## Statistical validity conditions {#ValiditySampleMean}
\index{Statistical validity!one mean}

As with any confidence interval, the underlying mathematics requires certain conditions to be met so that the results are statistically valid (i.e., the sampling distribution is sufficiently like a normal distribution).

Statistical validity can be assessed using these criteria:

* When $n \ge 25$, the test is statistically valid.
  (If the distribution of the data is highly skewed, the sample size may need to be larger.)
* When $n < 25$, the test is statistically valid only if the data come from a *population* with a normal distribution.

The sample size of\ $25$ is a rough figure, and some books give other values (such as\ $30$).
For a CI for one mean, these conditions are the same as for the hypothesis test for one mean (Sect.\ \@ref(ValiditySampleMeanTest)).


This condition ensures that the *sampling distribution of the sample means has an approximate normal distribution* (so that, for example, the $68$--$95$--$99.7$ rule can be used).\index{68@$68$--$95$--$99.7$ rule}
Provided the sample size is larger than about\ $25$, this will be approximately true *even if* the distribution of the individuals in the
population do not have a normal distribution.
That is, when $n \ge 25$ the sample means generally have an approximate normal distribution, even if the data themselves do not follow a normal distribution.
The units of analysis are also assumed to be *independent* (e.g., from a simple random sample).

If the statistical validity conditions are not met, other methods (e.g., non-parametric methods\index{Non-parametric statistics} [@bauer1972constructing]; resampling methods\index{Resampling methods} [@efron2021computer]) may be used.


::: {.importantBox .important data-latex="{iconmonstr-warning-8-240.png}"}
When $n \ge 25$ approximately, the *data* do not have to have a normal distribution.
The *sample means* need to have a normal distribution, which is approximately true if the statistical validity conditions are true.
:::


::: {.example #DiceConditions name="Statistical validity"}
In the die example (Sect.\ \@ref(OneMeanCI)), where $n = 25$, the CI is statistically valid.
:::


Deciding whether the *population* has a normal distribution is obviously difficult; we do not have access to the whole population.
All we can reasonably do is to identify (from the sample) populations that are likely to be non-normal (when the CI would be not valid).


<div style="float:right; width: 222x; border: 1px; padding:10px">
<img src="Illustrations/pexels-anna-shvets-4225923.jpg" width="200px"/>
</div>


::: {.example #AssumptionsCT name="Statistical validity"}
A study [@data:silverman:CT; @data:zou:fluoroscopy] examine exposure to radiation for CT scans in the abdomen for $n = 17$ patients.
As the sample size is 'small' (less than\ $25$), the *population data* must have a normal distribution for a CI for\ $\mu$ to be statistically valid.

A histogram of the total radiation dose received using the *sample* data (Fig.\ \@ref(fig:CTscanHistogram)) suggests this is very unlikely.
Even though the histogram is from *sample* data, it seems improbable that the data in the sample would have come from a *population* with a normal distribution.

A CI for the mean of these data will probably be *not* statistically valid.
Other methods (such as resampling methods\index{Resampling methods}, which are beyond the scope of this book) are needed to compute a CI for the mean.
:::


```{r CTscanHistogram, fig.cap="The radiation doses from CT scans for $17$ people.", fig.align="center", fig.width=5, fig.height=3.25}
data(Fluoro)

hist(Fluoro$Dose,
	col = plot.colour,
	las = 1,
	xlab = "Radiation dose (in rads)",
	ylab = "Number of people",
	main = "Radiation dose for 17 people\nundergoing a CT scan")
box()
```

<iframe src="https://learningapps.org/watch?v=ppetqnq4322" style="border:0px;width:100%;height:500px" allowfullscreen="true" webkitallowfullscreen="true" mozallowfullscreen="true"></iframe>


## Example: cadmium in peanuts {#Cadmium-In-Peanuts}


<div style="float:right; width: 222x; border: 1px; padding:10px">
<img src="Illustrations/tom-hermans-ZPfd3ZobOc0-unsplash.jpg" width="200px"/>
</div>


@data:Blair2017:Peanuts studied peanuts gathered from a variety of regions in the United States over various times (a representative sample).
They found the sample mean cadmium concentration was $\bar{x} = 0.076$\ppms with a standard deviation of $s = 0.0460$\ppms, from a sample of $290$\ peanuts.
The parameter is\ $\mu$, the population mean cadmium concentration in peanuts.

Every sample of $n = 290$ peanuts is likely to produce a different sample mean, so *sampling variation* in $\bar{x}$\ exists and can be measured using the standard error:
$$
	\text{s.e.}(\bar{x}) = \frac{s}{\sqrt{n}} = \frac{0.0460}{\sqrt{290}} = 0.002701\text{ ppm}.
$$
The approximate $95$%\ CI\ is $0.0768 \pm (2 \times 0.002701)$, or $0.0768 \pm 0.00540$, which is from\ $0.0714$ to\ $0.0822$\ppms.
(The *margin of error* is $0.00540$.)\index{Margin of error}
We write:

> The sample mean cadmium concentration of peanuts is $\bar{x} = 0.0768$\ppms (s.e.: $0.00270$; $n = 290$), with an approximate $95$%\ CI from\ $0.0714$ to\ $0.0822$\ppms.

If we repeatedly took samples of size\ $290$ from this population, about\ $95$% of the $95$%\ CIs would contain the population mean (*our* CI may or may not contain the value of\ $\mu$).
The plausible values of\ $\mu$ that could have produced $\bar{x} = 0.0768$ are between\ $0.0714$ and\ $0.0822$\ppms.
Alternatively, we are about\ $95$% confident that the CI of\ $0.0714$ to\ $0.0822$\ppms straddles the population mean.

Since the sample size is larger than\ $25$, the CI is statistically valid.


```{r}
data(LungCap)

LC.F11 <- subset(LungCap, 
                 Age == 11 & Gender == "F")
```



## Chapter summary

To compute a confidence interval (CI) for a mean, compute the sample mean,\ $\bar{x}$, and identify the sample size\ $n$.
Then compute the standard error, which quantifies how much the value of\ $\bar{x}$ varies across all possible samples:
$$
  \text{s.e.}(\bar{x})
  =
  \frac{s}{\sqrt{n}},
$$
where\ $s$ is the sample standard deviation.
The *margin of error* is (multiplier${}\times{}$standard error), where the multiplier is\ $2$ for an approximate $95$%\ CI (from the $68$--$95$--$99.7$ rule).
Then the CI is:
$$
   \bar{x} \pm \left( \text{multiplier}\times\text{standard error} \right).
$$
The statistical validity conditions should also be checked.


## Quick review questions {#Chap25-QuickReview}

::: {.webex-check .webex-box}
Are the following statements *true* or *false*?

1. The value of $\bar{x}$ varies from sample to sample. \tightlist  
`r if( knitr::is_html_output() ) {torf(answer = TRUE )}`
1. A CI for\ $\mu$ is never statistically valid if the histogram of the *data* has a non-normal distribution.  
`r if( knitr::is_html_output() ) {torf(answer = FALSE )}`
1. A sample of data produces $s = 8$ and $n = 20$; the standard error of the mean is $1.7889$.
`r if( knitr::is_html_output() ) {torf(answer = TRUE )}`
1. Since the sample size is less than\ $25$, the standard error is not statistically valid.
`r if( knitr::is_html_output() ) {torf(answer = FALSE )}`
:::



## Exercises {#OneMeanConfIntervalExercises}

Answers to odd-numbered exercises are available in App.\ \@ref(Answers).

`r if( knitr::is_latex_output() ) "\\captionsetup{font=small}"`

::: {.exercise #OneMeanCIBears}
@bartareau2017estimating studied American black bears, and found the mean weight of the $n = 185$ male bears was  $\bar{x} = 84.9$\kgs, with a standard deviation of $s = 51.1$\kgs.

1. Define the *parameter* of interest.
1. Compute the standard error of the mean.
1. Draw a picture of the approximate sampling distribution for\ $\bar{x}$.
1. Compute the approximate $95$%\ CI.
1. Write a conclusion.
1. Is the CI statistically valid?
:::


::: {.exercise #BagsCI}
@data:Dianat2014:schoolbags studied the weight of the school bags of a sample of $586$\ children in Grades $6$--$8$ in Tabriz, Iran.
The mean weight was $\bar{x} = 2.8$\kgs with a standard deviation of $s = 0.94$\kgs.

1. Define the *parameter* of interest.
1. Compute the standard error of the mean.
1. Draw a picture of the approximate sampling distribution for\ $\bar{x}$.
1. Compute the approximate $95$%\ CI.
1. Write a conclusion.
1. Is the CI statistically valid?
:::


::: {.exercise #CIOneMeanLungCapacityInChildren}
@data:Tager:FEV studied the lung capacity of children in East Boston.
They measured the forced expiratory volume (FEV) of a sample of $n = 45$ eleven-year-old girls.
For these children, the mean lung capacity was $\bar{x} = 2.85$ litres and the standard deviation was $s = 0.43$ litres [@BIB:data:FEV].
Find an approximate $95$%\ CI for the population mean lung capacity of eleven-year-old females from East Boston.
:::


::: {.exercise #CIOneMeanEnvironmentalPollution}
@data:Taylor2013:Lead studied lead smelter emissions near children's public playgrounds.
They found the mean lead concentration at one playground (Memorial Park, Port Pirie, in South Australia) was $6\,956.41$ micrograms per square metre, with a standard deviation of $7\,571.74$ micrograms of lead per square metre, from a sample of $n = 58$ wipes taken over a seven-day period.
(As a reference, the Western Australian Government recommends a maximum of $400$\ micrograms of lead per square metre.)

Find an approximate $95$%\ CI for the mean lead concentration at this playground.
Would these results apply to playgrounds in other parts of Australia?
:::


::: {.exercise #CIOneMeanToothbrushing}
@data:Macgregor1985:ToothbrushinghYoungAdults studied the brushing time for $60$\ young adults (aged $18$--$22$ years old), and found the mean tooth brushing time was\ $33.0$\secs, with a standard deviation of\ $12.0$\secs.
Find an approximate $95$%\ CI for the mean brushing time for young adults.
:::


::: {.exercise #CIOneMeanBloodLoss}
@data:Williams2007:BloodLoss asked paramedics ($n = 199$) to estimate the amount of blood loss on four different surfaces.
When the actual amount of blood spill on concrete was\ $1\,000$\mLs, the mean guess was\ $846.4$\mLs (with a standard deviation of\ $651.1$\mLs).

1. What is the approximate $95$%\ CI for the mean guess of blood loss?
1. Do you think the participants are good at estimating the amount of blood loss on concrete?
1. Is this CI statistically valid?
:::


::: {.exercise #OneMeanCINHANESInterpret}
Using data from the <span style="font-variant:small-caps;">bhadp</span> study [@data:NHANES3], the approximate $95$%\ CI for the mean direct HDL cholesterol is\ $1.356$ to\ $1.374$\mmols/L.\spacex
Which (if any) of these interpretations are acceptable?
Explain *why* are the other interpretations are incorrect.

1. In the *sample*, about $95$%\ of individuals have a direct HDL concentration between $1.356$ to $1.374$\mmols/L.
1. In the *population*, about $95$%\ of individuals have a direct HDL concentration between $1.356$ to $1.374$\mmols/L.
1. About $95$%\ of the *samples* are between\ $1.356$ to\ $1.374$\mmols/L.
1. About $95$%\ of the *populations* are between\ $1.356$ to\ $1.374$\mmols/L.
1. The *population* mean varies so that it is between\ $1.356$ to\ $1.374$\mmols/L about $95$% of the time.
1. We are about $95$%\ sure that *sample* mean is between\ $1.356$ to\ $1.374$\mmols/L.
1. It is plausible that the *sample* mean is between\ $1.356$ to\ $1.374$\mmols/L.
:::


::: {.exercise #OneMeanStdError}
@data:Grabosky2016:Trees describe the diameter of *Quercus bicolor* trees planted in a lawn as having a mean of\ $25.8$\cms, with a standard error of\ $0.64$\cms, from a sample of\ $19$ trees.
Which (if any) of the following is correct?
  
1. About\ $95$% of the trees in the *sample* will have a diameter between $25.8 - (2\times 0.64)$ and $25.8 + (2\times 0.64)$\cms (using the $68$--$95$--$99.7$ rule).
1. About\ $95$% of these types of trees in the *population* will have a diameter between $25.8 - (2\times 0.64)$ and $25.8 + (2\times 0.64)$\cms (using the $68$--$95$--$99.7$ rule)?
:::


::: {.exercise #ChewingTime}
@watanabe1995estimation studied $n = 30$ five-year-old children, and found the mean time for the children to eat a cookie was\ $61.3$\secs, with a standard deviation of\ $29.4$\secs.

1. What is an approximate $95$%\ CI for the population mean time for a five-year-old child to eat a cookie? 
2. Is the CI statistically valid?
:::


::: {.exercise #CIPizzas}
[*Dataset*: `PizzaSize`]
In\ 2011, *Eagle Boys Pizza* ran a campaign that claimed (among many other claims) that *Eagle Boys* pizzas were 'Real size $12$-inch large pizzas' in an effort to out-market *Domino's Pizza*.
*Eagle Boys* made the data behind the campaign publicly available [@mypapers:Dunn:PizzaSize].
A summary of the diameters of a sample of\ $125$ of *Eagle Boys* large pizzas is shown in Fig.\ \@ref(fig:PizzaCIjamovi).

1. What do\ $\mu$ and\ $\bar{x}$ represent in this context? \tightlist 
2. Write down the *values* of\ $\mu$ and\ $\bar{x}$.
3. Write down the *values* of\ $\sigma$ and\ $s$.
4. Compute the value of the standard error of the mean.
5. Explain the difference in *meaning* between\ $s$ and\ $\text{s.e.}(\bar{x})$ here.
6. If someone else takes a sample of\ $125$ *Eagle Boys* pizzas, will the sample mean be $11.486$\ inches again (as in this sample)?
   Why or why not?
7. Draw a picture of the approximate sampling distribution for\ $\bar{x}$.
8. Compute an approximate $95$%\ confidence interval for the mean pizza diameter.
9. Write a statement that communicates your $95$%\ CI for the mean pizza diameter.
10. What are the *statistical* validity conditions?
11. Which of these conditions must we *assume* are met for this CI to be *statistically* valid?
    Explain.
    
    - The sample size is greater than about\ $25$.
    - The population has a normal distribution.
    - The population standard deviation is known.
    - The sample has a normal distribution.
12. Do you think that, on average, the pizzas do have a mean diameter of $12$\ inches in the population, as Eagle Boy's claim?
    Explain.
:::


::: {.exercise #CIMatchesPerBox}

Claire and Jake were wondering about the mean number of matches in a box.
The boxes contain this statement:

> An average of $45$\ matches per box.

They purchased a carton containing $25$\ boxes of matches, and Jake counted the number of matches in *one* of those \$25$ boxes.
The box contained $44$\ matches.

'Oh wow.  Just wow.' said Jake.
'They lie.  There's only\ $44$ in this box.'

1. What is Jake's misunderstanding? \tightlist
2. Then, they counted the number of matches in *each* of the twenty-five boxes.  
   
   Claire found that the mean number of matches per box was\ $44.9$ matches, and the standard deviation was\ $0.124$.  
   
   Jake notes that the mean is $44.9$\ matches per box, and says: 'You can't have $0.9$\ of a match.  That's dumb.'
   How would you respond?
3. 'Wow!' said Jake.
   'The claim is $45$\ matches per box on average, but the mean really is\ $44.9$!
   They're liars!  Liar, liar, pants on...'  
   
   What is Jake's misunderstanding?
4. What two broad reasons could explain why the sample mean is *not*\ $45$?
5. 'Come on, Jake,' said Claire.
   'As if the mean will be *exactly*\ $45$ in a sample every single time.
   Let's work out the confidence interval.'  
   
   Why does Claire think a CI is needed?
   What will it tell them?
6. What is an approximate $95$%\ confidence interval for the mean for Claire's sample?
7. 'Aha---I told you so!  They *are* absolutely lying!
   Your confidence interval doesn't even include their mean of\ $45$!' said Jake.
   'The manufacturer *must* be lying!'  
   
   Is Jake correct?
   Why or why not?
   What does the CI *mean*?
8. In this scenario, what does\ $\bar{x}$ represent?
   What is the *value* of\ $\bar{x}$?
9. In this scenario, what does\ $\mu$ represent?
   What is the *value* of\ $\mu$?

:::



(ref:EBSummaryCaption) Summary statistics for the diameter of *Eagle Boys* large pizzas.

```{r PizzaCIjamovi, echo=FALSE, fig.cap="(ref:EBSummaryCaption)", fig.align="center", fig.align="center", out.width='40%'}
knitr::include_graphics( "jamovi/PizzaDiameter/PizzaDiameters-jamovi.png" )
```




`r if( knitr::is_latex_output() ) "\\captionsetup{font=normalsize}"`



<!-- QUICK REVIEW ANSWERS -->
`r if (knitr::is_html_output()) '<!--'`
::: {.EOCanswerBox .EOCanswer data-latex="{iconmonstr-check-mark-14-240.png}"}
**Answers to *Quick Revision* questions:**
**1.** True
**2.** False.
**3.** True.
**4.** False.
:::
`r if (knitr::is_html_output()) '-->'`

